{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#path to save trained model\n",
    "sd=10000\n",
    "\n",
    "\n",
    "wheretosave='/home/ubuntu/Notebooks/kdd_toffa46_sd' + str(sd) + '.mat'\n",
    "no_data_ex=6656 \n",
    "no_data_validation=171 \n",
    "no_data_test=160\n",
    "train_loss_size = 32 \n",
    "total_data_ex=6987 \n",
    "\n",
    "#number of pixels in training images\n",
    "numpix=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import hdf5storage\n",
    "from __future__ import division\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "run_opts = tf.RunOptions(report_tensor_allocations_upon_oom = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Handle training data: Stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "#load preconvolved stimuli\n",
    "\n",
    "\n",
    "datapath='/home/ubuntu/Notebooks/kdd_toffa_preconv_data.mat'\n",
    "\n",
    "data = hdf5storage.loadmat(datapath)\n",
    "\n",
    "## Handle training data: Stimuli convolved with bipolar cell kernels\n",
    "\n",
    "input_bip1_0 = data['b1_input']\n",
    "input_bip2_0 = data['b2_input']\n",
    "input_bip3_0 = data['b3_input']\n",
    "input_bip4_0 = data['b4_input']\n",
    "input_bip5_0 = data['b5_input']\n",
    "input_bip6_0 = data['b6_input']\n",
    "input_bip7_0 = data['b7_input']\n",
    "input_bip8_0 = data['b8_input']\n",
    "input_bip9_0 = data['b9_input']\n",
    "input_bip10_0 = data['b10_input']\n",
    "input_bip11_0 = data['b11_input']\n",
    "input_bip12_0 = data['b12_input']\n",
    "input_bip13_0 = data['b13_input']\n",
    "input_bip14_0 = data['b14_input']\n",
    "input_bip15_0 = data['b15_input']\n",
    "input_bip16_0 = data['b16_input']\n",
    "\n",
    "\n",
    "\n",
    "data_duration1=input_bip1_0.shape[1]\n",
    "print(data_duration1)\n",
    "\n",
    "data_duration = 990\n",
    "\n",
    "def rearrange_bip_input(input_bip_0, startind, endind):\n",
    "    input_bip_1 = reshape(input_bip_0, [1, total_data_ex, data_duration1, numpix])\n",
    "    input_bip_11 = input_bip_1[:, startind:endind, 7:997, :]\n",
    "    input_bip_2 = np.swapaxes(input_bip_11, 0, 3)\n",
    "    input_bip_3 = reshape(input_bip_2, [numpix, total_data_ex, data_duration])\n",
    "    return input_bip_3\n",
    "\n",
    "startind = 0\n",
    "endind = total_data_ex\n",
    "\n",
    "input_bip1_3 = rearrange_bip_input(input_bip1_0, startind, endind)\n",
    "input_bip2_3 = rearrange_bip_input(input_bip2_0, startind, endind)\n",
    "input_bip3_3 = rearrange_bip_input(input_bip3_0, startind, endind)\n",
    "input_bip4_3 = rearrange_bip_input(input_bip4_0, startind, endind)\n",
    "input_bip5_3 = rearrange_bip_input(input_bip5_0, startind, endind)\n",
    "input_bip6_3 = rearrange_bip_input(input_bip6_0, startind, endind)\n",
    "input_bip7_3 = rearrange_bip_input(input_bip7_0, startind, endind)\n",
    "input_bip8_3 = rearrange_bip_input(input_bip8_0, startind, endind)\n",
    "input_bip9_3 = rearrange_bip_input(input_bip9_0, startind, endind)\n",
    "input_bip10_3 = rearrange_bip_input(input_bip10_0, startind, endind)\n",
    "input_bip11_3 = rearrange_bip_input(input_bip11_0, startind, endind)\n",
    "input_bip12_3 = rearrange_bip_input(input_bip12_0, startind, endind)\n",
    "input_bip13_3 = rearrange_bip_input(input_bip13_0, startind, endind)\n",
    "input_bip14_3 = rearrange_bip_input(input_bip14_0, startind, endind)\n",
    "input_bip15_3 = rearrange_bip_input(input_bip15_0, startind, endind)\n",
    "input_bip16_3 = rearrange_bip_input(input_bip16_0, startind, endind)\n",
    "\n",
    "\n",
    "input_bip1_valid = input_bip1_3[:, 0:no_data_validation, :]\n",
    "input_bip1_train = input_bip1_3[:, no_data_validation:no_data_validation+no_data_ex, :]\n",
    "input_bip1_test = input_bip1_3[:, no_data_validation+no_data_ex:no_data_validation+no_data_ex+no_data_test, :]\n",
    "\n",
    "input_bip2_valid = input_bip2_3[:, 0:no_data_validation, :]\n",
    "input_bip2_train = input_bip2_3[:, no_data_validation:no_data_validation+no_data_ex, :]\n",
    "input_bip2_test = input_bip2_3[:, no_data_validation+no_data_ex:no_data_validation+no_data_ex+no_data_test, :]\n",
    "\n",
    "input_bip3_valid = input_bip3_3[:, 0:no_data_validation, :]\n",
    "input_bip3_train = input_bip3_3[:, no_data_validation:no_data_validation+no_data_ex, :]\n",
    "input_bip3_test = input_bip3_3[:, no_data_validation+no_data_ex:no_data_validation+no_data_ex+no_data_test, :]\n",
    "\n",
    "input_bip4_valid = input_bip4_3[:, 0:no_data_validation, :]\n",
    "input_bip4_train = input_bip4_3[:, no_data_validation:no_data_validation+no_data_ex, :]\n",
    "input_bip4_test = input_bip4_3[:, no_data_validation+no_data_ex:no_data_validation+no_data_ex+no_data_test, :]\n",
    "\n",
    "input_bip5_valid = input_bip5_3[:, 0:no_data_validation, :]\n",
    "input_bip5_train = input_bip5_3[:, no_data_validation:no_data_validation+no_data_ex, :]\n",
    "input_bip5_test = input_bip5_3[:, no_data_validation+no_data_ex:no_data_validation+no_data_ex+no_data_test, :]\n",
    "\n",
    "input_bip6_valid = input_bip6_3[:, 0:no_data_validation, :]\n",
    "input_bip6_train = input_bip6_3[:, no_data_validation:no_data_validation+no_data_ex, :]\n",
    "input_bip6_test = input_bip6_3[:, no_data_validation+no_data_ex:no_data_validation+no_data_ex+no_data_test, :]\n",
    "\n",
    "input_bip7_valid = input_bip7_3[:, 0:no_data_validation, :]\n",
    "input_bip7_train = input_bip7_3[:, no_data_validation:no_data_validation+no_data_ex, :]\n",
    "input_bip7_test = input_bip7_3[:, no_data_validation+no_data_ex:no_data_validation+no_data_ex+no_data_test, :]\n",
    "\n",
    "input_bip8_valid = input_bip8_3[:, 0:no_data_validation, :]\n",
    "input_bip8_train = input_bip8_3[:, no_data_validation:no_data_validation+no_data_ex, :]\n",
    "input_bip8_test = input_bip8_3[:, no_data_validation+no_data_ex:no_data_validation+no_data_ex+no_data_test, :]\n",
    "\n",
    "input_bip9_valid = input_bip9_3[:, 0:no_data_validation, :]\n",
    "input_bip9_train = input_bip9_3[:, no_data_validation:no_data_validation+no_data_ex, :]\n",
    "input_bip9_test = input_bip9_3[:, no_data_validation+no_data_ex:no_data_validation+no_data_ex+no_data_test, :]\n",
    "\n",
    "input_bip10_valid = input_bip10_3[:, 0:no_data_validation, :]\n",
    "input_bip10_train = input_bip10_3[:, no_data_validation:no_data_validation+no_data_ex, :]\n",
    "input_bip10_test = input_bip10_3[:, no_data_validation+no_data_ex:no_data_validation+no_data_ex+no_data_test, :]\n",
    "\n",
    "\n",
    "input_bip11_valid = input_bip11_3[:, 0:no_data_validation, :]\n",
    "input_bip11_train = input_bip11_3[:, no_data_validation:no_data_validation+no_data_ex, :]\n",
    "input_bip11_test = input_bip11_3[:, no_data_validation+no_data_ex:no_data_validation+no_data_ex+no_data_test, :]\n",
    "\n",
    "\n",
    "input_bip12_valid = input_bip12_3[:, 0:no_data_validation, :]\n",
    "input_bip12_train = input_bip12_3[:, no_data_validation:no_data_validation+no_data_ex, :]\n",
    "input_bip12_test = input_bip12_3[:, no_data_validation+no_data_ex:no_data_validation+no_data_ex+no_data_test, :]\n",
    "\n",
    "input_bip13_valid = input_bip13_3[:, 0:no_data_validation, :]\n",
    "input_bip13_train = input_bip13_3[:, no_data_validation:no_data_validation+no_data_ex, :]\n",
    "input_bip13_test = input_bip13_3[:, no_data_validation+no_data_ex:no_data_validation+no_data_ex+no_data_test, :]\n",
    "\n",
    "input_bip14_valid = input_bip14_3[:, 0:no_data_validation, :]\n",
    "input_bip14_train = input_bip14_3[:, no_data_validation:no_data_validation+no_data_ex, :]\n",
    "input_bip14_test = input_bip14_3[:, no_data_validation+no_data_ex:no_data_validation+no_data_ex+no_data_test, :]\n",
    "\n",
    "input_bip15_valid = input_bip15_3[:, 0:no_data_validation, :]\n",
    "input_bip15_train = input_bip15_3[:, no_data_validation:no_data_validation+no_data_ex, :]\n",
    "input_bip15_test = input_bip15_3[:, no_data_validation+no_data_ex:no_data_validation+no_data_ex+no_data_test, :]\n",
    "\n",
    "input_bip16_valid = input_bip16_3[:, 0:no_data_validation, :]\n",
    "input_bip16_train = input_bip16_3[:, no_data_validation:no_data_validation+no_data_ex, :]\n",
    "input_bip16_test = input_bip16_3[:, no_data_validation+no_data_ex:no_data_validation+no_data_ex+no_data_test, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load and handle ganglion cell responses\n",
    "\n",
    "datapath='/home/ubuntu/Notebooks/cell46_y_train.mat'\n",
    "data = hdf5storage.loadmat(datapath)\n",
    "\n",
    "y_train0 = 0.5*reshape(data['y_train'], [total_data_ex, 1, data_duration1])\n",
    "y_train0 = y_train0[0:total_data_ex, :, 0:990]\n",
    "\n",
    "y_valid = y_train0[ 0:no_data_validation, :, :]\n",
    "y_train = y_train0[no_data_validation:no_data_validation+no_data_ex, :, :]\n",
    "y_test = y_train0[no_data_validation+no_data_ex:no_data_validation+no_data_ex+no_data_test, :, :]\n",
    "\n",
    "gen_gc_w=[0.0, -0.5, 0.0, 1.0, 0.0]\n",
    "gen_gc_w=np.reshape(gen_gc_w, [5, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# SET NUMBER OF NEURONS IN EACH LAYER\n",
    "no_filters=16 #14\n",
    "no_filters_per_bc_type=1\n",
    "\n",
    "no_bipolar_rows = 1\n",
    "no_bipolars= numpix*no_bipolar_rows\n",
    "no_bipolar_types=2 \n",
    "no_relu=0\n",
    "no_am_types = 5\n",
    "no_am1=8 \n",
    "no_am2=21\n",
    "no_am3=21\n",
    "no_gc=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## load and handle filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Helper Functions\n",
    "\n",
    "\n",
    "def bias_var(shape, initial_val):\n",
    "    initial = tf.constant(initial_val, shape=shape)\n",
    "#     initial=tf.constant(initial_val+0.2*(np.random.uniform(low=-1.0, high=1.0, size=shape).astype(float32)), shape=shape)\n",
    "#     initial = tf.random_uniform(shape, minval=-1.0, maxval=0.0, dtype=tf.float32) #2.9\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bg_synapse_var(shape, initial_val):\n",
    "    initial=tf.constant(initial_val, shape=shape)\n",
    "#     initial=tf.constant(initial_val+0.2*(np.random.uniform(low=-1.0, high=1.0, size=shape).astype(float32)), shape=shape)\n",
    "    initial = tf.random_uniform(shape, minval=0.1, maxval=0.8, dtype=tf.float32) #2.9\n",
    "    return tf.Variable(initial) #initial\n",
    "\n",
    "def ba_synapse_var(shape, initial_val):\n",
    "    initial=tf.constant(initial_val, shape=shape)\n",
    "#     initial=tf.constant(initial_val+0.2*(np.random.uniform(low=-1.0, high=1.0, size=shape).astype(float32)), shape=shape)\n",
    "    initial = tf.random_uniform(shape, minval=0.05, maxval=0.2, dtype=tf.float32) #2.9\n",
    "    return tf.Variable(initial) #initial\n",
    "\n",
    "def zero_synapse_var(shape, initial_val):\n",
    "#     initial_val=tf.zeros(shape=shape)\n",
    "    initial=tf.constant(0.0*initial_val, shape=shape)\n",
    "#     initial=tf.constant(initial_val+0.2*(np.random.uniform(low=-1.0, high=1.0, size=shape).astype(float32)), shape=shape)\n",
    "#     initial = tf.random_uniform(shape, minval=0.05, maxval=0.2, dtype=tf.float32) #2.9\n",
    "    return tf.Variable(initial) #initial\n",
    "\n",
    "def synapse_var(shape, initial_val):\n",
    "    initial=tf.constant(initial_val, shape=shape)\n",
    "#     initial=tf.constant(initial_val+0.2*(np.random.uniform(low=-1.0, high=1.0, size=shape).astype(float32)), shape=shape)\n",
    "    initial = tf.random_uniform(shape, minval=0.15, maxval=0.18, dtype=tf.float32) #2.9\n",
    "    return tf.Variable(initial) #initial\n",
    "\n",
    "def linear_synapse_var(shape, initial_val):\n",
    "    initial=tf.constant(initial_val, shape=shape)\n",
    "#     initial=tf.constant(initial_val+0.2*(np.random.uniform(low=-1.0, high=1.0, size=shape).astype(float32)), shape=shape)\n",
    "    initial = tf.random_uniform(shape, minval=0.05, maxval=0.08, dtype=tf.float32) #2.9\n",
    "    return tf.Variable(initial) #initial\n",
    "\n",
    "def fb_synapse_var(shape, initial_val):\n",
    "    initial_val = initial_val.astype(float32)\n",
    "    initial=tf.constant(initial_val, shape=shape)\n",
    "    \n",
    "#     initial=tf.constant(initial_val+0.2*(np.random.uniform(low=-1.0, high=1.0, size=shape).astype(float32)), shape=shape)\n",
    "#     initial = tf.random_uniform(shape, minval=0.1, maxval=0.8, dtype=tf.float32) #2.9\n",
    "    return tf.Variable(initial) #initial\n",
    "\n",
    "def ab_synapse_var(shape, initial_val):\n",
    "    initial=tf.constant(initial_val, shape=shape)\n",
    "#     initial=tf.constant(initial_val+0.2*(np.random.uniform(low=-1.0, high=1.0, size=shape).astype(float32)), shape=shape)\n",
    "    initial = tf.random_uniform(shape, minval=0.1, maxval=0.2, dtype=tf.float32) #2.9\n",
    "    return tf.Variable(initial) #initial\n",
    "\n",
    "def ag_synapse_var(shape, true_initial_val, train_initial_val):\n",
    "    initial=tf.constant(true_initial_val, shape=shape)\n",
    "#     initial=tf.constant(train_initial_val, shape=shape)\n",
    "    \n",
    "#     initial=tf.constant(true_initial_val+0.2*(np.random.uniform(low=-1.0, high=1.0, size=shape).astype(float32)), shape=shape)\n",
    "    initial = tf.random_uniform(shape, minval=0.1, maxval=0.2, dtype=tf.float32) #2.9\n",
    "    return tf.Variable(initial) #initial\n",
    "\n",
    "def pbconv2d(x, W):\n",
    "    padsize=175 #200 #W.shape[0]\n",
    "    paddedx=tf.pad(x, [[0, 0], [padsize, 0], [0, 0], [0, 0]], 'CONSTANT')\n",
    "    outconv=tf.nn.conv2d(paddedx, W, strides=[1, 1, 1, 1], padding='SAME') #250 for movingdot and noise\n",
    "    #return tf.reshape(outconv[:, np.round(padsize/2).astype(int):np.round(padsize/2).astype(int)+250, 0, 0], [-1, 250, 1, 1])\n",
    "    return tf.reshape(outconv[:, np.round(padsize/2).astype(int):np.round(padsize/2).astype(int)+x_train.shape[1], 0, 0], [-1, x_train.shape[1], 1, 1])\n",
    "\n",
    "\n",
    "def gcconv2d(x, W):\n",
    "    padsize=5 #200 #W.shape[0]\n",
    "    paddedx=tf.pad(x, [[0, 0], [padsize, 0], [0, 0], [0, 0]], 'CONSTANT')\n",
    "    outconv=tf.nn.conv2d(paddedx, W, strides=[1, 1, 1, 1], padding='SAME') #250 for movingdot and noise\n",
    "    #return tf.reshape(outconv[:, np.round(padsize/2).astype(int):np.round(padsize/2).astype(int)+250, 0, 0], [-1, 250, 1, 1])\n",
    "    return tf.reshape(outconv[:, np.round(padsize/2).astype(int):np.round(padsize/2).astype(int)+data_duration, 0, 0], [-1, data_duration, 1, 1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#create input placeholder variables\n",
    "\n",
    "input_filt1_ = tf.placeholder(\"float32\", name=\"input_filt1\")\n",
    "input_filt2_ = tf.placeholder(\"float32\", name=\"input_filt2\")\n",
    "input_filt3_ = tf.placeholder(\"float32\", name=\"input_filt3\")\n",
    "input_filt4_ = tf.placeholder(\"float32\", name=\"input_filt4\")\n",
    "input_filt5_ = tf.placeholder(\"float32\", name=\"input_filt5\")\n",
    "input_filt6_ = tf.placeholder(\"float32\", name=\"input_filt6\")\n",
    "input_filt7_ = tf.placeholder(\"float32\", name=\"input_filt7\")\n",
    "input_filt8_ = tf.placeholder(\"float32\", name=\"input_filt8\")\n",
    "input_filt9_ = tf.placeholder(\"float32\", name=\"input_filt9\")\n",
    "input_filt10_ = tf.placeholder(\"float32\", name=\"input_filt10\")\n",
    "input_filt11_ = tf.placeholder(\"float32\", name=\"input_filt11\")\n",
    "input_filt12_ = tf.placeholder(\"float32\", name=\"input_filt12\")\n",
    "input_filt13_ = tf.placeholder(\"float32\", name=\"input_filt13\")\n",
    "input_filt14_ = tf.placeholder(\"float32\", name=\"input_filt14\")\n",
    "input_filt15_ = tf.placeholder(\"float32\", name=\"input_filt15\")\n",
    "input_filt16_ = tf.placeholder(\"float32\", name=\"input_filt16\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TO HAND ADJUST INITIALIZATIONS, USE PARAMS BELOW\n",
    "# NB: THIS IS FOR PLAYING AROUND/INSPECTION ONLY. \n",
    "# DURING ACTUAL TRAINING, VARIABLES ARE RANDOMLY INITIALIZED (see helper functions)\n",
    "\n",
    "b1g = [0.0]\n",
    "b2g = [0.0]\n",
    "b11g = [0.0]\n",
    "\n",
    "b1copyg = [0.0]\n",
    "b2copyg = [0.0]\n",
    "b11copyg = [0.0]\n",
    "\n",
    "b1b = 0.0\n",
    "b2b = -10.0\n",
    "b11b = -0.0 \n",
    "\n",
    "b1a1 = 0.0\n",
    "b2a1 = 0.0\n",
    "b11a1 = 1.0\n",
    "\n",
    "a1g = [0.0]\n",
    "\n",
    "a1b1copy = 5.0\n",
    "a1b2copy = 0.0\n",
    "\n",
    "\n",
    "bip1_gc_initial=np.zeros([no_bipolars, no_gc])\n",
    "for bip_i in range(4, 12):\n",
    "    for gc_i in range(no_gc):\n",
    "        bip1_gc_initial[bip_i, gc_i]=b1g[gc_i]\n",
    "bip1_gc_initial=bip1_gc_initial.astype(float32)\n",
    "\n",
    "bip2_gc_initial=np.zeros([no_bipolars, no_gc])\n",
    "for bip_i in range(4, 12):\n",
    "    for gc_i in range(no_gc):\n",
    "        bip2_gc_initial[bip_i, gc_i]=b2g[gc_i]\n",
    "bip2_gc_initial=bip2_gc_initial.astype(float32)\n",
    "\n",
    "\n",
    "bip11_gc_initial=np.zeros([no_bipolars, no_gc])\n",
    "for bip_i in range(4, 12):\n",
    "    for gc_i in range(no_gc):\n",
    "        bip11_gc_initial[bip_i, gc_i]=b11g[gc_i]\n",
    "bip11_gc_initial=bip11_gc_initial.astype(float32)\n",
    "\n",
    "\n",
    "\n",
    "bip1_copy_gc_initial=np.zeros([no_bipolars, no_gc])\n",
    "for bip_i in range(4, 12):\n",
    "    for gc_i in range(no_gc):\n",
    "        bip1_copy_gc_initial[bip_i, gc_i]=b1copyg[gc_i]\n",
    "bip1_copy_gc_initial=bip1_copy_gc_initial.astype(float32)\n",
    "\n",
    "bip2_copy_gc_initial=np.zeros([no_bipolars, no_gc])\n",
    "for bip_i in range(8):\n",
    "    for gc_i in range(no_gc):\n",
    "        bip2_copy_gc_initial[bip_i, gc_i]=b2copyg[gc_i]\n",
    "bip2_copy_gc_initial=bip2_copy_gc_initial.astype(float32)\n",
    "\n",
    "\n",
    "bip11_copy_gc_initial=np.zeros([no_bipolars, no_gc])\n",
    "for bip_i in range(8):\n",
    "    for gc_i in range(no_gc):\n",
    "        bip11_gc_initial[bip_i, gc_i]=b11copyg[gc_i]\n",
    "bip11_copy_gc_initial=bip11_copy_gc_initial.astype(float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "am1_b1copy_initial=np.zeros([no_am1, no_bipolars])\n",
    "for am_i in range(1):\n",
    "    for bip_i in range(4, 12):\n",
    "        am1_b1copy_initial[bip_i-4, bip_i]=a1b1copy\n",
    "am1_b1copy_initial=am1_b1copy_initial.astype(float32)\n",
    "\n",
    "am1_b2copy_initial=np.zeros([no_am1, no_bipolars])\n",
    "for am_i in range(3):\n",
    "    for bip_i in range(8):\n",
    "        am1_b2copy_initial[am_i, bip_i]=a1b2copy\n",
    "am1_b2copy_initial=am1_b2copy_initial.astype(float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "am1_gc_initial=np.zeros([no_am1, no_gc])\n",
    "for am_i in range(3):\n",
    "    for gc_i in range(no_gc):\n",
    "        am1_gc_initial[am_i, gc_i]=a1g[gc_i]\n",
    "am1_gc_initial=am1_gc_initial.astype(float32)\n",
    "\n",
    "am1_gc_train_initial=np.zeros([no_am1, no_gc])\n",
    "for am_i in range(no_am1):\n",
    "    am1_gc_train_initial[am_i, 0]=0.0*np.random.uniform()\n",
    "am1_gc_train_initial=am1_gc_train_initial.astype(float32)\n",
    "\n",
    "\n",
    "bip1_am1_initial=np.zeros([no_bipolars, no_am1])\n",
    "for am_i in range(1):\n",
    "    for bip_i in range(8, 16):\n",
    "        bip1_am1_initial[bip_i, am_i]=b1a1\n",
    "bip1_am1_initial=bip1_am1_initial.astype(float32)\n",
    "\n",
    "bip2_am1_initial=np.zeros([no_bipolars, no_am1])\n",
    "for am_i in range(1):\n",
    "    for bip_i in range(8, 16):\n",
    "        bip2_am1_initial[bip_i, am_i]=b2a1\n",
    "bip2_am1_initial=bip2_am1_initial.astype(float32)\n",
    "\n",
    "bip11_am1_initial=np.zeros([no_bipolars, no_am1])\n",
    "for bip_i in range(4, 12):\n",
    "#     for bip_i in range(4, 12):\n",
    "    bip11_am1_initial[bip_i-1, bip_i-4]=b11a1\n",
    "    bip11_am1_initial[bip_i, bip_i-4]=b11a1\n",
    "    bip11_am1_initial[bip_i+1, bip_i-4]=b11a1\n",
    "bip11_am1_initial=bip11_am1_initial.astype(float32)\n",
    "\n",
    "\n",
    "gc_stretch_initial=1.0*np.ones([no_gc, 1])\n",
    "gc_stretch_initial=gc_stretch_initial.astype(float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]]\n"
     ]
    }
   ],
   "source": [
    "# load synaptic weight masks (derived from IPL address book)\n",
    "\n",
    "maskpath='/home/ubuntu/Notebooks/alpha_realdata_syn_weight_masks_2D_30pix.mat'\n",
    "mask=sio.loadmat(maskpath)\n",
    "\n",
    "bip1_gc_mask = mask['bip1_gc_mask']\n",
    "bip2_gc_mask = mask['bip2_gc_mask']\n",
    "bip11_gc_mask = mask['bip11_gc_mask']\n",
    "\n",
    "bip1_am1_mask = mask['bip1_am1_mask']\n",
    "bip2_am1_mask = mask['bip2_am1_mask']\n",
    "bip11_am1_mask = mask['bip11_am1_mask']\n",
    "\n",
    "am1_gc_mask = mask['am1_gc_mask']+1.0\n",
    "\n",
    "print(am1_gc_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# INITIALIZE SYNAPTIC WEIGHT AND BIAS VARIABLES\n",
    "\n",
    "# bip1_gc_syn=tf.math.multiply(zero_synapse_var([no_bipolars, no_gc], bip1_gc_initial), bip1_gc_mask) #20201215\n",
    "bip1_gc_syn=tf.math.multiply(synapse_var([no_bipolars, no_gc], bip1_gc_initial), bip1_gc_mask)\n",
    "\n",
    "bip2_gc_syn=tf.math.multiply(linear_synapse_var([no_bipolars, no_gc], bip2_gc_initial), bip2_gc_mask)\n",
    "# bip2_gc_syn=tf.math.multiply(zero_synapse_var([no_bipolars, no_gc], bip2_gc_initial), bip2_gc_mask)\n",
    "\n",
    "bip11_gc_syn=tf.math.multiply(zero_synapse_var([no_bipolars, no_gc], bip11_gc_initial), bip11_gc_mask)\n",
    "\n",
    "bip1_copy_gc_syn=tf.math.multiply(zero_synapse_var([no_bipolars, no_gc], bip1_copy_gc_initial), bip1_gc_mask)#20201215\n",
    "\n",
    "bip2_copy_gc_syn=tf.math.multiply(zero_synapse_var([no_bipolars, no_gc], bip2_copy_gc_initial), bip2_gc_mask)\n",
    "\n",
    "bip11_copy_gc_syn=tf.math.multiply(bg_synapse_var([no_bipolars, no_gc], bip11_copy_gc_initial), bip11_gc_mask)\n",
    "\n",
    "bip1_am1_syn = tf.math.multiply(ba_synapse_var([no_bipolars, no_am1], bip1_am1_initial), bip1_am1_mask)\n",
    "bip2_am1_syn = tf.math.multiply(ba_synapse_var([no_bipolars, no_am1], bip2_am1_initial), bip2_am1_mask)\n",
    "bip11_am1_syn = tf.math.multiply(ba_synapse_var([no_bipolars, no_am1], bip11_am1_initial), bip11_am1_mask)\n",
    "\n",
    "\n",
    "\n",
    "am1_gc_syn = tf.math.multiply(ag_synapse_var([no_am1, no_gc], am1_gc_initial, am1_gc_train_initial), am1_gc_mask)\n",
    "\n",
    "am1_b1copy_syn = ab_synapse_var([no_am1, no_bipolars], am1_b1copy_initial)\n",
    "\n",
    "am1_b2copy_syn = zero_synapse_var([no_am1, no_bipolars], am1_b2copy_initial)\n",
    "\n",
    "\n",
    "b1_bias_initial=b1b*np.ones([no_bipolars, 1])\n",
    "b1_bias_initial=b1_bias_initial.astype(float32)\n",
    "\n",
    "b2_bias_initial=b2b*np.ones([no_bipolars, 1])\n",
    "b2_bias_initial=b2_bias_initial.astype(float32)\n",
    "\n",
    "\n",
    "b11_bias_initial=b11b*np.ones([no_bipolars, 1])\n",
    "b11_bias_initial=b11_bias_initial.astype(float32)\n",
    "\n",
    "\n",
    "\n",
    "b1_bias=bias_var([no_bipolars, 1], b1_bias_initial)\n",
    "b2_bias=bias_var([no_bipolars, 1], b2_bias_initial)\n",
    "\n",
    "b11_bias=bias_var([no_bipolars, 1], b11_bias_initial)\n",
    "\n",
    "\n",
    "am1_bias_initial=-50.0*np.ones([no_am1, 1])\n",
    "am1_bias_initial=am1_bias_initial.astype(float32)\n",
    "am1_bias=bias_var([no_am1, 1], am1_bias_initial)\n",
    "\n",
    "gc_bias_initial = np.array([[0.0]])\n",
    "gc_bias_initial=gc_bias_initial.astype(float32)\n",
    "gc_bias=bias_var([no_gc, 1], gc_bias_initial)\n",
    "\n",
    "gc_stretch=synapse_var([no_gc, 1], gc_stretch_initial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#INITIALIZE BIPOLAR CELL TEMPORAL KERNELS. three parameter sets modeled after biological bipolar cells.\n",
    "\n",
    "# ## normal filt\n",
    "# f1b1=32.4739\n",
    "# f2b1=-36.2776\n",
    "# f3b1=9.2522\n",
    "# f4b1=-24.8925\n",
    "# f5b1=-3.3185\n",
    "# f6b1=3.4590\n",
    "# f7b1=1.8170\n",
    "# f8b1=-2.8191\n",
    "# f9b1=0.2779\n",
    "# f10b1=-0.0095\n",
    "# f11b1=0.0742\n",
    "# f12b1=0.5002\n",
    "# f13b1=-0.8313\n",
    "# f14b1=1.0948\n",
    "# f15b1=-0.7449\n",
    "# f16b1=0.6164\n",
    "\n",
    "# f1b2=32.4739\n",
    "# f2b2=-36.2776\n",
    "# f3b2=9.2522\n",
    "# f4b2=-24.8925\n",
    "# f5b2=-3.3185\n",
    "# f6b2=3.4590\n",
    "# f7b2=1.8170\n",
    "# f8b2=-2.8191\n",
    "# f9b2=0.2779\n",
    "# f10b2=-0.0095\n",
    "# f11b2=0.0742\n",
    "# f12b2=0.5002\n",
    "# f13b2=-0.8313\n",
    "# f14b2=1.0948\n",
    "# f15b2=-0.7449\n",
    "# f16b2=0.6164\n",
    "\n",
    "# f1b11=-32.4739\n",
    "# f2b11=36.2776\n",
    "# f3b11=-9.2522\n",
    "# f4b11=24.8925\n",
    "# f5b11=3.3185\n",
    "# f6b11=-3.4590\n",
    "# f7b11=-1.8170\n",
    "# f8b11=2.8191\n",
    "# f9b11=-0.2779\n",
    "# f10b11=0.0095\n",
    "# f11b11=-0.0742\n",
    "# f12b11=-0.5002\n",
    "# f13b11=0.8313\n",
    "# f14b11=-1.0948\n",
    "# f15b11=0.7449\n",
    "# f16b11=-0.6164\n",
    "\n",
    "## slow filt\n",
    "f1b1=8.9629\n",
    "f2b1=-14.8934\n",
    "f3b1=-3.7342\n",
    "f4b1=-2.4524\n",
    "f5b1=-2.2385\n",
    "f6b1=4.8663\n",
    "f7b1=1.0306\n",
    "f8b1=-0.1179\n",
    "f9b1=-0.1026\n",
    "f10b1=0.1568\n",
    "f11b1=0.1731\n",
    "f12b1=0.1854\n",
    "f13b1=0.0526\n",
    "f14b1=-0.0769\n",
    "f15b1=-0.0104\n",
    "f16b1=-0.0069\n",
    "\n",
    "f1b2=8.9629\n",
    "f2b2=-14.8934\n",
    "f3b2=-3.7342\n",
    "f4b2=-2.4524\n",
    "f5b2=-2.2385\n",
    "f6b2=4.8663\n",
    "f7b2=1.0306\n",
    "f8b2=-0.1179\n",
    "f9b2=-0.1026\n",
    "f10b2=0.1568\n",
    "f11b2=0.1731\n",
    "f12b2=0.1854\n",
    "f13b2=0.0526\n",
    "f14b2=-0.0769\n",
    "f15b2=-0.0104\n",
    "f16b2=-0.0069\n",
    "\n",
    "f1b11=-8.9629\n",
    "f2b11=14.8934\n",
    "f3b11=3.7342\n",
    "f4b11=2.4524\n",
    "f5b11=2.2385\n",
    "f6b11=-4.8663\n",
    "f7b11=-1.0306\n",
    "f8b11=0.1179\n",
    "f9b11=0.1026\n",
    "f10b11=-0.1568\n",
    "f11b11=-0.1731\n",
    "f12b11=-0.1854\n",
    "f13b11=-0.0526\n",
    "f14b11=0.0769\n",
    "f15b11=0.0104\n",
    "f16b11=0.0069\n",
    "\n",
    "\n",
    "# ## fast filt\n",
    "# f1b1=3.3739\n",
    "# f2b1=-3.0542\n",
    "# f3b1=4.9315\n",
    "# f4b1=-7.0294\n",
    "# f5b1=1.8001\n",
    "# f6b1=-5.5280\n",
    "# f7b1=-1.2573\n",
    "# f8b1=0.5046\n",
    "# f9b1=0.2822\n",
    "# f10b1=-0.1797\n",
    "# f11b1=-0.0894\n",
    "# f12b1=-0.6905\n",
    "# f13b1=0.0932\n",
    "# f14b1=-0.6807\n",
    "# f15b1=0.4166\n",
    "# f16b1=-0.8054\n",
    "\n",
    "\n",
    "# f1b2=3.3739\n",
    "# f2b2=-3.0542\n",
    "# f3b2=4.9315\n",
    "# f4b2=-7.0294\n",
    "# f5b2=1.8001\n",
    "# f6b2=-5.5280\n",
    "# f7b2=-1.2573\n",
    "# f8b2=0.5046\n",
    "# f9b2=0.2822\n",
    "# f10b2=-0.1797\n",
    "# f11b2=-0.0894\n",
    "# f12b2=-0.6905\n",
    "# f13b2=0.0932\n",
    "# f14b2=-0.6807\n",
    "# f15b2=0.4166\n",
    "# f16b2=-0.8054\n",
    "\n",
    "# f1b11=-3.3739\n",
    "# f2b11=3.0542\n",
    "# f3b11=-4.9315\n",
    "# f4b11=7.0294\n",
    "# f5b11=-1.8001\n",
    "# f6b11=5.5280\n",
    "# f7b11=1.2573\n",
    "# f8b11=-0.5046\n",
    "# f9b11=-0.2822\n",
    "# f10b11=0.1797\n",
    "# f11b11=0.0894\n",
    "# f12b11=0.6905\n",
    "# f13b11=-0.0932\n",
    "# f14b11=0.6807\n",
    "# f15b11=-0.4166\n",
    "# f16b11=0.8054\n",
    "\n",
    "f1b1_syn = fb_synapse_var([1, no_filters_per_bc_type], f1b1*np.ones([1, no_filters_per_bc_type]))\n",
    "f2b1_syn = fb_synapse_var([1, no_filters_per_bc_type], f2b1*np.ones([1, no_filters_per_bc_type]))\n",
    "f3b1_syn = fb_synapse_var([1, no_filters_per_bc_type], f3b1*np.ones([1, no_filters_per_bc_type]))\n",
    "f4b1_syn = fb_synapse_var([1, no_filters_per_bc_type], f4b1*np.ones([1, no_filters_per_bc_type]))\n",
    "f5b1_syn = fb_synapse_var([1, no_filters_per_bc_type], f5b1*np.ones([1, no_filters_per_bc_type]))\n",
    "f6b1_syn = fb_synapse_var([1, no_filters_per_bc_type], f6b1*np.ones([1, no_filters_per_bc_type]))\n",
    "f7b1_syn = fb_synapse_var([1, no_filters_per_bc_type], f7b1*np.ones([1, no_filters_per_bc_type]))\n",
    "f8b1_syn = fb_synapse_var([1, no_filters_per_bc_type], f8b1*np.ones([1, no_filters_per_bc_type]))\n",
    "f9b1_syn = fb_synapse_var([1, no_filters_per_bc_type], f9b1*np.ones([1, no_filters_per_bc_type]))\n",
    "f10b1_syn = fb_synapse_var([1, no_filters_per_bc_type], f10b1*np.ones([1, no_filters_per_bc_type]))\n",
    "f11b1_syn = fb_synapse_var([1, no_filters_per_bc_type], f11b1*np.ones([1, no_filters_per_bc_type]))\n",
    "f12b1_syn = fb_synapse_var([1, no_filters_per_bc_type], f12b1*np.ones([1, no_filters_per_bc_type]))\n",
    "f13b1_syn = fb_synapse_var([1, no_filters_per_bc_type], f13b1*np.ones([1, no_filters_per_bc_type]))\n",
    "f14b1_syn = fb_synapse_var([1, no_filters_per_bc_type], f14b1*np.ones([1, no_filters_per_bc_type]))\n",
    "f15b1_syn = fb_synapse_var([1, no_filters_per_bc_type], f15b1*np.ones([1, no_filters_per_bc_type]))\n",
    "f16b1_syn = fb_synapse_var([1, no_filters_per_bc_type], f16b1*np.ones([1, no_filters_per_bc_type]))\n",
    "\n",
    "f1b2_syn = fb_synapse_var([1, no_filters_per_bc_type], f1b2*np.ones([1, no_filters_per_bc_type]))\n",
    "f2b2_syn = fb_synapse_var([1, no_filters_per_bc_type], f2b2*np.ones([1, no_filters_per_bc_type]))\n",
    "f3b2_syn = fb_synapse_var([1, no_filters_per_bc_type], f3b2*np.ones([1, no_filters_per_bc_type]))\n",
    "f4b2_syn = fb_synapse_var([1, no_filters_per_bc_type], f4b2*np.ones([1, no_filters_per_bc_type]))\n",
    "f5b2_syn = fb_synapse_var([1, no_filters_per_bc_type], f5b2*np.ones([1, no_filters_per_bc_type]))\n",
    "f6b2_syn = fb_synapse_var([1, no_filters_per_bc_type], f6b2*np.ones([1, no_filters_per_bc_type]))\n",
    "f7b2_syn = fb_synapse_var([1, no_filters_per_bc_type], f7b2*np.ones([1, no_filters_per_bc_type]))\n",
    "f8b2_syn = fb_synapse_var([1, no_filters_per_bc_type], f8b2*np.ones([1, no_filters_per_bc_type]))\n",
    "f9b2_syn = fb_synapse_var([1, no_filters_per_bc_type], f9b2*np.ones([1, no_filters_per_bc_type]))\n",
    "f10b2_syn = fb_synapse_var([1, no_filters_per_bc_type], f10b2*np.ones([1, no_filters_per_bc_type]))\n",
    "f11b2_syn = fb_synapse_var([1, no_filters_per_bc_type], f11b2*np.ones([1, no_filters_per_bc_type]))\n",
    "f12b2_syn = fb_synapse_var([1, no_filters_per_bc_type], f12b2*np.ones([1, no_filters_per_bc_type]))\n",
    "f13b2_syn = fb_synapse_var([1, no_filters_per_bc_type], f13b2*np.ones([1, no_filters_per_bc_type]))\n",
    "f14b2_syn = fb_synapse_var([1, no_filters_per_bc_type], f14b2*np.ones([1, no_filters_per_bc_type]))\n",
    "f15b2_syn = fb_synapse_var([1, no_filters_per_bc_type], f15b2*np.ones([1, no_filters_per_bc_type]))\n",
    "f16b2_syn = fb_synapse_var([1, no_filters_per_bc_type], f16b2*np.ones([1, no_filters_per_bc_type]))\n",
    "\n",
    "f1b11_syn = fb_synapse_var([1, no_filters_per_bc_type], f1b11*np.ones([1, no_filters_per_bc_type]))\n",
    "f2b11_syn = fb_synapse_var([1, no_filters_per_bc_type], f2b11*np.ones([1, no_filters_per_bc_type]))\n",
    "f3b11_syn = fb_synapse_var([1, no_filters_per_bc_type], f3b11*np.ones([1, no_filters_per_bc_type]))\n",
    "f4b11_syn = fb_synapse_var([1, no_filters_per_bc_type], f4b11*np.ones([1, no_filters_per_bc_type]))\n",
    "f5b11_syn = fb_synapse_var([1, no_filters_per_bc_type], f5b11*np.ones([1, no_filters_per_bc_type]))\n",
    "f6b11_syn = fb_synapse_var([1, no_filters_per_bc_type], f6b11*np.ones([1, no_filters_per_bc_type]))\n",
    "f7b11_syn = fb_synapse_var([1, no_filters_per_bc_type], f7b11*np.ones([1, no_filters_per_bc_type]))\n",
    "f8b11_syn = fb_synapse_var([1, no_filters_per_bc_type], f8b11*np.ones([1, no_filters_per_bc_type]))\n",
    "f9b11_syn = fb_synapse_var([1, no_filters_per_bc_type], f9b11*np.ones([1, no_filters_per_bc_type]))\n",
    "f10b11_syn = fb_synapse_var([1, no_filters_per_bc_type], f10b11*np.ones([1, no_filters_per_bc_type]))\n",
    "f11b11_syn = fb_synapse_var([1, no_filters_per_bc_type], f11b11*np.ones([1, no_filters_per_bc_type]))\n",
    "f12b11_syn = fb_synapse_var([1, no_filters_per_bc_type], f12b11*np.ones([1, no_filters_per_bc_type]))\n",
    "f13b11_syn = fb_synapse_var([1, no_filters_per_bc_type], f13b11*np.ones([1, no_filters_per_bc_type]))\n",
    "f14b11_syn = fb_synapse_var([1, no_filters_per_bc_type], f14b11*np.ones([1, no_filters_per_bc_type]))\n",
    "f15b11_syn = fb_synapse_var([1, no_filters_per_bc_type], f15b11*np.ones([1, no_filters_per_bc_type]))\n",
    "f16b11_syn = fb_synapse_var([1, no_filters_per_bc_type], f16b11*np.ones([1, no_filters_per_bc_type]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dur=data_duration\n",
    "batchsize=32\n",
    "no_bip=no_bipolars\n",
    "\n",
    "batchsize_ = tf.placeholder(\"int32\", name=\"batch_size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# DEFINE ANN GRAPH\n",
    "\n",
    "@tf.function\n",
    "def biplayer(f1_input, f2_input, f3_input, f4_input, f5_input, f6_input, f7_input, f8_input, \n",
    "                f9_input, f10_input, f11_input, f12_input, f13_input, f14_input, f15_input, f16_input, \n",
    "                f1b_syn, f2b_syn, f3b_syn, f4b_syn, f5b_syn, f6b_syn, f7b_syn, f8b_syn, f9b_syn, f10b_syn, \n",
    "                f11b_syn, f12b_syn, f13b_syn, f14b_syn, f15b_syn, f16b_syn, b_bias, bip_gc_syn, no_bip, no_gc, batchsize, dur): #, no_bip, no_filt, batchsize, dur):\n",
    "    \n",
    "    f1b_syn=tf.broadcast_to(tf.reshape(f1b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f2b_syn=tf.broadcast_to(tf.reshape(f2b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f3b_syn=tf.broadcast_to(tf.reshape(f3b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f4b_syn=tf.broadcast_to(tf.reshape(f4b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f5b_syn=tf.broadcast_to(tf.reshape(f5b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f6b_syn=tf.broadcast_to(tf.reshape(f6b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f7b_syn=tf.broadcast_to(tf.reshape(f7b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f8b_syn=tf.broadcast_to(tf.reshape(f8b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f9b_syn=tf.broadcast_to(tf.reshape(f9b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f10b_syn=tf.broadcast_to(tf.reshape(f10b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f11b_syn=tf.broadcast_to(tf.reshape(f11b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f12b_syn=tf.broadcast_to(tf.reshape(f12b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f13b_syn=tf.broadcast_to(tf.reshape(f13b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f14b_syn=tf.broadcast_to(tf.reshape(f14b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f15b_syn=tf.broadcast_to(tf.reshape(f15b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f16b_syn=tf.broadcast_to(tf.reshape(f16b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    \n",
    "    f1_input=tf.tile(f1_input, [no_bipolar_rows, 1, 1])\n",
    "    f2_input=tf.tile(f2_input, [no_bipolar_rows, 1, 1])\n",
    "    f3_input=tf.tile(f3_input, [no_bipolar_rows, 1, 1])\n",
    "    f4_input=tf.tile(f4_input, [no_bipolar_rows, 1, 1])\n",
    "    f5_input=tf.tile(f5_input, [no_bipolar_rows, 1, 1])\n",
    "    f6_input=tf.tile(f6_input, [no_bipolar_rows, 1, 1])\n",
    "    f7_input=tf.tile(f7_input, [no_bipolar_rows, 1, 1])\n",
    "    f8_input=tf.tile(f8_input, [no_bipolar_rows, 1, 1])\n",
    "    f9_input=tf.tile(f9_input, [no_bipolar_rows, 1, 1])\n",
    "    f10_input=tf.tile(f10_input, [no_bipolar_rows, 1, 1])\n",
    "    f11_input=tf.tile(f11_input, [no_bipolar_rows, 1, 1])\n",
    "    f12_input=tf.tile(f12_input, [no_bipolar_rows, 1, 1])\n",
    "    f13_input=tf.tile(f13_input, [no_bipolar_rows, 1, 1])\n",
    "    f14_input=tf.tile(f14_input, [no_bipolar_rows, 1, 1])\n",
    "    f15_input=tf.tile(f15_input, [no_bipolar_rows, 1, 1])\n",
    "    f16_input=tf.tile(f16_input, [no_bipolar_rows, 1, 1])\n",
    "\n",
    "    \n",
    "    b_input = tf.add_n([tf.math.multiply(f1b_syn, f1_input), tf.math.multiply(f2b_syn, f2_input), tf.math.multiply(f3b_syn, f3_input),\\\n",
    "                        tf.math.multiply(f4b_syn, f4_input), tf.math.multiply(f5b_syn, f5_input), tf.math.multiply(f6b_syn, f6_input),\\\n",
    "                        tf.math.multiply(f7b_syn, f7_input), tf.math.multiply(f8b_syn, f8_input), tf.math.multiply(f9b_syn, f9_input),\\\n",
    "                        tf.math.multiply(f10b_syn, f10_input), tf.math.multiply(f11b_syn, f11_input), tf.math.multiply(f12b_syn, f12_input), \\\n",
    "                        tf.math.multiply(f13b_syn, f13_input), tf.math.multiply(f14b_syn, f14_input), tf.math.multiply(f15b_syn, f15_input), tf.math.multiply(f16b_syn, f16_input)])\n",
    "    b_bias_expand=tf.broadcast_to(tf.reshape(b_bias, [no_bipolars, 1, 1]), [no_bipolars, batchsize_, dur], name=\"bro1\")\n",
    "    b_bias_add=tf.add(b_input, b_bias_expand)\n",
    "    del b_input\n",
    "    del b_bias_expand\n",
    "    b_relu=tf.nn.relu(b_bias_add)\n",
    "    del b_bias_add\n",
    "    bip_layer_expand=tf.broadcast_to(tf.reshape(b_relu, [no_bip, batchsize_, 1, dur]), [no_bip, batchsize_, no_gc, dur], name=\"bro2\")\n",
    "    \n",
    "    bg_syn_expand=tf.broadcast_to(tf.reshape(tf.abs(bip_gc_syn), [no_bip, 1, no_gc, 1]), [no_bip, batchsize_, no_gc, dur], name=\"bro3\")\n",
    "    bg_mult=tf.math.multiply(bip_layer_expand, bg_syn_expand)\n",
    "    del bip_layer_expand \n",
    "    del bg_syn_expand\n",
    "    bg_sum=tf.reduce_sum(bg_mult, 0)\n",
    "    del bg_mult\n",
    "    return b_relu, bg_sum\n",
    "    \n",
    "@tf.function\n",
    "def linear_biplayer(f1_input, f2_input, f3_input, f4_input, f5_input, f6_input, f7_input, f8_input, \n",
    "                f9_input, f10_input, f11_input, f12_input, f13_input, f14_input, f15_input, f16_input, \n",
    "                f1b_syn, f2b_syn, f3b_syn, f4b_syn, f5b_syn, f6b_syn, f7b_syn, f8b_syn, f9b_syn, f10b_syn, \n",
    "                f11b_syn, f12b_syn, f13b_syn, f14b_syn, f15b_syn, f16b_syn, b_bias, bip_gc_syn, no_bip, no_gc, batchsize, dur): #, no_bip, no_filt, batchsize, dur):\n",
    "    \n",
    "    f1b_syn=tf.broadcast_to(tf.reshape(f1b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur], name='brosyn1')\n",
    "    f2b_syn=tf.broadcast_to(tf.reshape(f2b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f3b_syn=tf.broadcast_to(tf.reshape(f3b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f4b_syn=tf.broadcast_to(tf.reshape(f4b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f5b_syn=tf.broadcast_to(tf.reshape(f5b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f6b_syn=tf.broadcast_to(tf.reshape(f6b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f7b_syn=tf.broadcast_to(tf.reshape(f7b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f8b_syn=tf.broadcast_to(tf.reshape(f8b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f9b_syn=tf.broadcast_to(tf.reshape(f9b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f10b_syn=tf.broadcast_to(tf.reshape(f10b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f11b_syn=tf.broadcast_to(tf.reshape(f11b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f12b_syn=tf.broadcast_to(tf.reshape(f12b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f13b_syn=tf.broadcast_to(tf.reshape(f13b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f14b_syn=tf.broadcast_to(tf.reshape(f14b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f15b_syn=tf.broadcast_to(tf.reshape(f15b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f16b_syn=tf.broadcast_to(tf.reshape(f16b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    \n",
    "    f1_input=tf.tile(f1_input, [no_bipolar_rows, 1, 1])\n",
    "    f2_input=tf.tile(f2_input, [no_bipolar_rows, 1, 1])\n",
    "    f3_input=tf.tile(f3_input, [no_bipolar_rows, 1, 1])\n",
    "    f4_input=tf.tile(f4_input, [no_bipolar_rows, 1, 1])\n",
    "    f5_input=tf.tile(f5_input, [no_bipolar_rows, 1, 1])\n",
    "    f6_input=tf.tile(f6_input, [no_bipolar_rows, 1, 1])\n",
    "    f7_input=tf.tile(f7_input, [no_bipolar_rows, 1, 1])\n",
    "    f8_input=tf.tile(f8_input, [no_bipolar_rows, 1, 1])\n",
    "    f9_input=tf.tile(f9_input, [no_bipolar_rows, 1, 1])\n",
    "    f10_input=tf.tile(f10_input, [no_bipolar_rows, 1, 1])\n",
    "    f11_input=tf.tile(f11_input, [no_bipolar_rows, 1, 1])\n",
    "    f12_input=tf.tile(f12_input, [no_bipolar_rows, 1, 1])\n",
    "    f13_input=tf.tile(f13_input, [no_bipolar_rows, 1, 1])\n",
    "    f14_input=tf.tile(f14_input, [no_bipolar_rows, 1, 1])\n",
    "    f15_input=tf.tile(f15_input, [no_bipolar_rows, 1, 1])\n",
    "    f16_input=tf.tile(f16_input, [no_bipolar_rows, 1, 1])\n",
    "\n",
    "    \n",
    "\n",
    "    b_input = tf.add_n([tf.math.multiply(f1b_syn, f1_input), tf.math.multiply(f2b_syn, f2_input), tf.math.multiply(f3b_syn, f3_input),\\\n",
    "                        tf.math.multiply(f4b_syn, f4_input), tf.math.multiply(f5b_syn, f5_input), tf.math.multiply(f6b_syn, f6_input),\\\n",
    "                        tf.math.multiply(f7b_syn, f7_input), tf.math.multiply(f8b_syn, f8_input), tf.math.multiply(f9b_syn, f9_input),\\\n",
    "                        tf.math.multiply(f10b_syn, f10_input), tf.math.multiply(f11b_syn, f11_input), tf.math.multiply(f12b_syn, f12_input), \\\n",
    "                        tf.math.multiply(f13b_syn, f13_input), tf.math.multiply(f14b_syn, f14_input), tf.math.multiply(f15b_syn, f15_input), tf.math.multiply(f16b_syn, f16_input)])\n",
    "    b_bias_expand=tf.broadcast_to(tf.reshape(b_bias, [no_bipolars, 1, 1]), [no_bipolars, batchsize_, dur], name=\"bro1\")\n",
    "    b_bias_add=tf.add(b_input, b_bias_expand)\n",
    "    del b_input\n",
    "    del b_bias_expand\n",
    "    b_relu=b_bias_add\n",
    "    del b_bias_add\n",
    "    bip_layer_expand=tf.broadcast_to(tf.reshape(b_relu, [no_bip, batchsize_, 1, dur]), [no_bip, batchsize_, no_gc, dur], name=\"bro2\")\n",
    "\n",
    "    bg_syn_expand=tf.broadcast_to(tf.reshape(tf.abs(bip_gc_syn), [no_bip, 1, no_gc, 1]), [no_bip, batchsize_, no_gc, dur], name=\"bro3\")\n",
    "    bg_mult=tf.math.multiply(bip_layer_expand, bg_syn_expand)\n",
    "    del bip_layer_expand\n",
    "    del bg_syn_expand\n",
    "    bg_sum=tf.reduce_sum(bg_mult, 0)\n",
    "    del bg_mult\n",
    "    return b_relu, bg_sum\n",
    "    \n",
    "\n",
    "b1_relu, b1g_sum = biplayer(input_filt1_, input_filt2_, input_filt3_, input_filt4_, input_filt5_, input_filt6_, input_filt7_,\n",
    "                            input_filt8_, input_filt9_, input_filt10_, input_filt11_, input_filt12_, input_filt13_, \n",
    "                            input_filt14_, input_filt15_, input_filt16_, f1b1_syn, f2b1_syn, f3b1_syn, f4b1_syn, f5b1_syn,\n",
    "                            f6b1_syn, f7b1_syn, f8b1_syn, f9b1_syn, f10b1_syn, f11b1_syn, f12b1_syn, f13b1_syn, f14b1_syn, \n",
    "                            f15b1_syn, f16b1_syn, b1_bias, bip1_gc_syn, no_bip, no_gc, batchsize_, dur)\n",
    "\n",
    "b2_relu, b2g_sum = linear_biplayer(input_filt1_, input_filt2_, input_filt3_, input_filt4_, input_filt5_, input_filt6_, input_filt7_,\n",
    "                                   input_filt8_, input_filt9_, input_filt10_, input_filt11_, input_filt12_, input_filt13_, \n",
    "                                   input_filt14_, input_filt15_, input_filt16_, f1b2_syn, f2b2_syn, f3b2_syn, f4b2_syn, f5b2_syn,\n",
    "                                   f6b2_syn, f7b2_syn, f8b2_syn, f9b2_syn, f10b2_syn, f11b2_syn, f12b2_syn, f13b2_syn, f14b2_syn, \n",
    "                                   f15b2_syn, f16b2_syn, b2_bias, bip2_gc_syn, no_bip, no_gc, batchsize_, dur)\n",
    "\n",
    "\n",
    "b11_relu, b11g_sum = biplayer(input_filt1_, input_filt2_, input_filt3_, input_filt4_, input_filt5_, input_filt6_, input_filt7_,\n",
    "                              input_filt8_, input_filt9_, input_filt10_, input_filt11_, input_filt12_, input_filt13_, \n",
    "                              input_filt14_, input_filt15_, input_filt16_, f1b11_syn, f2b11_syn, f3b11_syn, f4b11_syn, f5b11_syn,\n",
    "                              f6b11_syn, f7b11_syn, f8b11_syn, f9b11_syn, f10b11_syn, f11b11_syn, f12b11_syn, f13b11_syn, f14b11_syn, \n",
    "                              f15b11_syn, f16b11_syn, b11_bias, bip11_gc_syn, no_bip, no_gc, batchsize_, dur)\n",
    "\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def bip_to_am_input(b_relu, bip_am_syn, no_bip, no_am, batchsize, dur):\n",
    "    bip_layer_am_expand=tf.broadcast_to(tf.reshape(b_relu, [no_bip, batchsize, 1, dur]), [no_bip, batchsize, no_am, dur], name=\"bro10\")\n",
    "    ba_syn_expand = tf.broadcast_to(tf.reshape(tf.abs(bip_am_syn), [no_bip, 1, no_am, 1]), [no_bip, batchsize, no_am, dur], name=\"bro11\")\n",
    "    del b_relu\n",
    "    ba_mult = tf.math.multiply(bip_layer_am_expand, ba_syn_expand)\n",
    "    del bip_layer_am_expand\n",
    "    del ba_syn_expand\n",
    "    ba_sum = tf.reduce_sum(ba_mult, 0)\n",
    "    return ba_mult, ba_sum\n",
    "\n",
    "b11a1_mult, b11a1_sum = bip_to_am_input(b11_relu, bip11_am1_syn, no_bip, no_am1, batchsize_, dur)\n",
    "\n",
    "am1_activation = tf.add_n([b11a1_sum])\n",
    "\n",
    "am1_bias_expand = tf.broadcast_to(am1_bias, [batchsize_, no_am1, dur], name=\"bro20\")\n",
    "\n",
    "am1_bias_add = tf.add(am1_activation, am1_bias_expand)\n",
    "del am1_bias_expand\n",
    "\n",
    "\n",
    "am1_output = tf.nn.relu(am1_bias_add)\n",
    "del am1_bias_add\n",
    "\n",
    "am1_reshape = tf.reshape(am1_output, [batchsize_, no_am1, 1, dur])\n",
    "am1_expand=tf.broadcast_to(am1_reshape, [batchsize_, no_am1, no_gc, dur], name=\"bro22\")\n",
    "am1g_syn_expand=tf.broadcast_to(tf.reshape(tf.abs(am1_gc_syn), [1, no_am1, no_gc, 1]), [batchsize_, no_am1, no_gc, dur], name=\"bro23\")\n",
    "am1g_mult=tf.math.multiply(am1_expand, am1g_syn_expand)\n",
    "del am1_expand\n",
    "del am1g_syn_expand\n",
    "am1g_sum=tf.reduce_sum(am1g_mult, 1)\n",
    "del am1g_mult\n",
    "\n",
    "\n",
    "\n",
    "am1_bcopy_expand=tf.broadcast_to(am1_reshape, [batchsize_, no_am1, no_bip, dur], name=\"bro26\")\n",
    "del am1_reshape\n",
    "\n",
    "@tf.function\n",
    "def biplayer_copy_input(f1_input, f2_input, f3_input, f4_input, f5_input, f6_input, f7_input, f8_input, \n",
    "                        f9_input, f10_input, f11_input, f12_input, f13_input, f14_input, f15_input, f16_input, \n",
    "                        f1b_syn, f2b_syn, f3b_syn, f4b_syn, f5b_syn, f6b_syn, f7b_syn, f8b_syn, f9b_syn, f10b_syn, \n",
    "                        f11b_syn, f12b_syn, f13b_syn, f14b_syn, f15b_syn, f16b_syn,am_bcopy_expand, am_bcopy_syn, b_bias, bip_copy_gc_syn, no_bip, no_am, no_gc, batchsize, dur):\n",
    "    \n",
    "    f1b_syn=tf.broadcast_to(tf.reshape(f1b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f2b_syn=tf.broadcast_to(tf.reshape(f2b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f3b_syn=tf.broadcast_to(tf.reshape(f3b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f4b_syn=tf.broadcast_to(tf.reshape(f4b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f5b_syn=tf.broadcast_to(tf.reshape(f5b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f6b_syn=tf.broadcast_to(tf.reshape(f6b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f7b_syn=tf.broadcast_to(tf.reshape(f7b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f8b_syn=tf.broadcast_to(tf.reshape(f8b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f9b_syn=tf.broadcast_to(tf.reshape(f9b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f10b_syn=tf.broadcast_to(tf.reshape(f10b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f11b_syn=tf.broadcast_to(tf.reshape(f11b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f12b_syn=tf.broadcast_to(tf.reshape(f12b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f13b_syn=tf.broadcast_to(tf.reshape(f13b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f14b_syn=tf.broadcast_to(tf.reshape(f14b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f15b_syn=tf.broadcast_to(tf.reshape(f15b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    f16b_syn=tf.broadcast_to(tf.reshape(f16b_syn, [no_filters_per_bc_type, 1, 1]), [no_bipolars, batchsize_, dur])\n",
    "    \n",
    "    f1_input=tf.tile(f1_input, [no_bipolar_rows, 1, 1])\n",
    "    f2_input=tf.tile(f2_input, [no_bipolar_rows, 1, 1])\n",
    "    f3_input=tf.tile(f3_input, [no_bipolar_rows, 1, 1])\n",
    "    f4_input=tf.tile(f4_input, [no_bipolar_rows, 1, 1])\n",
    "    f5_input=tf.tile(f5_input, [no_bipolar_rows, 1, 1])\n",
    "    f6_input=tf.tile(f6_input, [no_bipolar_rows, 1, 1])\n",
    "    f7_input=tf.tile(f7_input, [no_bipolar_rows, 1, 1])\n",
    "    f8_input=tf.tile(f8_input, [no_bipolar_rows, 1, 1])\n",
    "    f9_input=tf.tile(f9_input, [no_bipolar_rows, 1, 1])\n",
    "    f10_input=tf.tile(f10_input, [no_bipolar_rows, 1, 1])\n",
    "    f11_input=tf.tile(f11_input, [no_bipolar_rows, 1, 1])\n",
    "    f12_input=tf.tile(f12_input, [no_bipolar_rows, 1, 1])\n",
    "    f13_input=tf.tile(f13_input, [no_bipolar_rows, 1, 1])\n",
    "    f14_input=tf.tile(f14_input, [no_bipolar_rows, 1, 1])\n",
    "    f15_input=tf.tile(f15_input, [no_bipolar_rows, 1, 1])\n",
    "    f16_input=tf.tile(f16_input, [no_bipolar_rows, 1, 1])\n",
    "\n",
    "    \n",
    "    b_input = tf.add_n([tf.math.multiply(f1b_syn, f1_input), tf.math.multiply(f2b_syn, f2_input), tf.math.multiply(f3b_syn, f3_input),\\\n",
    "                        tf.math.multiply(f4b_syn, f4_input), tf.math.multiply(f5b_syn, f5_input), tf.math.multiply(f6b_syn, f6_input),\\\n",
    "                        tf.math.multiply(f7b_syn, f7_input), tf.math.multiply(f8b_syn, f8_input), tf.math.multiply(f9b_syn, f9_input),\\\n",
    "                        tf.math.multiply(f10b_syn, f10_input), tf.math.multiply(f11b_syn, f11_input), tf.math.multiply(f12b_syn, f12_input), \\\n",
    "                        tf.math.multiply(f13b_syn, f13_input), tf.math.multiply(f14b_syn, f14_input), tf.math.multiply(f15b_syn, f15_input), tf.math.multiply(f16b_syn, f16_input)])\n",
    "    ambcopy_syn_expand=tf.broadcast_to(tf.reshape(tf.abs(am_bcopy_syn), [1, no_am, no_bip, 1]), [batchsize, no_am, no_bip, dur], name=\"bro33\")\n",
    "    ambcopy_mult=tf.math.multiply(am_bcopy_expand, ambcopy_syn_expand)\n",
    "    del am_bcopy_expand\n",
    "    del ambcopy_syn_expand\n",
    "    ambcopy_sum1=tf.squeeze(tf.reduce_sum(ambcopy_mult, 1))\n",
    "    ambcopy_sum=tf.transpose(ambcopy_sum1, [1, 0, 2])\n",
    "    \n",
    "    del ambcopy_mult\n",
    "    del ambcopy_sum1\n",
    "    \n",
    "    b_bias_expand=tf.broadcast_to(tf.reshape(b_bias, [no_bipolars, 1, 1]), [no_bipolars, batchsize, dur], name=\"bro1\")\n",
    "    b_bias_add=tf.add_n([b_input,-1.0*ambcopy_sum, b_bias_expand])\n",
    "    del b_input\n",
    "    del b_bias_expand\n",
    "    b_relu=tf.nn.relu(b_bias_add)\n",
    "    del b_bias_add\n",
    "    bip_layer_expand=tf.broadcast_to(tf.reshape(b_relu, [no_bip, batchsize_, 1, dur]), [no_bip, batchsize, no_gc, dur], name=\"bro2\")\n",
    "\n",
    "    bg_syn_expand=tf.broadcast_to(tf.reshape(tf.abs(10.0*bip_copy_gc_syn), [no_bip, 1, no_gc, 1]), [no_bip, batchsize, no_gc, dur], name=\"bro3\")\n",
    "    bg_mult=tf.math.multiply(bip_layer_expand, bg_syn_expand)\n",
    "    del bip_layer_expand\n",
    "    del bg_syn_expand\n",
    "    bg_sum=tf.reduce_sum(bg_mult, 0)\n",
    "    del bg_mult\n",
    "    return b_relu, bg_sum\n",
    "\n",
    "b1copy_relu, b1copyg_sum = biplayer_copy_input(input_filt1_, input_filt2_, input_filt3_, input_filt4_, input_filt5_, input_filt6_, input_filt7_,\n",
    "                                  input_filt8_, input_filt9_, input_filt10_, input_filt11_, input_filt12_, input_filt13_, \n",
    "                                  input_filt14_, input_filt15_, input_filt16_, f1b1_syn, f2b1_syn, f3b1_syn, f4b1_syn, f5b1_syn,\n",
    "                                  f6b1_syn, f7b1_syn, f8b1_syn, f9b1_syn, f10b1_syn, f11b1_syn, f12b1_syn, f13b1_syn, f14b1_syn, \n",
    "                                  f15b1_syn, f16b1_syn, am1_bcopy_expand, am1_b1copy_syn, b1_bias, bip1_copy_gc_syn, no_bip, \n",
    "                                  no_am1, no_gc, batchsize_, dur)\n",
    "b2copy_relu, b2copyg_sum = biplayer_copy_input(input_filt1_, input_filt2_, input_filt3_, input_filt4_, input_filt5_, input_filt6_, input_filt7_,\n",
    "                                  input_filt8_, input_filt9_, input_filt10_, input_filt11_, input_filt12_, input_filt13_, \n",
    "                                  input_filt14_, input_filt15_, input_filt16_, f1b2_syn, f2b2_syn, f3b2_syn, f4b2_syn, f5b2_syn,\n",
    "                                  f6b2_syn, f7b2_syn, f8b2_syn, f9b2_syn, f10b2_syn, f11b2_syn, f12b2_syn, f13b2_syn, f14b2_syn, \n",
    "                                  f15b2_syn, f16b2_syn,am1_bcopy_expand, am1_b2copy_syn, b2_bias, bip2_copy_gc_syn, \n",
    "                                  no_bip, no_am1, no_gc, batchsize_, dur)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gc_activation=tf.add_n([b1copyg_sum, b2copyg_sum, b1g_sum, b2g_sum, -1.0*am1g_sum])\n",
    "\n",
    "\n",
    "pre_gc=tf.reshape(tf.squeeze(gcconv2d(tf.reshape(gc_activation, [batchsize_, dur, 1, 1]) , gen_gc_w)), [batchsize_, no_gc, dur])\n",
    "# pre_gc=gc_activation\n",
    "\n",
    "del b1copyg_sum\n",
    "del b2copyg_sum\n",
    "del b1g_sum\n",
    "del b2g_sum\n",
    "del am1g_sum\n",
    "\n",
    "\n",
    "gc_bias_expand=tf.broadcast_to(gc_bias, [batchsize_, no_gc, dur])\n",
    "\n",
    "gc_bias_add=tf.add(pre_gc, gc_bias_expand)\n",
    "\n",
    "output=4.0*gc_stretch*tf.nn.relu(gc_bias_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_ = tf.placeholder(\"float\", name=\"output_spikes\")\n",
    "learn_rate=1e-3\n",
    "\n",
    "trainsampfd={batchsize_: train_loss_size, input_filt1_: input_bip1_train[:, 0:train_loss_size, :], \\\n",
    "             input_filt2_: input_bip2_train[:, 0:train_loss_size, :], input_filt3_: input_bip3_train[:, 0:train_loss_size, :],\\\n",
    "             input_filt4_: input_bip4_train[:, 0:train_loss_size, :], input_filt5_: input_bip5_train[:, 0:train_loss_size, :],\\\n",
    "             input_filt6_: input_bip6_train[:, 0:train_loss_size, :], input_filt7_: input_bip7_train[:, 0:train_loss_size, :],\\\n",
    "             input_filt8_: input_bip8_train[:, 0:train_loss_size, :], input_filt9_: input_bip9_train[:, 0:train_loss_size, :],\\\n",
    "             input_filt10_: input_bip10_train[:, 0:train_loss_size, :], input_filt11_: input_bip11_train[:, 0:train_loss_size, :],\\\n",
    "             input_filt12_: input_bip12_train[:, 0:train_loss_size, :], input_filt13_: input_bip13_train[:, 0:train_loss_size, :],\\\n",
    "             input_filt14_: input_bip14_train[:, 0:train_loss_size, :], input_filt15_: input_bip15_train[:, 0:train_loss_size, :],\\\n",
    "             input_filt16_: input_bip16_train[:, 0:train_loss_size, :], y_:y_train[0:train_loss_size, :, :]}\n",
    "\n",
    "singlefd={batchsize_: 32, input_filt1_: input_bip1_train[:, 0:32, :], \\\n",
    "          input_filt2_: input_bip2_train[:, 0:32, :], input_filt3_: input_bip3_train[:, 0:32, :],\\\n",
    "          input_filt4_: input_bip4_train[:, 0:32, :], input_filt5_: input_bip5_train[:, 0:32, :],\\\n",
    "          input_filt6_: input_bip6_train[:, 0:32, :], input_filt7_: input_bip7_train[:, 0:32, :],\\\n",
    "          input_filt8_: input_bip8_train[:, 0:32, :], input_filt9_: input_bip9_train[:, 0:32, :],\\\n",
    "          input_filt10_: input_bip10_train[:, 0:32, :], input_filt11_: input_bip11_train[:, 0:32, :],\\\n",
    "          input_filt12_: input_bip12_train[:, 0:32, :], input_filt13_: input_bip13_train[:, 0:32, :],\\\n",
    "          input_filt14_: input_bip14_train[:, 0:32, :], input_filt15_: input_bip15_train[:, 0:32, :],\\\n",
    "          input_filt16_: input_bip16_train[:, 0:32, :], y_:y_train[0:32, :, :]}\n",
    "\n",
    "\n",
    "batchsize= 32\n",
    "\n",
    "# L2 loss (normalized)\n",
    "loss = (tf.nn.l2_loss((output - y_), name='loss'))/(batchsize*data_duration) \n",
    "single_loss = tf.reduce_sum((abs(output - y_))/(batchsize*data_duration), 1)\n",
    "\n",
    "# L1 regularization on weights and output\n",
    "reg1 = tf.add_n([tf.reduce_sum(tf.abs(bip1_gc_syn)), tf.reduce_sum(tf.abs(bip2_gc_syn)),  tf.reduce_sum(tf.abs(bip11_gc_syn))])\n",
    "reg2 = tf.add_n([tf.reduce_sum(tf.abs(bip1_copy_gc_syn)), tf.reduce_sum(tf.abs(bip2_copy_gc_syn)), tf.reduce_sum(tf.abs(bip11_copy_gc_syn))])\n",
    "reg3 = tf.add_n([tf.reduce_sum(tf.abs(bip1_am1_syn)), tf.reduce_sum(tf.abs(bip2_am1_syn)), tf.reduce_sum(tf.abs(bip11_am1_syn))])\n",
    "reg5 = tf.add_n([tf.reduce_sum(tf.abs(am1_gc_syn))])\n",
    "reg6 = tf.add_n([tf.reduce_sum(tf.abs(am1_b1copy_syn)), tf.reduce_sum(tf.abs(am1_b2copy_syn))])\n",
    "reg7 = 1e-4*tf.reduce_sum(tf.abs(output))\n",
    "# regularizer=tf.add_n([reg1, reg2, reg3, reg5, reg6, reg7])\n",
    "regularizer=tf.add_n([reg1, reg3, reg5])\n",
    "\n",
    "\n",
    "\n",
    "# lambda1=1e-1 \n",
    "lambda1=1e1 \n",
    "\n",
    "objective=tf.add(loss, lambda1*regularizer)\n",
    "\n",
    "algorithm_choice=2  #1\n",
    "\n",
    "if algorithm_choice==1:\n",
    "    train_step = tf.train.GradientDescentOptimizer(learn_rate).minimize(objective)\n",
    "elif algorithm_choice==2:\n",
    "    my_epsilon=1e-4 #1e-8\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate=learn_rate, epsilon=my_epsilon).minimize(objective)\n",
    "elif algorithm_choice==3:\n",
    "    momentum_par=0.9\n",
    "    train_step = tf.train.MomentumOptimizer(learn_rate, momentum_par).minimize(objective)\n",
    "elif algorithm_choice==4:\n",
    "    train_step = tf.train.AdagradOptimizer(learn_rate).minimize(objective)\n",
    "elif algorithm_choice==5:\n",
    "    train_step = tf.train.RMSPropOptimizer(learn_rate).minimize(objective)\n",
    "\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# initialize arrays to store weight histories\n",
    "\n",
    "f1b1_syn_hist=tf.reshape(f1b1_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f2b1_syn_hist=tf.reshape(f2b1_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f3b1_syn_hist=tf.reshape(f3b1_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f4b1_syn_hist=tf.reshape(f4b1_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f5b1_syn_hist=tf.reshape(f5b1_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f6b1_syn_hist=tf.reshape(f6b1_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f7b1_syn_hist=tf.reshape(f7b1_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f8b1_syn_hist=tf.reshape(f8b1_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f9b1_syn_hist=tf.reshape(f9b1_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f10b1_syn_hist=tf.reshape(f10b1_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f11b1_syn_hist=tf.reshape(f11b1_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f12b1_syn_hist=tf.reshape(f12b1_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f13b1_syn_hist=tf.reshape(f13b1_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f14b1_syn_hist=tf.reshape(f14b1_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f15b1_syn_hist=tf.reshape(f15b1_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f16b1_syn_hist=tf.reshape(f16b1_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "\n",
    "f1b2_syn_hist=tf.reshape(f1b2_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f2b2_syn_hist=tf.reshape(f2b2_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f3b2_syn_hist=tf.reshape(f3b2_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f4b2_syn_hist=tf.reshape(f4b2_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f5b2_syn_hist=tf.reshape(f5b2_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f6b2_syn_hist=tf.reshape(f6b2_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f7b2_syn_hist=tf.reshape(f7b2_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f8b2_syn_hist=tf.reshape(f8b2_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f9b2_syn_hist=tf.reshape(f9b2_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f10b2_syn_hist=tf.reshape(f10b2_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f11b2_syn_hist=tf.reshape(f11b2_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f12b2_syn_hist=tf.reshape(f12b2_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f13b2_syn_hist=tf.reshape(f13b2_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f14b2_syn_hist=tf.reshape(f14b2_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f15b2_syn_hist=tf.reshape(f15b2_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f16b2_syn_hist=tf.reshape(f16b2_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "\n",
    "f1b11_syn_hist=tf.reshape(f1b11_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f2b11_syn_hist=tf.reshape(f2b11_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f3b11_syn_hist=tf.reshape(f3b11_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f4b11_syn_hist=tf.reshape(f4b11_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f5b11_syn_hist=tf.reshape(f5b11_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f6b11_syn_hist=tf.reshape(f6b11_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f7b11_syn_hist=tf.reshape(f7b11_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f8b11_syn_hist=tf.reshape(f8b11_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f9b11_syn_hist=tf.reshape(f9b11_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f10b11_syn_hist=tf.reshape(f10b11_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f11b11_syn_hist=tf.reshape(f11b11_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f12b11_syn_hist=tf.reshape(f12b11_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f13b11_syn_hist=tf.reshape(f13b11_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f14b11_syn_hist=tf.reshape(f14b11_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f15b11_syn_hist=tf.reshape(f15b11_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "f16b11_syn_hist=tf.reshape(f16b11_syn.eval(session=sess), [1, no_filters_per_bc_type])\n",
    "\n",
    "bip1_gc_syn_hist=tf.reshape(bip1_gc_syn.eval(session=sess), [1, no_bipolars, no_gc])\n",
    "bip2_gc_syn_hist=tf.reshape(bip2_gc_syn.eval(session=sess), [1, no_bipolars, no_gc])\n",
    "bip11_gc_syn_hist=tf.reshape(bip11_gc_syn.eval(session=sess), [1, no_bipolars, no_gc])\n",
    "\n",
    "\n",
    "bip1_copy_gc_syn_hist=tf.reshape(bip1_copy_gc_syn.eval(session=sess), [1, no_bipolars, no_gc])\n",
    "bip2_copy_gc_syn_hist=tf.reshape(bip2_copy_gc_syn.eval(session=sess), [1, no_bipolars, no_gc])\n",
    "bip11_copy_gc_syn_hist=tf.reshape(bip11_copy_gc_syn.eval(session=sess), [1, no_bipolars, no_gc])\n",
    "\n",
    "\n",
    "b1_bias_hist=tf.reshape(b1_bias.eval(session=sess), [1, no_bipolars])\n",
    "b2_bias_hist=tf.reshape(b2_bias.eval(session=sess), [1, no_bipolars])\n",
    "b11_bias_hist=tf.reshape(b11_bias.eval(session=sess), [1, no_bipolars])\n",
    "\n",
    "\n",
    "am1_bias_hist=tf.reshape(am1_bias.eval(session=sess), [1, no_am1])\n",
    "gc_bias_hist=tf.reshape(gc_bias.eval(session=sess), [1, no_gc])\n",
    "\n",
    "gc_stretch_hist=tf.reshape(gc_stretch.eval(session=sess), [1, no_gc])\n",
    "\n",
    "bip1_am1_syn_hist=tf.reshape(bip1_am1_syn.eval(session=sess), [1, no_bipolars, no_am1])\n",
    "bip2_am1_syn_hist=tf.reshape(bip2_am1_syn.eval(session=sess), [1, no_bipolars, no_am1])\n",
    "bip11_am1_syn_hist=tf.reshape(bip11_am1_syn.eval(session=sess), [1, no_bipolars, no_am1])\n",
    "\n",
    "am1_b1copy_syn_hist=tf.reshape(am1_b1copy_syn.eval(session=sess), [1, no_am1, no_bipolars])\n",
    "am1_b2copy_syn_hist=tf.reshape(am1_b2copy_syn.eval(session=sess), [1, no_am1, no_bipolars])\n",
    "\n",
    "am1_gc_syn_hist=tf.reshape(am1_gc_syn.eval(session=sess), [1, no_am1, no_gc])\n",
    "\n",
    "output_hist=tf.reshape(output.eval(session=sess, feed_dict=singlefd), [1, 32, data_duration])\n",
    "\n",
    "#loss\n",
    "loss_hist = ones([1])\n",
    "valid_hist = ones([1])\n",
    "test_hist = ones([1])\n",
    "\n",
    "\n",
    "check=1.0\n",
    "step=0\n",
    "end_flag=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.35672857822516\n"
     ]
    }
   ],
   "source": [
    "# COMPUTE LOSS AT ANN INITIALIZATION\n",
    "\n",
    "loss_val = (batchsize/78.0)*sess.run(loss, feed_dict= trainsampfd)\n",
    "print(loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# INITIALIZE LOSS HISTORIES\n",
    "\n",
    "fddd={batchsize_: no_data_test, input_filt1_: input_bip1_test, \\\n",
    "          input_filt2_: input_bip2_test, input_filt3_: input_bip3_test,\\\n",
    "          input_filt4_: input_bip4_test, input_filt5_: input_bip5_test,\\\n",
    "          input_filt6_: input_bip6_test, input_filt7_: input_bip7_test,\\\n",
    "          input_filt8_: input_bip8_test, input_filt9_: input_bip9_test,\\\n",
    "          input_filt10_: input_bip10_test, input_filt11_: input_bip11_test,\\\n",
    "          input_filt12_: input_bip12_test, input_filt13_: input_bip13_test,\\\n",
    "          input_filt14_: input_bip14_test, input_filt15_: input_bip15_test,\\\n",
    "          input_filt16_: input_bip16_test, y_:y_test}\n",
    "\n",
    "test_loss = (batchsize/input_bip1_test.shape[1])*sess.run(loss, feed_dict=fddd)\n",
    "\n",
    "loss_hist=loss_val*loss_hist\n",
    "test_hist=test_loss*test_hist\n",
    "\n",
    "batch_loss_hist=np.zeros([1])\n",
    "batch_loss_hist=batch_loss_hist.astype(float32)\n",
    "L1_hist=np.zeros([1])\n",
    "L1_hist=L1_hist.astype(float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0  loss: = 41.489517 \n",
      "\n",
      "step: 1  loss: = 37.494652 \n",
      "\n",
      "step: 2  loss: = 36.285851 \n",
      "\n",
      "step: 3  loss: = 34.652893 \n",
      "\n",
      "step: 4  loss: = 33.183300 \n",
      "\n",
      "step: 5  loss: = 32.092728 \n",
      "\n",
      "step: 6  loss: = 31.166462 \n",
      "\n",
      "step: 7  loss: = 30.194567 \n",
      "\n",
      "step: 8  loss: = 30.238373 \n",
      "\n",
      "step: 9  loss: = 29.913153 \n",
      "\n",
      "step: 10  loss: = 29.541197 \n",
      "\n",
      "step: 11  loss: = 29.238832 \n",
      "\n",
      "step: 12  loss: = 29.712873 \n",
      "\n",
      "step: 13  loss: = 29.470957 \n",
      "\n",
      "step: 14  loss: = 29.083477 \n",
      "\n",
      "step: 15  loss: = 29.039101 \n",
      "\n",
      "step: 16  loss: = 29.186764 \n",
      "\n",
      "step: 17  loss: = 28.879431 \n",
      "\n",
      "step: 18  loss: = 28.875761 \n",
      "\n",
      "step: 19  loss: = 28.646278 \n",
      "\n",
      "step: 20  loss: = 28.638256 \n",
      "\n",
      "step: 21  loss: = 28.451725 \n",
      "\n",
      "step: 22  loss: = 28.420645 \n",
      "\n",
      "step: 23  loss: = 28.748260 \n",
      "\n",
      "step: 24  loss: = 28.372929 \n",
      "\n",
      "step: 25  loss: = 28.470583 \n",
      "\n",
      "step: 26  loss: = 28.300177 \n",
      "\n",
      "step: 27  loss: = 28.052887 \n",
      "\n",
      "step: 28  loss: = 28.301867 \n",
      "\n",
      "step: 29  loss: = 28.033768 \n",
      "\n",
      "step: 30  loss: = 27.968470 \n",
      "\n",
      "step: 31  loss: = 28.282669 \n",
      "\n",
      "step: 32  loss: = 27.960503 \n",
      "\n",
      "step: 33  loss: = 27.991915 \n",
      "\n",
      "step: 34  loss: = 28.191336 \n",
      "\n",
      "step: 35  loss: = 27.820700 \n",
      "\n",
      "step: 36  loss: = 27.652285 \n",
      "\n",
      "step: 37  loss: = 27.916309 \n",
      "\n",
      "step: 38  loss: = 27.633492 \n",
      "\n",
      "step: 39  loss: = 27.718573 \n",
      "\n",
      "step: 40  loss: = 28.140945 \n",
      "\n",
      "step: 41  loss: = 27.802181 \n",
      "\n",
      "step: 42  loss: = 27.541800 \n",
      "\n",
      "step: 43  loss: = 27.780685 \n",
      "\n",
      "step: 44  loss: = 27.837603 \n",
      "\n",
      "step: 45  loss: = 27.713961 \n",
      "\n",
      "step: 46  loss: = 27.725412 \n",
      "\n",
      "step: 47  loss: = 27.582352 \n",
      "\n",
      "step: 48  loss: = 27.648825 \n",
      "\n",
      "step: 49  loss: = 27.976997 \n",
      "\n",
      "step: 50  loss: = 27.470480 \n",
      "\n",
      "step: 51  loss: = 28.048943 \n",
      "\n",
      "step: 52  loss: = 27.914324 \n",
      "\n",
      "step: 53  loss: = 27.321859 \n",
      "\n",
      "step: 54  loss: = 27.648762 \n",
      "\n",
      "step: 55  loss: = 27.899221 \n",
      "\n",
      "step: 56  loss: = 27.575930 \n",
      "\n",
      "step: 57  loss: = 27.375412 \n",
      "\n",
      "step: 58  loss: = 27.771872 \n",
      "\n",
      "step: 59  loss: = 27.679588 \n",
      "\n",
      "step: 60  loss: = 27.703390 \n",
      "\n",
      "step: 61  loss: = 27.673311 \n",
      "\n",
      "step: 62  loss: = 27.559042 \n",
      "\n",
      "step: 63  loss: = 27.900452 \n",
      "\n",
      "step: 64  loss: = 28.018759 \n",
      "\n",
      "step: 65  loss: = 27.655560 \n",
      "\n",
      "step: 66  loss: = 27.133554 \n",
      "\n",
      "step: 67  loss: = 27.475157 \n",
      "\n",
      "step: 68  loss: = 27.893059 \n",
      "\n",
      "step: 69  loss: = 27.776937 \n",
      "\n",
      "step: 70  loss: = 28.152739 \n",
      "\n",
      "step: 71  loss: = 28.213972 \n",
      "\n",
      "step: 72  loss: = 27.636980 \n",
      "\n",
      "step: 73  loss: = 27.707939 \n",
      "\n",
      "step: 74  loss: = 27.371948 \n",
      "\n",
      "step: 75  loss: = 27.808846 \n",
      "\n",
      "step: 76  loss: = 27.783756 \n",
      "\n",
      "step: 77  loss: = 27.644787 \n",
      "\n",
      "step: 78  loss: = 27.155832 \n",
      "\n",
      "step: 79  loss: = 27.281185 \n",
      "\n",
      "step: 80  loss: = 27.914553 \n",
      "\n",
      "step: 81  loss: = 27.400896 \n",
      "\n",
      "step: 82  loss: = 27.761040 \n",
      "\n",
      "step: 83  loss: = 27.862041 \n",
      "\n",
      "step: 84  loss: = 27.809750 \n",
      "\n",
      "step: 85  loss: = 27.646017 \n",
      "\n",
      "step: 86  loss: = 27.803001 \n",
      "\n",
      "step: 87  loss: = 27.449692 \n",
      "\n",
      "step: 88  loss: = 27.558672 \n",
      "\n",
      "step: 89  loss: = 27.482334 \n",
      "\n",
      "step: 90  loss: = 27.850138 \n",
      "\n",
      "step: 91  loss: = 27.727526 \n",
      "\n",
      "step: 92  loss: = 27.289846 \n",
      "\n",
      "step: 93  loss: = 27.474476 \n",
      "\n",
      "step: 94  loss: = 27.345472 \n",
      "\n",
      "step: 95  loss: = 27.466236 \n",
      "\n",
      "step: 96  loss: = 27.713846 \n",
      "\n",
      "step: 97  loss: = 27.521023 \n",
      "\n",
      "step: 98  loss: = 27.328939 \n",
      "\n",
      "step: 99  loss: = 27.280560 \n",
      "\n",
      "step: 100  loss: = 27.501633 \n",
      "\n",
      "step: 101  loss: = 27.426065 \n",
      "\n",
      "step: 102  loss: = 27.344259 \n",
      "\n",
      "step: 103  loss: = 28.301764 \n",
      "\n",
      "step: 104  loss: = 27.199144 \n",
      "\n",
      "step: 105  loss: = 27.524830 \n",
      "\n",
      "step: 106  loss: = 27.057055 \n",
      "\n",
      "step: 107  loss: = 27.201115 \n",
      "\n",
      "step: 108  loss: = 27.535467 \n",
      "\n",
      "step: 109  loss: = 27.792982 \n",
      "\n",
      "step: 110  loss: = 27.270668 \n",
      "\n",
      "step: 111  loss: = 27.909609 \n",
      "\n",
      "step: 112  loss: = 27.622807 \n",
      "\n",
      "step: 113  loss: = 27.414412 \n",
      "\n",
      "step: 114  loss: = 27.159695 \n",
      "\n",
      "step: 115  loss: = 27.215654 \n",
      "\n",
      "step: 116  loss: = 27.457790 \n",
      "\n",
      "step: 117  loss: = 27.623257 \n",
      "\n",
      "step: 118  loss: = 27.527599 \n",
      "\n",
      "step: 119  loss: = 27.809690 \n",
      "\n",
      "step: 120  loss: = 26.910519 \n",
      "\n",
      "step: 121  loss: = 27.704620 \n",
      "\n",
      "step: 122  loss: = 27.325201 \n",
      "\n",
      "step: 123  loss: = 27.158869 \n",
      "\n",
      "step: 124  loss: = 27.123724 \n",
      "\n",
      "step: 125  loss: = 27.755217 \n",
      "\n",
      "step: 126  loss: = 28.165869 \n",
      "\n",
      "step: 127  loss: = 27.590662 \n",
      "\n",
      "step: 128  loss: = 27.911451 \n",
      "\n",
      "step: 129  loss: = 27.237062 \n",
      "\n",
      "step: 130  loss: = 27.371948 \n",
      "\n",
      "step: 131  loss: = 27.819092 \n",
      "\n",
      "step: 132  loss: = 27.231335 \n",
      "\n",
      "step: 133  loss: = 27.626942 \n",
      "\n",
      "step: 134  loss: = 27.420832 \n",
      "\n",
      "step: 135  loss: = 27.899006 \n",
      "\n",
      "step: 136  loss: = 27.383648 \n",
      "\n",
      "step: 137  loss: = 27.500494 \n",
      "\n",
      "step: 138  loss: = 27.660570 \n",
      "\n",
      "step: 139  loss: = 27.570459 \n",
      "\n",
      "step: 140  loss: = 27.793379 \n",
      "\n",
      "step: 141  loss: = 27.094255 \n",
      "\n",
      "step: 142  loss: = 27.331692 \n",
      "\n",
      "step: 143  loss: = 26.916958 \n",
      "\n",
      "step: 144  loss: = 27.281067 \n",
      "\n",
      "step: 145  loss: = 27.790491 \n",
      "\n",
      "step: 146  loss: = 27.772800 \n",
      "\n",
      "step: 147  loss: = 27.406593 \n",
      "\n",
      "step: 148  loss: = 27.279840 \n",
      "\n",
      "step: 149  loss: = 27.703310 \n",
      "\n",
      "step: 150  loss: = 27.633492 \n",
      "\n",
      "step: 151  loss: = 27.288010 \n",
      "\n",
      "step: 152  loss: = 27.729551 \n",
      "\n",
      "step: 153  loss: = 27.716845 \n",
      "\n",
      "step: 154  loss: = 27.521732 \n",
      "\n",
      "step: 155  loss: = 27.598600 \n",
      "\n",
      "step: 156  loss: = 27.623560 \n",
      "\n",
      "step: 157  loss: = 27.403578 \n",
      "\n",
      "step: 158  loss: = 27.330727 \n",
      "\n",
      "step: 159  loss: = 27.779123 \n",
      "\n",
      "step: 160  loss: = 27.490719 \n",
      "\n",
      "step: 161  loss: = 27.167143 \n",
      "\n",
      "step: 162  loss: = 27.510981 \n",
      "\n",
      "step: 163  loss: = 27.832113 \n",
      "\n",
      "step: 164  loss: = 27.300972 \n",
      "\n",
      "step: 165  loss: = 27.510098 \n",
      "\n",
      "step: 166  loss: = 26.828346 \n",
      "\n",
      "step: 167  loss: = 27.178068 \n",
      "\n",
      "step: 168  loss: = 27.322243 \n",
      "\n",
      "step: 169  loss: = 27.401258 \n",
      "\n",
      "step: 170  loss: = 27.667809 \n",
      "\n",
      "step: 171  loss: = 27.619303 \n",
      "\n",
      "step: 172  loss: = 27.779955 \n",
      "\n",
      "step: 173  loss: = 27.470552 \n",
      "\n",
      "step: 174  loss: = 27.548782 \n",
      "\n",
      "step: 175  loss: = 27.605646 \n",
      "\n",
      "step: 176  loss: = 27.202635 \n",
      "\n",
      "step: 177  loss: = 27.281303 \n",
      "\n",
      "step: 178  loss: = 27.826874 \n",
      "\n",
      "step: 179  loss: = 27.303831 \n",
      "\n",
      "step: 180  loss: = 26.931545 \n",
      "\n",
      "step: 181  loss: = 28.008404 \n",
      "\n",
      "step: 182  loss: = 27.677103 \n",
      "\n",
      "step: 183  loss: = 27.058136 \n",
      "\n",
      "step: 184  loss: = 27.497791 \n",
      "\n",
      "step: 185  loss: = 27.385956 \n",
      "\n",
      "step: 186  loss: = 27.410412 \n",
      "\n",
      "step: 187  loss: = 27.410538 \n",
      "\n",
      "step: 188  loss: = 27.179911 \n",
      "\n",
      "step: 189  loss: = 27.492037 \n",
      "\n",
      "step: 190  loss: = 27.311352 \n",
      "\n",
      "step: 191  loss: = 27.318434 \n",
      "\n",
      "step: 192  loss: = 27.736498 \n",
      "\n",
      "step: 193  loss: = 27.317019 \n",
      "\n",
      "step: 194  loss: = 27.148748 \n",
      "\n",
      "step: 195  loss: = 27.413471 \n",
      "\n",
      "step: 196  loss: = 27.789165 \n",
      "\n",
      "step: 197  loss: = 27.559532 \n",
      "\n",
      "step: 198  loss: = 27.941422 \n",
      "\n",
      "step: 199  loss: = 26.997463 \n",
      "\n",
      "step: 200  loss: = 27.430895 \n",
      "\n",
      "step: 201  loss: = 27.397051 \n",
      "\n",
      "step: 202  loss: = 27.319975 \n",
      "\n",
      "step: 203  loss: = 27.554506 \n",
      "\n",
      "step: 204  loss: = 26.945187 \n",
      "\n",
      "step: 205  loss: = 27.244963 \n",
      "\n",
      "step: 206  loss: = 27.357647 \n",
      "\n",
      "step: 207  loss: = 27.202242 \n",
      "\n",
      "step: 208  loss: = 27.560614 \n",
      "\n",
      "step: 209  loss: = 27.439083 \n",
      "\n",
      "step: 210  loss: = 27.191744 \n",
      "\n",
      "step: 211  loss: = 27.694408 \n",
      "\n",
      "step: 212  loss: = 27.567900 \n",
      "\n",
      "step: 213  loss: = 27.263750 \n",
      "\n",
      "step: 214  loss: = 27.563410 \n",
      "\n",
      "step: 215  loss: = 27.309504 \n",
      "\n",
      "step: 216  loss: = 27.347616 \n",
      "\n",
      "step: 217  loss: = 27.759052 \n",
      "\n",
      "step: 218  loss: = 27.886890 \n",
      "\n",
      "step: 219  loss: = 26.989584 \n",
      "\n",
      "step: 220  loss: = 27.510622 \n",
      "\n",
      "step: 221  loss: = 27.424393 \n",
      "\n",
      "step: 222  loss: = 27.431688 \n",
      "\n",
      "step: 223  loss: = 27.064795 \n",
      "\n",
      "step: 224  loss: = 27.464828 \n",
      "\n",
      "step: 225  loss: = 27.659559 \n",
      "\n",
      "step: 226  loss: = 27.233513 \n",
      "\n",
      "step: 227  loss: = 27.434727 \n",
      "\n",
      "step: 228  loss: = 27.499960 \n",
      "\n",
      "step: 229  loss: = 27.694153 \n",
      "\n",
      "step: 230  loss: = 27.637941 \n",
      "\n",
      "step: 231  loss: = 27.696793 \n",
      "\n",
      "step: 232  loss: = 27.465202 \n",
      "\n",
      "step: 233  loss: = 27.457878 \n",
      "\n",
      "step: 234  loss: = 27.381723 \n",
      "\n",
      "step: 235  loss: = 27.338940 \n",
      "\n",
      "step: 236  loss: = 27.318232 \n",
      "\n",
      "step: 237  loss: = 27.280840 \n",
      "\n",
      "step: 238  loss: = 27.400417 \n",
      "\n",
      "step: 239  loss: = 27.330564 \n",
      "\n",
      "step: 240  loss: = 28.132355 \n",
      "\n",
      "step: 241  loss: = 27.157764 \n",
      "\n",
      "step: 242  loss: = 27.551508 \n",
      "\n",
      "step: 243  loss: = 27.726463 \n",
      "\n",
      "step: 244  loss: = 27.854170 \n",
      "\n",
      "step: 245  loss: = 27.508236 \n",
      "\n",
      "step: 246  loss: = 27.160566 \n",
      "\n",
      "step: 247  loss: = 27.518166 \n",
      "\n",
      "step: 248  loss: = 27.332378 \n",
      "\n",
      "step: 249  loss: = 27.288265 \n",
      "\n",
      "step: 250  loss: = 27.984106 \n",
      "\n",
      "step: 251  loss: = 27.832788 \n",
      "\n",
      "step: 252  loss: = 27.168983 \n",
      "\n",
      "step: 253  loss: = 27.989653 \n",
      "\n",
      "step: 254  loss: = 27.420797 \n",
      "\n",
      "step: 255  loss: = 27.249485 \n",
      "\n",
      "step: 256  loss: = 27.475365 \n",
      "\n",
      "step: 257  loss: = 27.615717 \n",
      "\n",
      "step: 258  loss: = 27.870270 \n",
      "\n",
      "step: 259  loss: = 27.588285 \n",
      "\n",
      "step: 260  loss: = 27.644709 \n",
      "\n",
      "step: 261  loss: = 27.653353 \n",
      "\n",
      "step: 262  loss: = 27.219282 \n",
      "\n",
      "step: 263  loss: = 27.412460 \n",
      "\n",
      "step: 264  loss: = 27.480602 \n",
      "\n",
      "step: 265  loss: = 27.437477 \n",
      "\n",
      "step: 266  loss: = 27.550808 \n",
      "\n",
      "step: 267  loss: = 27.454739 \n",
      "\n",
      "step: 268  loss: = 27.590536 \n",
      "\n",
      "step: 269  loss: = 28.154995 \n",
      "\n",
      "step: 270  loss: = 27.316603 \n",
      "\n",
      "step: 271  loss: = 27.695166 \n",
      "\n",
      "step: 272  loss: = 27.648842 \n",
      "\n",
      "step: 273  loss: = 27.121752 \n",
      "\n",
      "step: 274  loss: = 27.742270 \n",
      "\n",
      "step: 275  loss: = 27.694601 \n",
      "\n",
      "step: 276  loss: = 27.048315 \n",
      "\n",
      "step: 277  loss: = 27.224558 \n",
      "\n",
      "step: 278  loss: = 28.109636 \n",
      "\n",
      "step: 279  loss: = 27.695538 \n",
      "\n",
      "step: 280  loss: = 27.309864 \n",
      "\n",
      "step: 281  loss: = 27.168728 \n",
      "\n",
      "step: 282  loss: = 27.548323 \n",
      "\n",
      "step: 283  loss: = 28.034050 \n",
      "\n",
      "step: 284  loss: = 27.727825 \n",
      "\n",
      "step: 285  loss: = 27.314997 \n",
      "\n",
      "step: 286  loss: = 27.508335 \n",
      "\n",
      "step: 287  loss: = 27.382271 \n",
      "\n",
      "step: 288  loss: = 27.448793 \n",
      "\n",
      "step: 289  loss: = 27.733932 \n",
      "\n",
      "step: 290  loss: = 27.375898 \n",
      "\n",
      "step: 291  loss: = 27.482119 \n",
      "\n",
      "step: 292  loss: = 27.256760 \n",
      "\n",
      "step: 293  loss: = 27.041498 \n",
      "\n",
      "step: 294  loss: = 27.447609 \n",
      "\n",
      "step: 295  loss: = 28.119415 \n",
      "\n",
      "step: 296  loss: = 27.153631 \n",
      "\n",
      "step: 297  loss: = 27.705137 \n",
      "\n",
      "step: 298  loss: = 27.378124 \n",
      "\n",
      "step: 299  loss: = 27.368454 \n",
      "\n",
      "step: 300  loss: = 27.458069 \n",
      "\n",
      "step: 301  loss: = 27.563309 \n",
      "\n",
      "step: 302  loss: = 27.594957 \n",
      "\n",
      "step: 303  loss: = 27.433840 \n",
      "\n",
      "step: 304  loss: = 27.118807 \n",
      "\n",
      "step: 305  loss: = 27.481333 \n",
      "\n",
      "step: 306  loss: = 27.534000 \n",
      "\n",
      "step: 307  loss: = 27.231569 \n",
      "\n",
      "step: 308  loss: = 27.619381 \n",
      "\n",
      "step: 309  loss: = 27.466354 \n",
      "\n",
      "step: 310  loss: = 27.518076 \n",
      "\n",
      "step: 311  loss: = 28.188210 \n",
      "\n",
      "step: 312  loss: = 27.207331 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAIN THE NETWORK\n",
    "\n",
    "for step in range(100): \n",
    "    \n",
    "    inds = np.reshape(np.random.permutation(range(input_bip1_train.shape[1])), [-1, batchsize])\n",
    "\n",
    "    for n in range(len(inds)):  \n",
    "        fdd = {batchsize_: batchsize, input_filt1_: input_bip1_train[:, inds[n, :], :], \\\n",
    "               input_filt2_: input_bip2_train[:, inds[n, :], :], input_filt3_: input_bip3_train[:, inds[n, :], :],\\\n",
    "               input_filt4_: input_bip4_train[:, inds[n, :], :], input_filt5_: input_bip5_train[:, inds[n, :], :],\\\n",
    "               input_filt6_: input_bip6_train[:, inds[n, :], :], input_filt7_: input_bip7_train[:, inds[n, :], :],\\\n",
    "               input_filt8_: input_bip8_train[:, inds[n, :], :], input_filt9_: input_bip9_train[:, inds[n, :], :],\\\n",
    "               input_filt10_: input_bip10_train[:, inds[n, :], :], input_filt11_: input_bip11_train[:, inds[n, :], :],\\\n",
    "               input_filt12_: input_bip12_train[:, inds[n, :], :], input_filt13_: input_bip13_train[:, inds[n, :], :],\\\n",
    "               input_filt14_: input_bip14_train[:, inds[n, :], :], input_filt15_: input_bip15_train[:, inds[n, :], :],\\\n",
    "               input_filt16_: input_bip16_train[:, inds[n, :], :], y_:y_train[inds[n, :], :, :]}\n",
    "        sess.run(train_step, feed_dict=fdd, options = run_opts)\n",
    "        \n",
    "    batch_loss=sess.run(loss, feed_dict=fdd)\n",
    "    batch_loss_hist=np.concatenate([batch_loss_hist, np.array([batch_loss])], axis=0)\n",
    "    L1=sess.run(regularizer, feed_dict=fdd)\n",
    "    L1_hist=np.concatenate([L1_hist, np.array([L1])], axis=0)\n",
    "    loss_val = (batchsize/train_loss_size)*sess.run(loss, feed_dict= trainsampfd)\n",
    "    loss_hist=np.concatenate([loss_hist, np.array([loss_val])], axis=0)\n",
    "    check=loss_val\n",
    "\n",
    "    print(\"step: %d  loss: = %9f \\n\" % (step, loss_val))\n",
    "    \n",
    "    if (step % 10 == 0):\n",
    "        test_loss = (batchsize/input_bip1_test.shape[1])*sess.run(loss, feed_dict= fddd)\n",
    "        test_hist=np.concatenate([test_hist, np.array([test_loss])], axis=0)\n",
    " \n",
    "\n",
    "    if (step % 10 == 0):\n",
    "        \n",
    "        f1b1_syn_hist=tf.concat([f1b1_syn_hist, tf.reshape(f1b1_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f1b1_syn_concat')\n",
    "        f2b1_syn_hist=tf.concat([f2b1_syn_hist, tf.reshape(f2b1_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f2b1_syn_concat')\n",
    "        f3b1_syn_hist=tf.concat([f3b1_syn_hist, tf.reshape(f3b1_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f3b1_syn_concat')\n",
    "        f4b1_syn_hist=tf.concat([f4b1_syn_hist, tf.reshape(f4b1_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f4b1_syn_concat')\n",
    "        f5b1_syn_hist=tf.concat([f5b1_syn_hist, tf.reshape(f5b1_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f5b1_syn_concat')\n",
    "        f6b1_syn_hist=tf.concat([f6b1_syn_hist, tf.reshape(f6b1_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f6b1_syn_concat')\n",
    "        f7b1_syn_hist=tf.concat([f7b1_syn_hist, tf.reshape(f7b1_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f7b1_syn_concat')\n",
    "        f8b1_syn_hist=tf.concat([f8b1_syn_hist, tf.reshape(f8b1_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f8b1_syn_concat')\n",
    "        f9b1_syn_hist=tf.concat([f9b1_syn_hist, tf.reshape(f9b1_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f9b1_syn_concat')\n",
    "        f10b1_syn_hist=tf.concat([f10b1_syn_hist, tf.reshape(f10b1_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f10b1_syn_concat')\n",
    "        f11b1_syn_hist=tf.concat([f11b1_syn_hist, tf.reshape(f11b1_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f11b1_syn_concat')\n",
    "        f12b1_syn_hist=tf.concat([f12b1_syn_hist, tf.reshape(f12b1_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f12b1_syn_concat')\n",
    "        f13b1_syn_hist=tf.concat([f13b1_syn_hist, tf.reshape(f13b1_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f13b1_syn_concat')\n",
    "        f14b1_syn_hist=tf.concat([f14b1_syn_hist, tf.reshape(f14b1_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f14b1_syn_concat')\n",
    "        f15b1_syn_hist=tf.concat([f15b1_syn_hist, tf.reshape(f15b1_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f15b1_syn_concat')\n",
    "        f16b1_syn_hist=tf.concat([f16b1_syn_hist, tf.reshape(f16b1_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f16b1_syn_concat')\n",
    "        \n",
    "        f1b2_syn_hist=tf.concat([f1b2_syn_hist, tf.reshape(f1b2_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f1b2_syn_concat')\n",
    "        f2b2_syn_hist=tf.concat([f2b2_syn_hist, tf.reshape(f2b2_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f2b2_syn_concat')\n",
    "        f3b2_syn_hist=tf.concat([f3b2_syn_hist, tf.reshape(f3b2_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f3b2_syn_concat')\n",
    "        f4b2_syn_hist=tf.concat([f4b2_syn_hist, tf.reshape(f4b2_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f4b2_syn_concat')\n",
    "        f5b2_syn_hist=tf.concat([f5b2_syn_hist, tf.reshape(f5b2_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f5b2_syn_concat')\n",
    "        f6b2_syn_hist=tf.concat([f6b2_syn_hist, tf.reshape(f6b2_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f6b2_syn_concat')\n",
    "        f7b2_syn_hist=tf.concat([f7b2_syn_hist, tf.reshape(f7b2_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f7b2_syn_concat')\n",
    "        f8b2_syn_hist=tf.concat([f8b2_syn_hist, tf.reshape(f8b2_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f8b2_syn_concat')\n",
    "        f9b2_syn_hist=tf.concat([f9b2_syn_hist, tf.reshape(f9b2_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f9b2_syn_concat')\n",
    "        f10b2_syn_hist=tf.concat([f10b2_syn_hist, tf.reshape(f10b2_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f10b2_syn_concat')\n",
    "        f11b2_syn_hist=tf.concat([f11b2_syn_hist, tf.reshape(f11b2_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f11b2_syn_concat')\n",
    "        f12b2_syn_hist=tf.concat([f12b2_syn_hist, tf.reshape(f12b2_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f12b2_syn_concat')\n",
    "        f13b2_syn_hist=tf.concat([f13b2_syn_hist, tf.reshape(f13b2_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f13b2_syn_concat')\n",
    "        f14b2_syn_hist=tf.concat([f14b2_syn_hist, tf.reshape(f14b2_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f14b2_syn_concat')\n",
    "        f15b2_syn_hist=tf.concat([f15b2_syn_hist, tf.reshape(f15b2_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f15b2_syn_concat')\n",
    "        f16b2_syn_hist=tf.concat([f16b2_syn_hist, tf.reshape(f16b2_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f16b2_syn_concat')\n",
    "        \n",
    "        f1b11_syn_hist=tf.concat([f1b11_syn_hist, tf.reshape(f1b11_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f1b11_syn_concat')\n",
    "        f2b11_syn_hist=tf.concat([f2b11_syn_hist, tf.reshape(f2b11_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f2b11_syn_concat')\n",
    "        f3b11_syn_hist=tf.concat([f3b11_syn_hist, tf.reshape(f3b11_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f3b11_syn_concat')\n",
    "        f4b11_syn_hist=tf.concat([f4b11_syn_hist, tf.reshape(f4b11_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f4b11_syn_concat')\n",
    "        f5b11_syn_hist=tf.concat([f5b11_syn_hist, tf.reshape(f5b11_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f5b11_syn_concat')\n",
    "        f6b11_syn_hist=tf.concat([f6b11_syn_hist, tf.reshape(f6b11_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f6b11_syn_concat')\n",
    "        f7b11_syn_hist=tf.concat([f7b11_syn_hist, tf.reshape(f7b11_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f7b11_syn_concat')\n",
    "        f8b11_syn_hist=tf.concat([f8b11_syn_hist, tf.reshape(f8b11_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f8b11_syn_concat')\n",
    "        f9b11_syn_hist=tf.concat([f9b11_syn_hist, tf.reshape(f9b11_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f9b11_syn_concat')\n",
    "        f10b11_syn_hist=tf.concat([f10b11_syn_hist, tf.reshape(f10b11_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f10b11_syn_concat')\n",
    "        f11b11_syn_hist=tf.concat([f11b11_syn_hist, tf.reshape(f11b11_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f11b11_syn_concat')\n",
    "        f12b11_syn_hist=tf.concat([f12b11_syn_hist, tf.reshape(f12b11_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f12b11_syn_concat')\n",
    "        f13b11_syn_hist=tf.concat([f13b11_syn_hist, tf.reshape(f13b11_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f13b11_syn_concat')\n",
    "        f14b11_syn_hist=tf.concat([f14b11_syn_hist, tf.reshape(f14b11_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f14b11_syn_concat')\n",
    "        f15b11_syn_hist=tf.concat([f15b11_syn_hist, tf.reshape(f15b11_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f15b11_syn_concat')\n",
    "        f16b11_syn_hist=tf.concat([f16b11_syn_hist, tf.reshape(f16b11_syn.eval(session=sess), [1, no_filters_per_bc_type])], 0, name = 'f16b11_syn_concat')\n",
    "        \n",
    "        bip1_gc_syn_hist=tf.concat([bip1_gc_syn_hist, tf.reshape(bip1_gc_syn.eval(session=sess), [1, no_bipolars, no_gc])], 0, name = 'bip1_gc_syn_concat')\n",
    "        bip2_gc_syn_hist=tf.concat([bip2_gc_syn_hist, tf.reshape(bip2_gc_syn.eval(session=sess), [1, no_bipolars, no_gc])], 0, name = 'bip2_gc_syn_concat')\n",
    "        bip11_gc_syn_hist=tf.concat([bip11_gc_syn_hist, tf.reshape(bip11_gc_syn.eval(session=sess), [1, no_bipolars, no_gc])], 0, name = 'bip11_gc_syn_concat')\n",
    "\n",
    "        bip1_copy_gc_syn_hist=tf.concat([bip1_copy_gc_syn_hist, tf.reshape(bip1_copy_gc_syn.eval(session=sess), [1, no_bipolars, no_gc])], 0, name = 'bip1_copy_gc_syn_concat')\n",
    "        bip2_copy_gc_syn_hist=tf.concat([bip2_copy_gc_syn_hist, tf.reshape(bip2_copy_gc_syn.eval(session=sess), [1, no_bipolars, no_gc])], 0, name = 'bip2_copy_gc_syn_concat')\n",
    "        bip11_copy_gc_syn_hist=tf.concat([bip11_copy_gc_syn_hist, tf.reshape(bip11_copy_gc_syn.eval(session=sess), [1, no_bipolars, no_gc])], 0, name = 'bip11_copy_gc_syn_concat')\n",
    "\n",
    "        bip1_am1_syn_hist=tf.concat([bip1_am1_syn_hist, tf.reshape(bip1_am1_syn.eval(session=sess), [1, no_bipolars, no_am1])], 0, name = 'bip1_am1_syn_concat')\n",
    "        bip2_am1_syn_hist=tf.concat([bip2_am1_syn_hist, tf.reshape(bip2_am1_syn.eval(session=sess), [1, no_bipolars, no_am1])], 0, name = 'bip2_am1_syn_concat')\n",
    "        bip11_am1_syn_hist=tf.concat([bip11_am1_syn_hist, tf.reshape(bip11_am1_syn.eval(session=sess), [1, no_bipolars, no_am1])], 0, name = 'bip11_am1_syn_concat')\n",
    "        \n",
    "        am1_gc_syn_hist=tf.concat([am1_gc_syn_hist, tf.reshape(am1_gc_syn.eval(session=sess), [1, no_am1, no_gc])], 0, name = 'am1_gc_syn_concat')\n",
    "\n",
    "        am1_b1copy_syn_hist=tf.concat([am1_b1copy_syn_hist, tf.reshape(am1_b1copy_syn.eval(session=sess), [1, no_am1, no_bipolars])], 0, name = 'am1_b1copy_syn_concat')\n",
    "        am1_b2copy_syn_hist=tf.concat([am1_b2copy_syn_hist, tf.reshape(am1_b2copy_syn.eval(session=sess), [1, no_am1, no_bipolars])], 0, name = 'am1_b2copy_syn_concat')\n",
    "\n",
    "        b1_bias_hist=tf.concat([b1_bias_hist, tf.reshape(b1_bias.eval(session=sess), [1, no_bipolars])], 0, name = 'bip1_bias_concat')\n",
    "        b2_bias_hist=tf.concat([b2_bias_hist, tf.reshape(b2_bias.eval(session=sess), [1, no_bipolars])], 0, name = 'bip2_bias_concat')\n",
    "        b11_bias_hist=tf.concat([b11_bias_hist, tf.reshape(b11_bias.eval(session=sess), [1, no_bipolars])], 0, name = 'bip11_bias_concat')\n",
    "\n",
    "        am1_bias_hist=tf.concat([am1_bias_hist, tf.reshape(am1_bias.eval(session=sess), [1, no_am1])], 0, name = 'am1_bias_concat')\n",
    "        gc_bias_hist=tf.concat([gc_bias_hist, tf.reshape(gc_bias.eval(session=sess), [1, no_gc])], 0, name = 'gc_bias_concat')\n",
    "        gc_stretch_hist=tf.concat([gc_stretch_hist, tf.reshape(gc_stretch.eval(session=sess), [1, no_gc])], 0, name = 'gc_bias_concat')\n",
    "\n",
    "        output_hist=tf.concat([output_hist, tf.reshape(output.eval(session=sess, feed_dict=singlefd), [1, 32, data_duration])], 0, name = 'output_concat')\n",
    "    \n",
    "        db={}\n",
    "        \n",
    "        db['f1b1_syn_hist']=f1b1_syn_hist.eval(session=sess)\n",
    "        db['f2b1_syn_hist']=f2b1_syn_hist.eval(session=sess)\n",
    "        db['f3b1_syn_hist']=f3b1_syn_hist.eval(session=sess)\n",
    "        db['f4b1_syn_hist']=f4b1_syn_hist.eval(session=sess)\n",
    "        db['f5b1_syn_hist']=f5b1_syn_hist.eval(session=sess)\n",
    "        db['f6b1_syn_hist']=f6b1_syn_hist.eval(session=sess)\n",
    "        db['f7b1_syn_hist']=f7b1_syn_hist.eval(session=sess)\n",
    "        db['f8b1_syn_hist']=f8b1_syn_hist.eval(session=sess)\n",
    "        db['f9b1_syn_hist']=f9b1_syn_hist.eval(session=sess)\n",
    "        db['f10b1_syn_hist']=f10b1_syn_hist.eval(session=sess)\n",
    "        db['f11b1_syn_hist']=f11b1_syn_hist.eval(session=sess)\n",
    "        db['f12b1_syn_hist']=f12b1_syn_hist.eval(session=sess)\n",
    "        db['f13b1_syn_hist']=f13b1_syn_hist.eval(session=sess)\n",
    "        db['f14b1_syn_hist']=f14b1_syn_hist.eval(session=sess)\n",
    "        db['f15b1_syn_hist']=f15b1_syn_hist.eval(session=sess)\n",
    "        db['f16b1_syn_hist']=f16b1_syn_hist.eval(session=sess)\n",
    "        \n",
    "        db['f1b2_syn_hist']=f1b2_syn_hist.eval(session=sess)\n",
    "        db['f2b2_syn_hist']=f2b2_syn_hist.eval(session=sess)\n",
    "        db['f3b2_syn_hist']=f3b2_syn_hist.eval(session=sess)\n",
    "        db['f4b2_syn_hist']=f4b2_syn_hist.eval(session=sess)\n",
    "        db['f5b2_syn_hist']=f5b2_syn_hist.eval(session=sess)\n",
    "        db['f6b2_syn_hist']=f6b2_syn_hist.eval(session=sess)\n",
    "        db['f7b2_syn_hist']=f7b2_syn_hist.eval(session=sess)\n",
    "        db['f8b2_syn_hist']=f8b2_syn_hist.eval(session=sess)\n",
    "        db['f9b2_syn_hist']=f9b2_syn_hist.eval(session=sess)\n",
    "        db['f10b2_syn_hist']=f10b2_syn_hist.eval(session=sess)\n",
    "        db['f11b2_syn_hist']=f11b2_syn_hist.eval(session=sess)\n",
    "        db['f12b2_syn_hist']=f12b2_syn_hist.eval(session=sess)\n",
    "        db['f13b2_syn_hist']=f13b2_syn_hist.eval(session=sess)\n",
    "        db['f14b2_syn_hist']=f14b2_syn_hist.eval(session=sess)\n",
    "        db['f15b2_syn_hist']=f15b2_syn_hist.eval(session=sess)\n",
    "        db['f16b2_syn_hist']=f16b2_syn_hist.eval(session=sess)\n",
    "        \n",
    "        db['f1b11_syn_hist']=f1b11_syn_hist.eval(session=sess)\n",
    "        db['f2b11_syn_hist']=f2b11_syn_hist.eval(session=sess)\n",
    "        db['f3b11_syn_hist']=f3b11_syn_hist.eval(session=sess)\n",
    "        db['f4b11_syn_hist']=f4b11_syn_hist.eval(session=sess)\n",
    "        db['f5b11_syn_hist']=f5b11_syn_hist.eval(session=sess)\n",
    "        db['f6b11_syn_hist']=f6b11_syn_hist.eval(session=sess)\n",
    "        db['f7b11_syn_hist']=f7b11_syn_hist.eval(session=sess)\n",
    "        db['f8b11_syn_hist']=f8b11_syn_hist.eval(session=sess)\n",
    "        db['f9b11_syn_hist']=f9b11_syn_hist.eval(session=sess)\n",
    "        db['f10b11_syn_hist']=f10b11_syn_hist.eval(session=sess)\n",
    "        db['f11b11_syn_hist']=f11b11_syn_hist.eval(session=sess)\n",
    "        db['f12b11_syn_hist']=f12b11_syn_hist.eval(session=sess)\n",
    "        db['f13b11_syn_hist']=f13b11_syn_hist.eval(session=sess)\n",
    "        db['f14b11_syn_hist']=f14b11_syn_hist.eval(session=sess)\n",
    "        db['f15b11_syn_hist']=f15b11_syn_hist.eval(session=sess)\n",
    "        db['f16b11_syn_hist']=f16b11_syn_hist.eval(session=sess)\n",
    "        \n",
    "        db['loss_hist']=loss_hist\n",
    "        db['test_hist']=test_hist   \n",
    "        \n",
    "        db['output_hist']=output_hist.eval(session=sess)\n",
    "        db['bip1_gc_syn_hist']=bip1_gc_syn_hist.eval(session=sess)\n",
    "        db['bip2_gc_syn_hist']=bip2_gc_syn_hist.eval(session=sess)\n",
    "        db['bip11_gc_syn_hist']=bip11_gc_syn_hist.eval(session=sess)\n",
    "        \n",
    "        db['bip1_copy_gc_syn_hist']=bip1_copy_gc_syn_hist.eval(session=sess)\n",
    "        db['bip2_copy_gc_syn_hist']=bip2_copy_gc_syn_hist.eval(session=sess)\n",
    "        db['bip11_copy_gc_syn_hist']=bip11_copy_gc_syn_hist.eval(session=sess)\n",
    "\n",
    "        db['bip1_am1_syn_hist']=bip1_am1_syn_hist.eval(session=sess)\n",
    "        db['bip2_am1_syn_hist']=bip2_am1_syn_hist.eval(session=sess)\n",
    "        db['bip11_am1_syn_hist']=bip11_am1_syn_hist.eval(session=sess)\n",
    "       \n",
    "        db['am1_gc_syn_hist']=am1_gc_syn_hist.eval(session=sess)\n",
    "        \n",
    "        db['am1_b1copy_syn_hist']=am1_b1copy_syn_hist.eval(session=sess)\n",
    "        db['am1_b2copy_syn_hist']=am1_b2copy_syn_hist.eval(session=sess)\n",
    "        \n",
    "        db['b1_bias_hist']=b1_bias_hist.eval(session=sess)\n",
    "        db['b2_bias_hist']=b2_bias_hist.eval(session=sess)\n",
    "\n",
    "        db['b11_bias_hist']=b11_bias_hist.eval(session=sess)\n",
    "\n",
    "        db['am1_bias_hist']=am1_bias_hist.eval(session=sess)\n",
    "        db['gc_bias_hist']=gc_bias_hist.eval(session=sess)\n",
    "        db['gc_stretch_hist']=gc_stretch_hist.eval(session=sess)\n",
    "\n",
    "        db['learning_rate']=learn_rate\n",
    "        db['lambda']=lambda1\n",
    "        \n",
    "        if algorithm_choice==1: \n",
    "            db['algorithm']='Gradient_Descent'\n",
    "        elif algorithm_choice==2:\n",
    "            db['algorithm']='Adam'\n",
    "            db['epsilon']=my_epsilon\n",
    "        elif algorithm_choice==3:\n",
    "            db['algorithm']='Momentum'\n",
    "            db['momentum']=momentum_par\n",
    "        elif algorithm_choice==4:\n",
    "            db['algorithm']='Adagrad'\n",
    "        elif algorithm_choice==5:\n",
    "            db['algorithm']='RMSProp'\n",
    "\n",
    "        sio.savemat(wheretosave, db)        \n",
    "\n",
    "    step=step+1\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# COMPUTE TRAINED NETWORK OUTPUT\n",
    "\n",
    "o_output=zeros([1024, dur])\n",
    "\n",
    "batchsz=32\n",
    "for bbatch in range(32):\n",
    "    startind=(bbatch)*batchsz\n",
    "    endind=(bbatch+1)*batchsz\n",
    "    fd={batchsize_: train_loss_size, input_filt1_: input_bip1_train[:, startind:endind, :], \\\n",
    "             input_filt2_: input_bip2_train[:, startind:endind, :], input_filt3_: input_bip3_train[:, startind:endind, :],\\\n",
    "             input_filt4_: input_bip4_train[:, startind:endind, :], input_filt5_: input_bip5_train[:, startind:endind, :],\\\n",
    "             input_filt6_: input_bip6_train[:, startind:endind, :], input_filt7_: input_bip7_train[:, startind:endind, :],\\\n",
    "             input_filt8_: input_bip8_train[:, startind:endind, :], input_filt9_: input_bip9_train[:, startind:endind, :],\\\n",
    "             input_filt10_: input_bip10_train[:, startind:endind, :], input_filt11_: input_bip11_train[:, startind:endind, :],\\\n",
    "             input_filt12_: input_bip12_train[:, startind:endind, :], input_filt13_: input_bip13_train[:, startind:endind, :],\\\n",
    "             input_filt14_: input_bip14_train[:, startind:endind, :], input_filt15_: input_bip15_train[:, startind:endind, :],\\\n",
    "             input_filt16_: input_bip16_train[:, startind:endind, :]}\n",
    "    o_output[startind:endind, :]=np.reshape(sess.run([output], fd), [32, dur]) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# SAVE PARAMETER HISTORIES AND TRAINING HYPERPARAMSIB\n",
    "\n",
    "db={}\n",
    "\n",
    "# db['b1_out']=b1copy_out\n",
    "# db['am1_out']=am1_out\n",
    "# db['b11a1_mult']=b11a1_mult.eval(session=sess, feed_dict=singlefd)\n",
    "# db['b1copy_out']=b1copy_out\n",
    "# db['b2_out']=b2_out\n",
    "# db['b11_out']=b11_out\n",
    "# db['input1']=input1\n",
    "\n",
    "db['f1b1_syn_hist']=f1b1_syn_hist.eval(session=sess)\n",
    "db['f2b1_syn_hist']=f2b1_syn_hist.eval(session=sess)\n",
    "db['f3b1_syn_hist']=f3b1_syn_hist.eval(session=sess)\n",
    "db['f4b1_syn_hist']=f4b1_syn_hist.eval(session=sess)\n",
    "db['f5b1_syn_hist']=f5b1_syn_hist.eval(session=sess)\n",
    "db['f6b1_syn_hist']=f6b1_syn_hist.eval(session=sess)\n",
    "db['f7b1_syn_hist']=f7b1_syn_hist.eval(session=sess)\n",
    "db['f8b1_syn_hist']=f8b1_syn_hist.eval(session=sess)\n",
    "db['f9b1_syn_hist']=f9b1_syn_hist.eval(session=sess)\n",
    "db['f10b1_syn_hist']=f10b1_syn_hist.eval(session=sess)\n",
    "db['f11b1_syn_hist']=f11b1_syn_hist.eval(session=sess)\n",
    "db['f12b1_syn_hist']=f12b1_syn_hist.eval(session=sess)\n",
    "db['f13b1_syn_hist']=f13b1_syn_hist.eval(session=sess)\n",
    "db['f14b1_syn_hist']=f14b1_syn_hist.eval(session=sess)\n",
    "db['f15b1_syn_hist']=f15b1_syn_hist.eval(session=sess)\n",
    "db['f16b1_syn_hist']=f16b1_syn_hist.eval(session=sess)\n",
    "\n",
    "db['f1b2_syn_hist']=f1b2_syn_hist.eval(session=sess)\n",
    "db['f2b2_syn_hist']=f2b2_syn_hist.eval(session=sess)\n",
    "db['f3b2_syn_hist']=f3b2_syn_hist.eval(session=sess)\n",
    "db['f4b2_syn_hist']=f4b2_syn_hist.eval(session=sess)\n",
    "db['f5b2_syn_hist']=f5b2_syn_hist.eval(session=sess)\n",
    "db['f6b2_syn_hist']=f6b2_syn_hist.eval(session=sess)\n",
    "db['f7b2_syn_hist']=f7b2_syn_hist.eval(session=sess)\n",
    "db['f8b2_syn_hist']=f8b2_syn_hist.eval(session=sess)\n",
    "db['f9b2_syn_hist']=f9b2_syn_hist.eval(session=sess)\n",
    "db['f10b2_syn_hist']=f10b2_syn_hist.eval(session=sess)\n",
    "db['f11b2_syn_hist']=f11b2_syn_hist.eval(session=sess)\n",
    "db['f12b2_syn_hist']=f12b2_syn_hist.eval(session=sess)\n",
    "db['f13b2_syn_hist']=f13b2_syn_hist.eval(session=sess)\n",
    "db['f14b2_syn_hist']=f14b2_syn_hist.eval(session=sess)\n",
    "db['f15b2_syn_hist']=f15b2_syn_hist.eval(session=sess)\n",
    "db['f16b2_syn_hist']=f16b2_syn_hist.eval(session=sess)\n",
    "\n",
    "db['f1b11_syn_hist']=f1b11_syn_hist.eval(session=sess)\n",
    "db['f2b11_syn_hist']=f2b11_syn_hist.eval(session=sess)\n",
    "db['f3b11_syn_hist']=f3b11_syn_hist.eval(session=sess)\n",
    "db['f4b11_syn_hist']=f4b11_syn_hist.eval(session=sess)\n",
    "db['f5b11_syn_hist']=f5b11_syn_hist.eval(session=sess)\n",
    "db['f6b11_syn_hist']=f6b11_syn_hist.eval(session=sess)\n",
    "db['f7b11_syn_hist']=f7b11_syn_hist.eval(session=sess)\n",
    "db['f8b11_syn_hist']=f8b11_syn_hist.eval(session=sess)\n",
    "db['f9b11_syn_hist']=f9b11_syn_hist.eval(session=sess)\n",
    "db['f10b11_syn_hist']=f10b11_syn_hist.eval(session=sess)\n",
    "db['f11b11_syn_hist']=f11b11_syn_hist.eval(session=sess)\n",
    "db['f12b11_syn_hist']=f12b11_syn_hist.eval(session=sess)\n",
    "db['f13b11_syn_hist']=f13b11_syn_hist.eval(session=sess)\n",
    "db['f14b11_syn_hist']=f14b11_syn_hist.eval(session=sess)\n",
    "db['f15b11_syn_hist']=f15b11_syn_hist.eval(session=sess)\n",
    "db['f16b11_syn_hist']=f16b11_syn_hist.eval(session=sess)\n",
    "\n",
    "db['bip1_gc_syn_hist']=bip1_gc_syn_hist.eval(session=sess)\n",
    "db['bip2_gc_syn_hist']=bip2_gc_syn_hist.eval(session=sess)\n",
    "\n",
    "db['bip11_gc_syn_hist']=bip11_gc_syn_hist.eval(session=sess)\n",
    "\n",
    "db['bip1_copy_gc_syn_hist']=bip1_copy_gc_syn_hist.eval(session=sess)\n",
    "db['bip2_copy_gc_syn_hist']=bip2_copy_gc_syn_hist.eval(session=sess)\n",
    "\n",
    "db['bip11_copy_gc_syn_hist']=bip11_copy_gc_syn_hist.eval(session=sess)\n",
    "\n",
    "db['bip1_am1_syn_hist']=bip1_am1_syn_hist.eval(session=sess)\n",
    "db['bip2_am1_syn_hist']=bip2_am1_syn_hist.eval(session=sess)\n",
    "\n",
    "db['bip11_am1_syn_hist']=bip11_am1_syn_hist.eval(session=sess)\n",
    "\n",
    "db['am1_gc_syn_hist']=am1_gc_syn_hist.eval(session=sess)\n",
    "\n",
    "db['am1_b1copy_syn_hist']=am1_b1copy_syn_hist.eval(session=sess)\n",
    "db['am1_b2copy_syn_hist']=am1_b2copy_syn_hist.eval(session=sess)\n",
    "\n",
    "db['b1_bias_hist']=b1_bias_hist.eval(session=sess)\n",
    "db['b2_bias_hist']=b2_bias_hist.eval(session=sess)\n",
    "db['b11_bias_hist']=b11_bias_hist.eval(session=sess)\n",
    "\n",
    "db['am1_bias_hist']=am1_bias_hist.eval(session=sess)\n",
    "db['gc_bias_hist']=gc_bias_hist.eval(session=sess)\n",
    "db['gc_stretch_hist']=gc_stretch_hist.eval(session=sess)\n",
    "\n",
    "db['output']=o_output\n",
    "\n",
    "db['loss_hist']=loss_hist\n",
    "db['batch_loss_hist']=batch_loss_hist\n",
    "db['test_hist']=test_hist\n",
    "\n",
    "db['learning_rate']=learn_rate\n",
    "db['lambda']=lambda1\n",
    "db['batch_size']=batchsize\n",
    "db['no_data_ex']=no_data_ex\n",
    "\n",
    "db['datapath']=datapath\n",
    "\n",
    "db['L1_hist']=L1_hist\n",
    "db['output_hist']=output_hist.eval(session=sess)\n",
    "\n",
    "\n",
    "\n",
    "if algorithm_choice==1: \n",
    "    db['algorithm']='Gradient_Descent'\n",
    "elif algorithm_choice==2:\n",
    "    db['algorithm']='Adam'\n",
    "    db['epsilon']=my_epsilon\n",
    "elif algorithm_choice==3:\n",
    "    db['algorithm']='Momentum'\n",
    "    db['momentum']=momentum_par\n",
    "elif algorithm_choice==4:\n",
    "    db['algorithm']='Adagrad'\n",
    "elif algorithm_choice==5:\n",
    "    db['algorithm']='RMSProp'\n",
    "\n",
    "sio.savemat(wheretosave, db)\n",
    "print(wheretosave)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
