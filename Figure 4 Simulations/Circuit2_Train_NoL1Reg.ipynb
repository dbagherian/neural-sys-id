{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from __future__ import division\n",
    "import random\n",
    "import scipy\n",
    "import h5py\n",
    "import hdf5storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random seed will dictate the random initialization\n",
    "sd=30000\n",
    "np.random.seed(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98000, 100, 100, 1)\n",
      "(2000, 100, 100, 1)\n",
      "(1, 10, 10, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2800.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxker=28\n",
    "traindatapath='/home/ubuntu/Notebooks/Circuit2_Training_Data.h5'\n",
    "data = hdf5storage.loadmat(traindatapath)\n",
    "\n",
    "\n",
    "x_train = data['x_train']\n",
    "x_test = data['x_test']\n",
    "y_train = data['y_train']\n",
    "y_test = reshape(data['y_test'], [1, 2000])\n",
    "\n",
    "gc_bias_init = data['gc_bias']\n",
    "bipkernels = data['bipkernels']\n",
    "bip_gc_syn_init = data['bip_gc_syn']\n",
    "bip_am_syn_init = data['bip_am_syn']\n",
    "am_gc_syn_init = data['am_gc_syn']\n",
    "\n",
    "#sparsity params for weight matrix initializations\n",
    "init_sparsity = 0.0 \n",
    "init_sparsity_bg = 0.01\n",
    "\n",
    "\n",
    "bip_gc_syn_mask1 = np.random.rand(maxker*100, 1)\n",
    "bip_gc_syn_mask1 = reshape(bip_gc_syn_mask1, [1, 10, 10, maxker])\n",
    "bip_gc_syn_mask1 = bip_gc_syn_mask1 >(1.0 - init_sparsity_bg)\n",
    "\n",
    "\n",
    "bip_gc_syn_init_full = np.zeros([1, 10, 10, maxker])\n",
    "bip_gc_syn_mask_true = reshape(bip_gc_syn_init, [1, 10, 10, 3])>0.0\n",
    "bip_gc_syn_init_full[:, :, :, 0:3] = bip_gc_syn_mask_true\n",
    "\n",
    "bip_gc_syn_mask = np.maximum(bip_gc_syn_mask1, bip_gc_syn_init_full)\n",
    "\n",
    "bip_gc_syn_init11 = tf.random_uniform([1, 10, 10, maxker], minval=0.1, maxval=0.2, dtype=tf.float32)\n",
    "bip_gc_syn_init1=bip_gc_syn_init11\n",
    "\n",
    "bip_am_syn_mask = np.zeros([10, 10, maxker, 5, 5])\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        for k in range(maxker):\n",
    "            bip_am_syn_mask[i, j, k, int(floor(i/2)), int(floor(j/2))] = 1.0\n",
    "bip_am_syn_mask = bip_am_syn_mask.astype(float32)\n",
    "\n",
    "bip_am_syn_inds = np.zeros([maxker*100, 6])\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        for k in range(maxker):\n",
    "            bip_am_syn_inds[maxker*10*(i)+28*(j)+k]=[0, i, j, k, floor(i/2), floor(j/2)]\n",
    "bip_am_syn_inds = bip_am_syn_inds.astype(int64)\n",
    "\n",
    "\n",
    "bip_am_syn_init11 = abs(np.random.normal(0.0, (sqrt(2.0)/112.0), size=[maxker*100]))\n",
    "bip_am_syn_init11=bip_am_syn_init11.astype(float32)        \n",
    "\n",
    "am_gc_syn_init1 = tf.random_uniform([1, 5, 5], minval=0.1, maxval=0.2, dtype=tf.float32)\n",
    " \n",
    "\n",
    "print(shape(x_train))\n",
    "print(shape(x_test))\n",
    "print(shape(bip_gc_syn_init))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 98000)\n",
      "(1, 2000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbcd991bb90>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(shape(y_train))\n",
    "print(shape(y_test))\n",
    "plt.figure()\n",
    "plt.plot(squeeze(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_var(shape, initial_val):\n",
    "    initial = tf.constant(initial_val, shape=shape)\n",
    "    return tf.constant(initial_val) #initial\n",
    "\n",
    "def bip_conv2d(x, W):\n",
    "    padsize=10 \n",
    "    paddedx=tf.pad(x, [[0, 0], [padsize, padsize], [padsize, padsize], [0, 0]], 'CONSTANT')\n",
    "    outconv=tf.nn.conv2d(paddedx, W, strides=[1, 10, 10, 1], padding='SAME') #250 for movingdot and noise\n",
    "    return outconv[:, 1:11, 1:11, :]\n",
    "\n",
    "def synapse_var(shape, initial_val):\n",
    "#      initial=tf.constant(initial_val, shape=shape)\n",
    "#     initial = tf.random_uniform(shape, minval=0.1, maxval=0.8, dtype=tf.float32)\n",
    "    return tf.Variable(initial_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7fbcd888ddd0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 28 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## create layer 1 convolutional kernels (difference of gaussians)\n",
    "\n",
    "def difference_of_gaussians(ctr_sigma, surr_sigma, ctr_strength, surr_strength, x, y):\n",
    "    \n",
    "    center=0.4*(1/ctr_sigma)*exp(-0.5*square(sqrt(square(x)+square(y))/ctr_sigma))\n",
    "    \n",
    "    surround=0.4*(1/surr_sigma)*exp(-0.5*square(sqrt(square(x)+square(y))/surr_sigma))\n",
    "    \n",
    "    kernel = ctr_strength*center - surr_strength*surround\n",
    "    \n",
    "    maxk = amax(abs(kernel)) #normalization factor\n",
    "    \n",
    "    return kernel/maxk\n",
    "\n",
    "\n",
    "x = np.linspace(-5, 5, 11)\n",
    "y = np.linspace(-5, 5, 11)\n",
    "xv, yv = np.meshgrid(x, y)\n",
    "\n",
    "bipkernels = np.zeros([11, 11, 1, maxker])\n",
    "\n",
    "\n",
    "kernel1 = difference_of_gaussians(3, 6, 13, 12.9, xv, yv) \n",
    "kernel2 = difference_of_gaussians(5, 6, 18, 18, xv, yv)\n",
    "kernel3 = difference_of_gaussians(2, 4, 20, 14, xv, yv)\n",
    "kernel4 = difference_of_gaussians(3, 6, 13, 0, xv, yv)\n",
    "kernel5 = difference_of_gaussians(4, 6, 13, 0, xv, yv) \n",
    "kernel6 = difference_of_gaussians(2, 4, 20, 0, xv, yv)\n",
    "\n",
    "kernel7 = difference_of_gaussians(3, 6, 13, 20, xv, yv)\n",
    "kernel8 = difference_of_gaussians(5, 6, 18, 20, xv, yv) \n",
    "kernel9 = difference_of_gaussians(2, 4, 20, 24, xv, yv)\n",
    "\n",
    "kernel10 = difference_of_gaussians(5, 8, 13, 20, xv, yv)\n",
    "kernel11 = difference_of_gaussians(2, 8, 15, 15, xv, yv)\n",
    "kernel12 = difference_of_gaussians(3, 8, 20, 12, xv, yv)\n",
    "kernel13 = difference_of_gaussians(5, 8, 20, 18, xv, yv)\n",
    "kernel14 = difference_of_gaussians(2, 8, 13, 18, xv, yv)\n",
    "\n",
    "\n",
    "bipkernels[:, :, 0, 0]=kernel1\n",
    "bipkernels[:, :, 0, 1]=kernel2\n",
    "bipkernels[:, :, 0, 2]=kernel3\n",
    "bipkernels[:, :, 0, 3]=kernel4\n",
    "bipkernels[:, :, 0, 4]=-1.0*kernel1\n",
    "bipkernels[:, :, 0, 5]=-1.0*kernel2\n",
    "bipkernels[:, :, 0, 6]=-1.0*kernel3\n",
    "bipkernels[:, :, 0, 7]=-1.0*kernel4\n",
    "bipkernels[:, :, 0, 8]=kernel5\n",
    "bipkernels[:, :, 0, 9]=kernel6\n",
    "bipkernels[:, :, 0, 10]=kernel7\n",
    "bipkernels[:, :, 0, 11]=kernel8\n",
    "bipkernels[:, :, 0, 12]=-1.0*kernel5\n",
    "bipkernels[:, :, 0, 13]=-1.0*kernel6\n",
    "bipkernels[:, :, 0, 14]=-1.0*kernel7\n",
    "bipkernels[:, :, 0, 15]=-1.0*kernel8\n",
    "bipkernels[:, :, 0, 16]=kernel9\n",
    "bipkernels[:, :, 0, 17]=kernel10\n",
    "bipkernels[:, :, 0, 18]=kernel11\n",
    "bipkernels[:, :, 0, 19]=kernel12\n",
    "bipkernels[:, :, 0, 20]=-1.0*kernel9\n",
    "bipkernels[:, :, 0, 21]=-1.0*kernel10\n",
    "bipkernels[:, :, 0, 22]=-1.0*kernel11\n",
    "bipkernels[:, :, 0, 23]=-1.0*kernel12\n",
    "bipkernels[:, :, 0, 24]=kernel13\n",
    "bipkernels[:, :, 0, 25]=kernel14\n",
    "bipkernels[:, :, 0, 26]=-1.0*kernel13\n",
    "bipkernels[:, :, 0, 27]=-1.0*kernel14\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(4, 4, 1)\n",
    "plt.imshow(kernel1, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 2)\n",
    "plt.imshow(kernel2, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 3)\n",
    "plt.imshow(kernel3, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 4)\n",
    "plt.imshow(kernel4, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 5)\n",
    "plt.imshow(kernel5, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 6)\n",
    "plt.imshow(kernel6, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 7)\n",
    "plt.imshow(kernel7, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 8)\n",
    "plt.imshow(kernel8, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 9)\n",
    "plt.imshow(kernel9, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 10)\n",
    "plt.imshow(kernel10, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 11)\n",
    "plt.imshow(kernel11, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 12)\n",
    "plt.imshow(kernel12, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 13)\n",
    "plt.imshow(kernel13, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 14)\n",
    "plt.imshow(kernel14, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395704.03\n",
      "419092.6\n",
      "step: 0  loss: = 3952.943438\n",
      "step: 100  loss: = 3460.217813\n",
      "step: 200  loss: = 2545.715625\n",
      "step: 300  loss: = 1388.632969\n",
      "step: 400  loss: = 542.463828\n",
      "step: 500  loss: = 177.279512\n",
      "step: 600  loss: = 152.748418\n",
      "step: 700  loss: = 107.654395\n",
      "step: 800  loss: = 63.516787\n",
      "step: 900  loss: = 45.196865\n",
      "step: 1000  loss: = 40.055002\n",
      "step: 1100  loss: = 39.589634\n",
      "step: 1200  loss: = 38.488682\n",
      "step: 1300  loss: = 36.427927\n",
      "step: 1400  loss: = 33.554451\n",
      "step: 1500  loss: = 30.480408\n",
      "step: 1600  loss: = 30.071084\n",
      "step: 1700  loss: = 29.084900\n",
      "step: 1800  loss: = 27.254705\n",
      "step: 1900  loss: = 24.908809\n",
      "step: 2000  loss: = 22.688672\n",
      "step: 2100  loss: = 22.414844\n",
      "step: 2200  loss: = 21.776597\n",
      "step: 2300  loss: = 20.656089\n",
      "step: 2400  loss: = 19.319047\n",
      "step: 2500  loss: = 18.175927\n",
      "step: 2600  loss: = 18.043683\n",
      "step: 2700  loss: = 17.737500\n",
      "step: 2800  loss: = 17.214187\n",
      "step: 2900  loss: = 16.609396\n",
      "step: 3000  loss: = 16.152628\n",
      "step: 3100  loss: = 16.102661\n",
      "step: 3200  loss: = 15.984771\n",
      "step: 3300  loss: = 15.789565\n",
      "step: 3400  loss: = 15.577393\n",
      "step: 3500  loss: = 15.422407\n",
      "step: 3600  loss: = 15.405223\n",
      "step: 3700  loss: = 15.370140\n",
      "step: 3800  loss: = 15.320186\n",
      "step: 3900  loss: = 15.273314\n",
      "step: 4000  loss: = 15.243890\n",
      "step: 4100  loss: = 15.240990\n",
      "step: 4200  loss: = 15.235471\n",
      "step: 4300  loss: = 15.224154\n",
      "step: 4400  loss: = 15.213103\n",
      "step: 4500  loss: = 15.204288\n",
      "step: 4600  loss: = 15.203583\n",
      "step: 4700  loss: = 15.201511\n",
      "step: 4800  loss: = 15.198832\n",
      "step: 4900  loss: = 15.196985\n",
      "step: 5000  loss: = 15.196289\n",
      "step: 5100  loss: = 15.196101\n",
      "step: 5200  loss: = 15.196052\n",
      "step: 5300  loss: = 15.196041\n",
      "step: 5400  loss: = 15.196036\n",
      "step: 5500  loss: = 15.196034\n",
      "step: 5600  loss: = 15.196036\n",
      "step: 5700  loss: = 15.196034\n",
      "step: 5800  loss: = 15.196033\n",
      "step: 5900  loss: = 15.196029\n",
      "step: 6000  loss: = 15.196028\n",
      "step: 6100  loss: = 15.196028\n",
      "step: 6200  loss: = 15.196033\n",
      "step: 6300  loss: = 15.196034\n",
      "step: 6400  loss: = 15.196036\n",
      "step: 6500  loss: = 15.196039\n",
      "step: 6600  loss: = 15.196029\n",
      "completed data: 60  kernels: =  3.000000\n",
      "1307842.0\n",
      "1262204.9\n",
      "step: 0  loss: = 13063.715000\n",
      "step: 100  loss: = 11298.698750\n",
      "step: 200  loss: = 8078.630000\n",
      "step: 300  loss: = 4149.760938\n",
      "step: 400  loss: = 1451.155625\n",
      "step: 500  loss: = 389.644805\n",
      "step: 600  loss: = 323.778828\n",
      "step: 700  loss: = 206.313477\n",
      "step: 800  loss: = 99.917500\n",
      "step: 900  loss: = 62.146763\n",
      "step: 1000  loss: = 52.877832\n",
      "step: 1100  loss: = 51.983984\n",
      "step: 1200  loss: = 49.821138\n",
      "step: 1300  loss: = 45.649292\n",
      "step: 1400  loss: = 39.835303\n",
      "step: 1500  loss: = 33.826387\n",
      "step: 1600  loss: = 33.049495\n",
      "step: 1700  loss: = 31.196790\n",
      "step: 1800  loss: = 27.863259\n",
      "step: 1900  loss: = 23.876199\n",
      "step: 2000  loss: = 20.484761\n",
      "step: 2100  loss: = 20.100352\n",
      "step: 2200  loss: = 19.232253\n",
      "step: 2300  loss: = 17.847222\n",
      "step: 2400  loss: = 16.498608\n",
      "step: 2500  loss: = 15.679463\n",
      "step: 2600  loss: = 15.610547\n",
      "step: 2700  loss: = 15.475343\n",
      "step: 2800  loss: = 15.330621\n",
      "step: 2900  loss: = 15.313689\n",
      "step: 3000  loss: = 15.415308\n",
      "step: 3100  loss: = 15.432960\n",
      "step: 3200  loss: = 15.476299\n",
      "step: 3300  loss: = 15.555959\n",
      "step: 3400  loss: = 15.643358\n",
      "completed data: 60  kernels: =  8.000000\n",
      "2457780.8\n",
      "2427006.5\n",
      "step: 0  loss: = 24536.832500\n",
      "step: 100  loss: = 19751.873750\n",
      "step: 200  loss: = 11852.036250\n",
      "step: 300  loss: = 4157.892187\n",
      "step: 400  loss: = 756.014609\n",
      "step: 500  loss: = 118.534004\n",
      "step: 600  loss: = 99.509424\n",
      "step: 700  loss: = 72.677300\n",
      "step: 800  loss: = 56.624829\n",
      "step: 900  loss: = 49.308936\n",
      "step: 1000  loss: = 42.741704\n",
      "step: 1100  loss: = 41.880913\n",
      "step: 1200  loss: = 39.812795\n",
      "step: 1300  loss: = 36.068650\n",
      "step: 1400  loss: = 31.570864\n",
      "step: 1500  loss: = 27.831902\n",
      "step: 1600  loss: = 27.412937\n",
      "step: 1700  loss: = 26.473516\n",
      "step: 1800  loss: = 24.988684\n",
      "step: 1900  loss: = 23.548721\n",
      "step: 2000  loss: = 22.627178\n",
      "step: 2100  loss: = 22.542615\n",
      "step: 2200  loss: = 22.363364\n",
      "step: 2300  loss: = 22.117080\n",
      "step: 2400  loss: = 21.930430\n",
      "step: 2500  loss: = 21.844226\n",
      "step: 2600  loss: = 21.839124\n",
      "step: 2700  loss: = 21.826438\n",
      "step: 2800  loss: = 21.809331\n",
      "step: 2900  loss: = 21.794270\n",
      "step: 3000  loss: = 21.783452\n",
      "step: 3100  loss: = 21.782412\n",
      "step: 3200  loss: = 21.779958\n",
      "step: 3300  loss: = 21.776748\n",
      "step: 3400  loss: = 21.774604\n",
      "step: 3500  loss: = 21.773943\n",
      "step: 3600  loss: = 21.773921\n",
      "step: 3700  loss: = 21.773889\n",
      "step: 3800  loss: = 21.773833\n",
      "step: 3900  loss: = 21.773816\n",
      "step: 4000  loss: = 21.773801\n",
      "step: 4100  loss: = 21.773799\n",
      "step: 4200  loss: = 21.773801\n",
      "step: 4300  loss: = 21.773787\n",
      "step: 4400  loss: = 21.773779\n",
      "step: 4500  loss: = 21.773789\n",
      "step: 4600  loss: = 21.773787\n",
      "step: 4700  loss: = 21.773792\n",
      "step: 4800  loss: = 21.773782\n",
      "step: 4900  loss: = 21.773782\n",
      "step: 5000  loss: = 21.773787\n",
      "step: 5100  loss: = 21.773782\n",
      "step: 5200  loss: = 21.773779\n",
      "step: 5300  loss: = 21.773796\n",
      "step: 5400  loss: = 21.773777\n",
      "step: 5500  loss: = 21.773774\n",
      "step: 5600  loss: = 21.773774\n",
      "step: 5700  loss: = 21.773779\n",
      "step: 5800  loss: = 21.773767\n",
      "step: 5900  loss: = 21.773777\n",
      "step: 6000  loss: = 21.756423\n",
      "step: 6100  loss: = 21.716982\n",
      "step: 6200  loss: = 21.766045\n",
      "step: 6300  loss: = 21.761221\n",
      "step: 6400  loss: = 21.761099\n",
      "step: 6500  loss: = 21.761113\n",
      "step: 6600  loss: = 21.761450\n",
      "step: 6700  loss: = 21.767288\n",
      "step: 6800  loss: = 21.781685\n",
      "step: 6900  loss: = 21.781694\n",
      "completed data: 60  kernels: = 16.000000\n",
      "11614634.0\n",
      "11510115.0\n",
      "step: 0  loss: = 115946.550000\n",
      "step: 100  loss: = 92654.710000\n",
      "step: 200  loss: = 54612.255000\n",
      "step: 300  loss: = 18435.970000\n",
      "step: 400  loss: = 3075.290625\n",
      "step: 500  loss: = 345.545820\n",
      "step: 600  loss: = 267.679551\n",
      "step: 700  loss: = 159.917939\n",
      "step: 800  loss: = 101.432139\n",
      "step: 900  loss: = 86.284990\n",
      "step: 1000  loss: = 77.147813\n",
      "step: 1100  loss: = 75.968481\n",
      "step: 1200  loss: = 73.112285\n",
      "step: 1300  loss: = 67.826235\n",
      "step: 1400  loss: = 61.300020\n",
      "step: 1500  loss: = 55.627646\n",
      "step: 1600  loss: = 54.970420\n",
      "step: 1700  loss: = 53.480259\n",
      "step: 1800  loss: = 51.090576\n",
      "step: 1900  loss: = 48.694858\n",
      "step: 2000  loss: = 47.119155\n",
      "step: 2100  loss: = 46.967837\n",
      "step: 2200  loss: = 46.641265\n",
      "step: 2300  loss: = 46.205254\n",
      "step: 2400  loss: = 45.880684\n",
      "step: 2500  loss: = 45.767021\n",
      "step: 2600  loss: = 45.767642\n",
      "step: 2700  loss: = 45.774312\n",
      "step: 2800  loss: = 45.809131\n",
      "step: 2900  loss: = 45.874243\n",
      "step: 3000  loss: = 45.943208\n",
      "completed data: 60  kernels: = 28.000000\n",
      "430200.8\n",
      "465177.78\n",
      "step: 0  loss: = 4278.281563\n",
      "step: 100  loss: = 1999.715312\n",
      "step: 200  loss: = 231.272520\n",
      "step: 300  loss: = 35.048992\n",
      "step: 400  loss: = 18.495809\n",
      "step: 500  loss: =  7.936149\n",
      "step: 600  loss: =  6.993286\n",
      "step: 700  loss: =  5.149886\n",
      "step: 800  loss: =  3.157624\n",
      "step: 900  loss: =  1.814856\n",
      "step: 1000  loss: =  1.081906\n",
      "step: 1100  loss: =  1.008725\n",
      "step: 1200  loss: =  0.862708\n",
      "step: 1300  loss: =  0.647312\n",
      "step: 1400  loss: =  0.466411\n",
      "step: 1500  loss: =  0.348524\n",
      "step: 1600  loss: =  0.337262\n",
      "step: 1700  loss: =  0.313773\n",
      "step: 1800  loss: =  0.278681\n",
      "step: 1900  loss: =  0.246609\n",
      "step: 2000  loss: =  0.217333\n",
      "step: 2100  loss: =  0.213030\n",
      "step: 2200  loss: =  0.206557\n",
      "step: 2300  loss: =  0.197559\n",
      "step: 2400  loss: =  0.184387\n",
      "step: 2500  loss: =  0.172046\n",
      "step: 2600  loss: =  0.170664\n",
      "step: 2700  loss: =  0.167305\n",
      "step: 2800  loss: =  0.166761\n",
      "step: 2900  loss: =  0.160206\n",
      "step: 3000  loss: =  0.151572\n",
      "step: 3100  loss: =  0.150142\n",
      "step: 3200  loss: =  0.147880\n",
      "step: 3300  loss: =  0.145301\n",
      "step: 3400  loss: =  0.142099\n",
      "step: 3500  loss: =  0.138561\n",
      "step: 3600  loss: =  0.136698\n",
      "step: 3700  loss: =  0.135153\n",
      "step: 3800  loss: =  0.134551\n",
      "step: 3900  loss: =  0.130981\n",
      "step: 4000  loss: =  0.130245\n",
      "step: 4100  loss: =  0.127208\n",
      "step: 4200  loss: =  0.125830\n",
      "step: 4300  loss: =  0.124730\n",
      "step: 4400  loss: =  0.123750\n",
      "step: 4500  loss: =  0.121766\n",
      "step: 4600  loss: =  0.120479\n",
      "step: 4700  loss: =  0.119352\n",
      "step: 4800  loss: =  0.117817\n",
      "step: 4900  loss: =  0.118569\n",
      "step: 5000  loss: =  0.114648\n",
      "step: 5100  loss: =  0.118528\n",
      "step: 5200  loss: =  0.114959\n",
      "step: 5300  loss: =  0.110335\n",
      "step: 5400  loss: =  0.113071\n",
      "step: 5500  loss: =  0.112081\n",
      "step: 5600  loss: =  0.108066\n",
      "step: 5700  loss: =  0.104163\n",
      "step: 5800  loss: =  0.102597\n",
      "step: 5900  loss: =  0.106123\n",
      "step: 6000  loss: =  0.103189\n",
      "step: 6100  loss: =  0.102553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6200  loss: =  0.097293\n",
      "step: 6300  loss: =  0.102717\n",
      "step: 6400  loss: =  0.098668\n",
      "step: 6500  loss: =  0.093027\n",
      "step: 6600  loss: =  0.097036\n",
      "step: 6700  loss: =  0.091022\n",
      "step: 6800  loss: =  0.093962\n",
      "step: 6900  loss: =  0.089325\n",
      "step: 7000  loss: =  0.089741\n",
      "step: 7100  loss: =  0.091463\n",
      "step: 7200  loss: =  0.097740\n",
      "step: 7300  loss: =  0.090458\n",
      "completed data: 340  kernels: =  3.000000\n",
      "853384.4\n",
      "876623.4\n",
      "step: 0  loss: = 8471.750000\n",
      "step: 100  loss: = 3021.862813\n",
      "step: 200  loss: = 150.319121\n",
      "step: 300  loss: = 42.443970\n",
      "step: 400  loss: = 21.351282\n",
      "step: 500  loss: =  7.270277\n",
      "step: 600  loss: =  6.015239\n",
      "step: 700  loss: =  3.588557\n",
      "step: 800  loss: =  1.685665\n",
      "step: 900  loss: =  0.764732\n",
      "step: 1000  loss: =  0.321787\n",
      "step: 1100  loss: =  0.285161\n",
      "step: 1200  loss: =  0.205709\n",
      "step: 1300  loss: =  0.109955\n",
      "step: 1400  loss: =  0.046469\n",
      "step: 1500  loss: =  0.013542\n",
      "step: 1600  loss: =  0.010858\n",
      "step: 1700  loss: =  0.005515\n",
      "step: 1800  loss: =  0.001068\n",
      "step: 1900  loss: =  0.000140\n",
      "step: 2000  loss: =  0.000003\n",
      "step: 2100  loss: =  0.000000\n",
      "step: 2200  loss: =  0.000139\n",
      "step: 2300  loss: =  0.000041\n",
      "step: 2400  loss: =  0.000687\n",
      "step: 2500  loss: =  0.000005\n",
      "step: 2600  loss: =  0.000000\n",
      "step: 2700  loss: =  0.000010\n",
      "step: 2800  loss: =  0.000649\n",
      "step: 2900  loss: =  0.000104\n",
      "step: 3000  loss: =  0.000444\n",
      "step: 3100  loss: =  0.000000\n",
      "step: 3200  loss: =  0.000006\n",
      "completed data: 340  kernels: =  8.000000\n",
      "3233928.5\n",
      "3068900.5\n",
      "step: 0  loss: = 32071.740000\n",
      "step: 100  loss: = 9699.815000\n",
      "step: 200  loss: = 225.169141\n",
      "step: 300  loss: = 44.235664\n",
      "step: 400  loss: = 22.765217\n",
      "step: 500  loss: =  8.561879\n",
      "step: 600  loss: =  6.993363\n",
      "step: 700  loss: =  3.544464\n",
      "step: 800  loss: =  0.917439\n",
      "step: 900  loss: =  0.133467\n",
      "step: 1000  loss: =  0.005382\n",
      "step: 1100  loss: =  0.002486\n",
      "step: 1200  loss: =  0.000164\n",
      "step: 1300  loss: =  0.016870\n",
      "step: 1400  loss: =  0.000012\n",
      "step: 1500  loss: =  0.000105\n",
      "step: 1600  loss: =  0.000000\n",
      "step: 1700  loss: =  0.000097\n",
      "step: 1800  loss: =  0.000057\n",
      "step: 1900  loss: =  0.003774\n",
      "step: 2000  loss: =  0.000001\n",
      "step: 2100  loss: =  0.000000\n",
      "step: 2200  loss: =  0.000471\n",
      "step: 2300  loss: =  0.000949\n",
      "step: 2400  loss: =  0.006623\n",
      "step: 2500  loss: =  0.002319\n",
      "step: 2600  loss: =  0.000000\n",
      "completed data: 340  kernels: = 16.000000\n",
      "13922044.0\n",
      "14122104.0\n",
      "step: 0  loss: = 137987.310000\n",
      "step: 100  loss: = 37927.445000\n",
      "step: 200  loss: = 496.565000\n",
      "step: 300  loss: = 112.056299\n",
      "step: 400  loss: = 56.167388\n",
      "step: 500  loss: = 18.261150\n",
      "step: 600  loss: = 14.335332\n",
      "step: 700  loss: =  6.127957\n",
      "step: 800  loss: =  1.312450\n",
      "step: 900  loss: =  0.164440\n",
      "step: 1000  loss: =  0.004386\n",
      "step: 1100  loss: =  0.001581\n",
      "step: 1200  loss: =  0.000072\n",
      "step: 1300  loss: =  0.000041\n",
      "step: 1400  loss: =  0.001498\n",
      "step: 1500  loss: =  0.003600\n",
      "step: 1600  loss: =  0.000003\n",
      "step: 1700  loss: =  0.000948\n",
      "step: 1800  loss: =  0.000093\n",
      "step: 1900  loss: =  0.028815\n",
      "completed data: 340  kernels: = 28.000000\n",
      "398367.8\n",
      "407663.16\n",
      "step: 0  loss: = 3936.345938\n",
      "step: 100  loss: = 608.336172\n",
      "step: 200  loss: = 36.746672\n",
      "step: 300  loss: =  8.976558\n",
      "step: 400  loss: =  2.995970\n",
      "step: 500  loss: =  1.173920\n",
      "step: 600  loss: =  1.024524\n",
      "step: 700  loss: =  0.779699\n",
      "step: 800  loss: =  0.552503\n",
      "step: 900  loss: =  0.433231\n",
      "step: 1000  loss: =  0.378702\n",
      "step: 1100  loss: =  0.378595\n",
      "step: 1200  loss: =  0.366688\n",
      "step: 1300  loss: =  0.346748\n",
      "step: 1400  loss: =  0.329519\n",
      "step: 1500  loss: =  0.314920\n",
      "step: 1600  loss: =  0.312983\n",
      "step: 1700  loss: =  0.312496\n",
      "step: 1800  loss: =  0.298702\n",
      "step: 1900  loss: =  0.299101\n",
      "step: 2000  loss: =  0.273356\n",
      "step: 2100  loss: =  0.274314\n",
      "step: 2200  loss: =  0.270509\n",
      "step: 2300  loss: =  0.269526\n",
      "step: 2400  loss: =  0.257428\n",
      "step: 2500  loss: =  0.252271\n",
      "step: 2600  loss: =  0.250590\n",
      "step: 2700  loss: =  0.248015\n",
      "step: 2800  loss: =  0.243220\n",
      "step: 2900  loss: =  0.238891\n",
      "step: 3000  loss: =  0.234453\n",
      "step: 3100  loss: =  0.232590\n",
      "step: 3200  loss: =  0.233199\n",
      "step: 3300  loss: =  0.228615\n",
      "step: 3400  loss: =  0.223954\n",
      "step: 3500  loss: =  0.218151\n",
      "step: 3600  loss: =  0.219211\n",
      "step: 3700  loss: =  0.219761\n",
      "step: 3800  loss: =  0.216604\n",
      "step: 3900  loss: =  0.216818\n",
      "step: 4000  loss: =  0.209944\n",
      "step: 4100  loss: =  0.208150\n",
      "completed data: 700  kernels: =  3.000000\n",
      "925477.25\n",
      "921652.5\n",
      "step: 0  loss: = 9120.851250\n",
      "step: 100  loss: = 830.413047\n",
      "step: 200  loss: = 38.593633\n",
      "step: 300  loss: =  7.858369\n",
      "step: 400  loss: =  1.803702\n",
      "step: 500  loss: =  0.669262\n",
      "step: 600  loss: =  0.591715\n",
      "step: 700  loss: =  0.464891\n",
      "step: 800  loss: =  0.327245\n",
      "step: 900  loss: =  0.256358\n",
      "step: 1000  loss: =  0.162510\n",
      "step: 1100  loss: =  0.155217\n",
      "step: 1200  loss: =  0.142120\n",
      "step: 1300  loss: =  0.131852\n",
      "step: 1400  loss: =  0.107762\n",
      "step: 1500  loss: =  0.091392\n",
      "step: 1600  loss: =  0.088933\n",
      "step: 1700  loss: =  0.087537\n",
      "step: 1800  loss: =  0.088370\n",
      "step: 1900  loss: =  0.080016\n",
      "step: 2000  loss: =  0.063723\n",
      "step: 2100  loss: =  0.066011\n",
      "step: 2200  loss: =  0.064322\n",
      "step: 2300  loss: =  0.060331\n",
      "step: 2400  loss: =  0.060238\n",
      "step: 2500  loss: =  0.052057\n",
      "step: 2600  loss: =  0.051711\n",
      "step: 2700  loss: =  0.050019\n",
      "step: 2800  loss: =  0.049391\n",
      "step: 2900  loss: =  0.045700\n",
      "step: 3000  loss: =  0.044709\n",
      "step: 3100  loss: =  0.042934\n",
      "step: 3200  loss: =  0.042148\n",
      "step: 3300  loss: =  0.039909\n",
      "step: 3400  loss: =  0.038043\n",
      "step: 3500  loss: =  0.036560\n",
      "step: 3600  loss: =  0.036657\n",
      "step: 3700  loss: =  0.037731\n",
      "step: 3800  loss: =  0.040946\n",
      "step: 3900  loss: =  0.034861\n",
      "step: 4000  loss: =  0.031366\n",
      "completed data: 700  kernels: =  8.000000\n",
      "3319102.8\n",
      "3328070.8\n",
      "step: 0  loss: = 32632.752500\n",
      "step: 100  loss: = 1697.387813\n",
      "step: 200  loss: = 54.435762\n",
      "step: 300  loss: =  9.759395\n",
      "step: 400  loss: =  2.125665\n",
      "step: 500  loss: =  0.479589\n",
      "step: 600  loss: =  0.381693\n",
      "step: 700  loss: =  0.234626\n",
      "step: 800  loss: =  0.106720\n",
      "step: 900  loss: =  0.041105\n",
      "step: 1000  loss: =  0.015135\n",
      "step: 1100  loss: =  0.012224\n",
      "step: 1200  loss: =  0.007965\n",
      "step: 1300  loss: =  0.004806\n",
      "step: 1400  loss: =  0.004191\n",
      "step: 1500  loss: =  0.003388\n",
      "step: 1600  loss: =  0.000083\n",
      "step: 1700  loss: =  0.000828\n",
      "step: 1800  loss: =  0.001700\n",
      "step: 1900  loss: =  0.002780\n",
      "step: 2000  loss: =  0.000280\n",
      "step: 2100  loss: =  0.000009\n",
      "completed data: 700  kernels: = 16.000000\n",
      "9208373.0\n",
      "8948187.0\n",
      "step: 0  loss: = 90109.140000\n",
      "step: 100  loss: = 1341.201719\n",
      "step: 200  loss: = 60.143560\n",
      "step: 300  loss: = 12.510339\n",
      "step: 400  loss: =  1.388579\n",
      "step: 500  loss: =  0.194838\n",
      "step: 600  loss: =  0.140932\n",
      "step: 700  loss: =  0.095956\n",
      "step: 800  loss: =  0.028659\n",
      "step: 900  loss: =  0.022168\n",
      "step: 1000  loss: =  0.006480\n",
      "step: 1100  loss: =  0.000576\n",
      "step: 1200  loss: =  0.000434\n",
      "step: 1300  loss: =  0.000363\n",
      "step: 1400  loss: =  0.028503\n",
      "step: 1500  loss: =  0.008808\n",
      "step: 1600  loss: =  0.000009\n",
      "step: 1700  loss: =  0.002154\n",
      "step: 1800  loss: =  0.004546\n",
      "step: 1900  loss: =  0.012081\n",
      "step: 2000  loss: =  0.002525\n",
      "step: 2100  loss: =  0.000028\n",
      "completed data: 700  kernels: = 28.000000\n",
      "475556.97\n",
      "452063.3\n",
      "step: 0  loss: = 4653.753437\n",
      "step: 100  loss: = 69.552837\n",
      "step: 200  loss: = 10.319023\n",
      "step: 300  loss: =  2.191671\n",
      "step: 400  loss: =  0.554608\n",
      "step: 500  loss: =  0.378260\n",
      "step: 600  loss: =  0.370824\n",
      "step: 700  loss: =  0.360313\n",
      "step: 800  loss: =  0.346011\n",
      "step: 900  loss: =  0.330696\n",
      "step: 1000  loss: =  0.299372\n",
      "step: 1100  loss: =  0.298056\n",
      "step: 1200  loss: =  0.297804\n",
      "step: 1300  loss: =  0.289397\n",
      "step: 1400  loss: =  0.279859\n",
      "step: 1500  loss: =  0.268295\n",
      "step: 1600  loss: =  0.267477\n",
      "step: 1700  loss: =  0.263985\n",
      "step: 1800  loss: =  0.260977\n",
      "step: 1900  loss: =  0.254425\n",
      "step: 2000  loss: =  0.238821\n",
      "step: 2100  loss: =  0.237075\n",
      "step: 2200  loss: =  0.238001\n",
      "step: 2300  loss: =  0.228111\n",
      "step: 2400  loss: =  0.222870\n",
      "step: 2500  loss: =  0.213869\n",
      "step: 2600  loss: =  0.212421\n",
      "step: 2700  loss: =  0.210249\n",
      "step: 2800  loss: =  0.213287\n",
      "step: 2900  loss: =  0.197886\n",
      "step: 3000  loss: =  0.190751\n",
      "step: 3100  loss: =  0.190056\n",
      "step: 3200  loss: =  0.183902\n",
      "step: 3300  loss: =  0.180291\n",
      "step: 3400  loss: =  0.178081\n",
      "step: 3500  loss: =  0.165128\n",
      "step: 3600  loss: =  0.164626\n",
      "step: 3700  loss: =  0.164153\n",
      "step: 3800  loss: =  0.160602\n",
      "step: 3900  loss: =  0.157180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4000  loss: =  0.151446\n",
      "step: 4100  loss: =  0.152559\n",
      "step: 4200  loss: =  0.150971\n",
      "step: 4300  loss: =  0.147391\n",
      "step: 4400  loss: =  0.146564\n",
      "step: 4500  loss: =  0.141914\n",
      "step: 4600  loss: =  0.140023\n",
      "step: 4700  loss: =  0.139431\n",
      "step: 4800  loss: =  0.140727\n",
      "step: 4900  loss: =  0.138788\n",
      "step: 5000  loss: =  0.138377\n",
      "step: 5100  loss: =  0.133035\n",
      "step: 5200  loss: =  0.129055\n",
      "step: 5300  loss: =  0.139286\n",
      "step: 5400  loss: =  0.128408\n",
      "step: 5500  loss: =  0.123718\n",
      "step: 5600  loss: =  0.126066\n",
      "step: 5700  loss: =  0.120888\n",
      "step: 5800  loss: =  0.134015\n",
      "step: 5900  loss: =  0.122936\n",
      "step: 6000  loss: =  0.119891\n",
      "step: 6100  loss: =  0.120719\n",
      "step: 6200  loss: =  0.117030\n",
      "step: 6300  loss: =  0.115836\n",
      "step: 6400  loss: =  0.112786\n",
      "step: 6500  loss: =  0.107231\n",
      "step: 6600  loss: =  0.103652\n",
      "step: 6700  loss: =  0.101103\n",
      "step: 6800  loss: =  0.098333\n",
      "step: 6900  loss: =  0.096461\n",
      "step: 7000  loss: =  0.091415\n",
      "step: 7100  loss: =  0.093039\n",
      "step: 7200  loss: =  0.087880\n",
      "step: 7300  loss: =  0.081650\n",
      "step: 7400  loss: =  0.077694\n",
      "step: 7500  loss: =  0.075512\n",
      "step: 7600  loss: =  0.072066\n",
      "step: 7700  loss: =  0.069679\n",
      "step: 7800  loss: =  0.067107\n",
      "step: 7900  loss: =  0.068417\n",
      "step: 8000  loss: =  0.063642\n",
      "step: 8100  loss: =  0.063405\n",
      "step: 8200  loss: =  0.063705\n",
      "step: 8300  loss: =  0.062509\n",
      "step: 8400  loss: =  0.063462\n",
      "step: 8500  loss: =  0.061565\n",
      "step: 8600  loss: =  0.065568\n",
      "step: 8700  loss: =  0.062241\n",
      "step: 8800  loss: =  0.061856\n",
      "step: 8900  loss: =  0.062251\n",
      "step: 9000  loss: =  0.063279\n",
      "step: 9100  loss: =  0.065951\n",
      "step: 9200  loss: =  0.064454\n",
      "step: 9300  loss: =  0.063200\n",
      "completed data: 1400  kernels: =  3.000000\n",
      "812784.06\n",
      "770079.94\n",
      "step: 0  loss: = 7884.125000\n",
      "step: 100  loss: = 44.698979\n",
      "step: 200  loss: =  6.741357\n",
      "step: 300  loss: =  1.706248\n",
      "step: 400  loss: =  0.662735\n",
      "step: 500  loss: =  0.412503\n",
      "step: 600  loss: =  0.394944\n",
      "step: 700  loss: =  0.368543\n",
      "step: 800  loss: =  0.348002\n",
      "step: 900  loss: =  0.314274\n",
      "step: 1000  loss: =  0.292433\n",
      "step: 1100  loss: =  0.286614\n",
      "step: 1200  loss: =  0.281271\n",
      "step: 1300  loss: =  0.272211\n",
      "step: 1400  loss: =  0.274373\n",
      "step: 1500  loss: =  0.253952\n",
      "step: 1600  loss: =  0.252606\n",
      "step: 1700  loss: =  0.250292\n",
      "step: 1800  loss: =  0.263648\n",
      "step: 1900  loss: =  0.241689\n",
      "step: 2000  loss: =  0.237339\n",
      "step: 2100  loss: =  0.234098\n",
      "step: 2200  loss: =  0.235926\n",
      "step: 2300  loss: =  0.231131\n",
      "step: 2400  loss: =  0.244656\n",
      "step: 2500  loss: =  0.222469\n",
      "step: 2600  loss: =  0.222112\n",
      "step: 2700  loss: =  0.222328\n",
      "step: 2800  loss: =  0.221030\n",
      "step: 2900  loss: =  0.219971\n",
      "step: 3000  loss: =  0.217751\n",
      "step: 3100  loss: =  0.214778\n",
      "step: 3200  loss: =  0.213314\n",
      "step: 3300  loss: =  0.215144\n",
      "step: 3400  loss: =  0.234435\n",
      "step: 3500  loss: =  0.212013\n",
      "step: 3600  loss: =  0.209410\n",
      "step: 3700  loss: =  0.212402\n",
      "step: 3800  loss: =  0.209889\n",
      "step: 3900  loss: =  0.240949\n",
      "step: 4000  loss: =  0.207528\n",
      "step: 4100  loss: =  0.206050\n",
      "step: 4200  loss: =  0.203195\n",
      "step: 4300  loss: =  0.206626\n",
      "step: 4400  loss: =  0.203243\n",
      "step: 4500  loss: =  0.201005\n",
      "step: 4600  loss: =  0.200751\n",
      "step: 4700  loss: =  0.200917\n",
      "step: 4800  loss: =  0.204622\n",
      "step: 4900  loss: =  0.197704\n",
      "step: 5000  loss: =  0.201199\n",
      "step: 5100  loss: =  0.212124\n",
      "step: 5200  loss: =  0.197080\n",
      "completed data: 1400  kernels: =  8.000000\n",
      "4080806.0\n",
      "4037139.2\n",
      "step: 0  loss: = 39566.180000\n",
      "step: 100  loss: = 62.070161\n",
      "step: 200  loss: = 11.043414\n",
      "step: 300  loss: =  1.222028\n",
      "step: 400  loss: =  0.380894\n",
      "step: 500  loss: =  0.249179\n",
      "step: 600  loss: =  0.250529\n",
      "step: 700  loss: =  0.216519\n",
      "step: 800  loss: =  0.211862\n",
      "step: 900  loss: =  0.179552\n",
      "step: 1000  loss: =  0.138372\n",
      "step: 1100  loss: =  0.135178\n",
      "step: 1200  loss: =  0.136460\n",
      "step: 1300  loss: =  0.133300\n",
      "step: 1400  loss: =  0.127910\n",
      "step: 1500  loss: =  0.114962\n",
      "step: 1600  loss: =  0.102949\n",
      "step: 1700  loss: =  0.106930\n",
      "step: 1800  loss: =  0.096873\n",
      "step: 1900  loss: =  0.098934\n",
      "step: 2000  loss: =  0.089261\n",
      "step: 2100  loss: =  0.082944\n",
      "step: 2200  loss: =  0.085537\n",
      "step: 2300  loss: =  0.083479\n",
      "step: 2400  loss: =  0.089093\n",
      "step: 2500  loss: =  0.075306\n",
      "step: 2600  loss: =  0.069094\n",
      "step: 2700  loss: =  0.067376\n",
      "step: 2800  loss: =  0.075626\n",
      "step: 2900  loss: =  0.068510\n",
      "step: 3000  loss: =  0.061929\n",
      "step: 3100  loss: =  0.058879\n",
      "step: 3200  loss: =  0.060465\n",
      "step: 3300  loss: =  0.066287\n",
      "step: 3400  loss: =  0.058454\n",
      "step: 3500  loss: =  0.055957\n",
      "step: 3600  loss: =  0.053619\n",
      "step: 3700  loss: =  0.063430\n",
      "step: 3800  loss: =  0.058222\n",
      "step: 3900  loss: =  0.055019\n",
      "step: 4000  loss: =  0.051247\n",
      "step: 4100  loss: =  0.048523\n",
      "step: 4200  loss: =  0.048850\n",
      "step: 4300  loss: =  0.045476\n",
      "step: 4400  loss: =  0.051738\n",
      "step: 4500  loss: =  0.046636\n",
      "step: 4600  loss: =  0.046464\n",
      "step: 4700  loss: =  0.044510\n",
      "step: 4800  loss: =  0.041080\n",
      "step: 4900  loss: =  0.049777\n",
      "step: 5000  loss: =  0.037325\n",
      "step: 5100  loss: =  0.040071\n",
      "step: 5200  loss: =  0.043978\n",
      "step: 5300  loss: =  0.039363\n",
      "step: 5400  loss: =  0.053792\n",
      "completed data: 1400  kernels: = 16.000000\n",
      "11851281.0\n",
      "11410558.0\n",
      "step: 0  loss: = 114000.440000\n",
      "step: 100  loss: = 141.948359\n",
      "step: 200  loss: =  7.279263\n",
      "step: 300  loss: =  0.768252\n",
      "step: 400  loss: =  0.341194\n",
      "step: 500  loss: =  0.208177\n",
      "step: 600  loss: =  0.188494\n",
      "step: 700  loss: =  0.164087\n",
      "step: 800  loss: =  0.226245\n",
      "step: 900  loss: =  0.113893\n",
      "step: 1000  loss: =  0.084665\n",
      "step: 1100  loss: =  0.068906\n",
      "step: 1200  loss: =  0.066713\n",
      "step: 1300  loss: =  0.054546\n",
      "step: 1400  loss: =  0.050029\n",
      "step: 1500  loss: =  0.040702\n",
      "step: 1600  loss: =  0.039151\n",
      "step: 1700  loss: =  0.030878\n",
      "step: 1800  loss: =  0.041953\n",
      "step: 1900  loss: =  0.029380\n",
      "step: 2000  loss: =  0.018020\n",
      "step: 2100  loss: =  0.017849\n",
      "step: 2200  loss: =  0.019196\n",
      "step: 2300  loss: =  0.026698\n",
      "step: 2400  loss: =  0.019282\n",
      "step: 2500  loss: =  0.011132\n",
      "step: 2600  loss: =  0.008262\n",
      "step: 2700  loss: =  0.010998\n",
      "step: 2800  loss: =  0.012588\n",
      "step: 2900  loss: =  0.019905\n",
      "step: 3000  loss: =  0.006463\n",
      "step: 3100  loss: =  0.005965\n",
      "completed data: 1400  kernels: = 28.000000\n",
      "612649.06\n",
      "690435.5\n",
      "step: 0  loss: = 5884.598750\n",
      "step: 100  loss: = 22.214109\n",
      "step: 200  loss: =  1.980905\n",
      "step: 300  loss: =  0.565003\n",
      "step: 400  loss: =  0.405952\n",
      "step: 500  loss: =  0.325342\n",
      "step: 600  loss: =  0.323514\n",
      "step: 700  loss: =  0.312449\n",
      "step: 800  loss: =  0.288662\n",
      "step: 900  loss: =  0.297031\n",
      "step: 1000  loss: =  0.247027\n",
      "step: 1100  loss: =  0.249406\n",
      "step: 1200  loss: =  0.256430\n",
      "step: 1300  loss: =  0.237592\n",
      "step: 1400  loss: =  0.221679\n",
      "completed data: 2800  kernels: =  3.000000\n",
      "805757.75\n",
      "807329.0\n",
      "step: 0  loss: = 7576.601250\n",
      "step: 100  loss: = 16.198821\n",
      "step: 200  loss: =  1.662481\n",
      "step: 300  loss: =  0.631807\n",
      "step: 400  loss: =  0.513691\n",
      "step: 500  loss: =  0.470558\n",
      "step: 600  loss: =  0.465411\n",
      "step: 700  loss: =  0.456366\n",
      "step: 800  loss: =  0.453089\n",
      "step: 900  loss: =  0.434016\n",
      "step: 1000  loss: =  0.421726\n",
      "step: 1100  loss: =  0.426341\n",
      "step: 1200  loss: =  0.417103\n",
      "step: 1300  loss: =  0.417192\n",
      "step: 1400  loss: =  0.418131\n",
      "step: 1500  loss: =  0.397647\n",
      "step: 1600  loss: =  0.400631\n",
      "completed data: 2800  kernels: =  8.000000\n",
      "3028394.0\n",
      "3005786.2\n",
      "step: 0  loss: = 28249.967500\n",
      "step: 100  loss: = 25.198564\n",
      "step: 200  loss: =  1.454044\n",
      "step: 300  loss: =  0.520619\n",
      "step: 400  loss: =  0.387497\n",
      "step: 500  loss: =  0.322513\n",
      "step: 600  loss: =  0.310755\n",
      "step: 700  loss: =  0.306441\n",
      "step: 800  loss: =  0.317470\n",
      "step: 900  loss: =  0.286566\n",
      "step: 1000  loss: =  0.267103\n",
      "step: 1100  loss: =  0.272671\n",
      "step: 1200  loss: =  0.274389\n",
      "step: 1300  loss: =  0.262962\n",
      "step: 1400  loss: =  0.257347\n",
      "step: 1500  loss: =  0.234043\n",
      "step: 1600  loss: =  0.246978\n",
      "step: 1700  loss: =  0.247659\n",
      "step: 1800  loss: =  0.266453\n",
      "step: 1900  loss: =  0.236229\n",
      "step: 2000  loss: =  0.250198\n",
      "completed data: 2800  kernels: = 16.000000\n",
      "11115094.0\n",
      "10790242.0\n",
      "step: 0  loss: = 102548.120000\n",
      "step: 100  loss: = 18.035616\n",
      "step: 200  loss: =  0.803474\n",
      "step: 300  loss: =  0.668629\n",
      "step: 400  loss: =  0.589275\n",
      "step: 500  loss: =  0.311326\n",
      "step: 600  loss: =  0.301623\n",
      "step: 700  loss: =  0.277390\n",
      "step: 800  loss: =  0.288746\n",
      "step: 900  loss: =  0.299556\n",
      "step: 1000  loss: =  0.230246\n",
      "step: 1100  loss: =  0.212887\n",
      "step: 1200  loss: =  0.196791\n",
      "step: 1300  loss: =  0.243456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1400  loss: =  0.273454\n",
      "step: 1500  loss: =  0.180359\n",
      "step: 1600  loss: =  0.183697\n",
      "step: 1700  loss: =  0.182919\n",
      "step: 1800  loss: =  0.188246\n",
      "completed data: 2800  kernels: = 28.000000\n",
      "321910.56\n",
      "335155.6\n",
      "step: 0  loss: = 2917.435625\n",
      "step: 100  loss: =  5.376389\n",
      "step: 200  loss: =  0.684322\n",
      "step: 300  loss: =  0.382277\n",
      "step: 400  loss: =  0.293006\n",
      "step: 500  loss: =  0.267761\n",
      "step: 600  loss: =  0.255702\n",
      "step: 700  loss: =  0.247980\n",
      "step: 800  loss: =  0.238955\n",
      "step: 900  loss: =  0.225186\n",
      "step: 1000  loss: =  0.201530\n",
      "step: 1100  loss: =  0.195656\n",
      "step: 1200  loss: =  0.194622\n",
      "step: 1300  loss: =  0.171255\n",
      "step: 1400  loss: =  0.156922\n",
      "step: 1500  loss: =  0.135334\n",
      "step: 1600  loss: =  0.134623\n",
      "step: 1700  loss: =  0.126032\n",
      "step: 1800  loss: =  0.116924\n",
      "step: 1900  loss: =  0.108341\n",
      "step: 2000  loss: =  0.083066\n",
      "step: 2100  loss: =  0.080192\n",
      "step: 2200  loss: =  0.074890\n",
      "step: 2300  loss: =  0.068986\n",
      "step: 2400  loss: =  0.059871\n",
      "step: 2500  loss: =  0.043615\n",
      "step: 2600  loss: =  0.041292\n",
      "step: 2700  loss: =  0.041412\n",
      "step: 2800  loss: =  0.042121\n",
      "step: 2900  loss: =  0.040825\n",
      "step: 3000  loss: =  0.040544\n",
      "step: 3100  loss: =  0.039142\n",
      "step: 3200  loss: =  0.039430\n",
      "step: 3300  loss: =  0.040100\n",
      "step: 3400  loss: =  0.040278\n",
      "step: 3500  loss: =  0.040734\n",
      "step: 3600  loss: =  0.040670\n",
      "completed data: 5500  kernels: =  3.000000\n",
      "784358.9\n",
      "782504.5\n",
      "step: 0  loss: = 6949.588125\n",
      "step: 100  loss: =  4.081241\n",
      "step: 200  loss: =  0.755150\n",
      "step: 300  loss: =  0.524477\n",
      "step: 400  loss: =  0.467997\n",
      "step: 500  loss: =  0.448225\n",
      "step: 600  loss: =  0.440102\n",
      "step: 700  loss: =  0.457459\n",
      "step: 800  loss: =  0.439954\n",
      "step: 900  loss: =  0.427399\n",
      "step: 1000  loss: =  0.420641\n",
      "step: 1100  loss: =  0.420209\n",
      "step: 1200  loss: =  0.414532\n",
      "step: 1300  loss: =  0.411848\n",
      "step: 1400  loss: =  0.411477\n",
      "step: 1500  loss: =  0.417364\n",
      "step: 1600  loss: =  0.415752\n",
      "step: 1700  loss: =  0.398741\n",
      "step: 1800  loss: =  0.434844\n",
      "step: 1900  loss: =  0.439371\n",
      "step: 2000  loss: =  0.405131\n",
      "step: 2100  loss: =  0.399882\n",
      "step: 2200  loss: =  0.397150\n",
      "step: 2300  loss: =  0.445905\n",
      "step: 2400  loss: =  0.483487\n",
      "step: 2500  loss: =  0.387064\n",
      "step: 2600  loss: =  0.389474\n",
      "step: 2700  loss: =  0.386350\n",
      "step: 2800  loss: =  0.383971\n",
      "completed data: 5500  kernels: =  8.000000\n",
      "3809571.5\n",
      "3654214.2\n",
      "step: 0  loss: = 33586.312500\n",
      "step: 100  loss: =  3.490302\n",
      "step: 200  loss: =  0.726082\n",
      "step: 300  loss: =  0.624637\n",
      "step: 400  loss: =  0.473672\n",
      "step: 500  loss: =  0.387622\n",
      "step: 600  loss: =  0.365037\n",
      "step: 700  loss: =  0.353685\n",
      "step: 800  loss: =  0.387451\n",
      "step: 900  loss: =  0.358870\n",
      "step: 1000  loss: =  0.336674\n",
      "step: 1100  loss: =  0.322598\n",
      "step: 1200  loss: =  0.336914\n",
      "step: 1300  loss: =  0.352625\n",
      "step: 1400  loss: =  0.381705\n",
      "step: 1500  loss: =  0.320635\n",
      "step: 1600  loss: =  0.313797\n",
      "completed data: 5500  kernels: = 16.000000\n",
      "11746647.0\n",
      "11651643.0\n",
      "step: 0  loss: = 100588.050000\n",
      "step: 100  loss: =  2.185629\n",
      "step: 200  loss: =  0.626507\n",
      "step: 300  loss: =  0.492654\n",
      "step: 400  loss: =  0.427785\n",
      "step: 500  loss: =  0.366538\n",
      "step: 600  loss: =  0.347505\n",
      "step: 700  loss: =  0.338758\n",
      "step: 800  loss: =  0.356683\n",
      "step: 900  loss: =  0.380052\n",
      "step: 1000  loss: =  0.319768\n",
      "step: 1100  loss: =  0.297155\n",
      "step: 1200  loss: =  0.302310\n",
      "step: 1300  loss: =  0.340686\n",
      "step: 1400  loss: =  0.386974\n",
      "step: 1500  loss: =  0.303495\n",
      "step: 1600  loss: =  0.291759\n",
      "completed data: 5500  kernels: = 28.000000\n",
      "559813.6\n",
      "535933.0\n",
      "step: 0  loss: = 4754.595312\n",
      "step: 100  loss: =  1.502240\n",
      "step: 200  loss: =  0.425855\n",
      "step: 300  loss: =  0.326929\n",
      "step: 400  loss: =  0.258656\n",
      "step: 500  loss: =  0.215747\n",
      "step: 600  loss: =  0.214784\n",
      "step: 700  loss: =  0.205370\n",
      "step: 800  loss: =  0.193551\n",
      "step: 900  loss: =  0.167725\n",
      "step: 1000  loss: =  0.120174\n",
      "step: 1100  loss: =  0.111850\n",
      "step: 1200  loss: =  0.100463\n",
      "step: 1300  loss: =  0.077492\n",
      "step: 1400  loss: =  0.071915\n",
      "step: 1500  loss: =  0.052816\n",
      "step: 1600  loss: =  0.050976\n",
      "step: 1700  loss: =  0.047730\n",
      "step: 1800  loss: =  0.044528\n",
      "step: 1900  loss: =  0.043748\n",
      "step: 2000  loss: =  0.041697\n",
      "step: 2100  loss: =  0.042245\n",
      "step: 2200  loss: =  0.041879\n",
      "step: 2300  loss: =  0.042672\n",
      "step: 2400  loss: =  0.042021\n",
      "step: 2500  loss: =  0.041075\n",
      "step: 2600  loss: =  0.041189\n",
      "step: 2700  loss: =  0.040957\n",
      "step: 2800  loss: =  0.041063\n",
      "step: 2900  loss: =  0.043479\n",
      "step: 3000  loss: =  0.040399\n",
      "step: 3100  loss: =  0.040933\n",
      "completed data: 11000  kernels: =  3.000000\n",
      "856799.56\n",
      "861293.5\n",
      "step: 0  loss: = 6736.615625\n",
      "step: 100  loss: =  1.220776\n",
      "step: 200  loss: =  0.517380\n",
      "step: 300  loss: =  0.466436\n",
      "step: 400  loss: =  0.450295\n",
      "step: 500  loss: =  0.446051\n",
      "step: 600  loss: =  0.425353\n",
      "step: 700  loss: =  0.433396\n",
      "step: 800  loss: =  0.422163\n",
      "step: 900  loss: =  0.431775\n",
      "step: 1000  loss: =  0.435815\n",
      "step: 1100  loss: =  0.410234\n",
      "step: 1200  loss: =  0.408119\n",
      "completed data: 11000  kernels: =  8.000000\n",
      "2898800.5\n",
      "2795058.2\n",
      "step: 0  loss: = 21765.347500\n",
      "step: 100  loss: =  1.117729\n",
      "step: 200  loss: =  0.565669\n",
      "step: 300  loss: =  0.431663\n",
      "step: 400  loss: =  0.435977\n",
      "step: 500  loss: =  0.410869\n",
      "step: 600  loss: =  0.400827\n",
      "step: 700  loss: =  0.402336\n",
      "step: 800  loss: =  0.393261\n",
      "step: 900  loss: =  0.405489\n",
      "step: 1000  loss: =  0.371414\n",
      "step: 1100  loss: =  0.381298\n",
      "step: 1200  loss: =  0.373778\n",
      "step: 1300  loss: =  0.379598\n",
      "step: 1400  loss: =  0.402875\n",
      "step: 1500  loss: =  0.368411\n",
      "step: 1600  loss: =  0.385312\n",
      "completed data: 11000  kernels: = 16.000000\n",
      "10609088.0\n",
      "10580635.0\n",
      "step: 0  loss: = 76328.040000\n",
      "step: 100  loss: =  0.981663\n",
      "step: 200  loss: =  0.673213\n",
      "step: 300  loss: =  0.531261\n",
      "step: 400  loss: =  0.450765\n",
      "step: 500  loss: =  0.410838\n",
      "step: 600  loss: =  0.354767\n",
      "step: 700  loss: =  0.378841\n",
      "step: 800  loss: =  0.392335\n",
      "step: 900  loss: =  0.390897\n",
      "step: 1000  loss: =  0.343290\n",
      "step: 1100  loss: =  0.347149\n",
      "step: 1200  loss: =  0.377553\n",
      "step: 1300  loss: =  0.459718\n",
      "step: 1400  loss: =  0.420525\n",
      "step: 1500  loss: =  0.352806\n",
      "completed data: 11000  kernels: = 28.000000\n"
     ]
    }
   ],
   "source": [
    "kernels = [3, 8, 16, 28]\n",
    "lambdas = [0.0, 0.0, 0.0, 0.0] \n",
    "\n",
    "datas = [60, 340, 700, 1400, 2800, 5500, 11000, 22000, 98000]\n",
    "training_epochs = [6000, 4000, 4000, 3000, 3000, 2000, 2000, 1000, 500]\n",
    "test_sizes = [2080, 2080, 2080, 2080, 2080, 2080, 2080, 2080, 2080, 2000]\n",
    "\n",
    "learn_rate = 1e-3\n",
    "learn_rate_late = 1e-4\n",
    "\n",
    "\n",
    "for i_data in range(7):\n",
    "    \n",
    "    for i_kernel in range(4): \n",
    "        \n",
    "        if i_kernel > 1: \n",
    "            del stimulus_\n",
    "            del bipolar_cell_layer\n",
    "            del gc_activation\n",
    "            del gc_output\n",
    "            del bipolar_bias\n",
    "            del bipkernels1\n",
    "        \n",
    "        no_train=datas[i_data]\n",
    "        epochs = training_epochs[i_data]\n",
    "        no_kernels = kernels[i_kernel]\n",
    "        lambda1 = lambdas[i_kernel]\n",
    "        bipkernels1 = bipkernels[:, :, :, 0:no_kernels]\n",
    "        bip_gc_syn_init = bip_gc_syn_init1[:, :, :, 0:no_kernels]\n",
    "        bip_am_syn_mask1 = bip_am_syn_mask[ :, :, 0:no_kernels, :, :]\n",
    "                \n",
    "        no_test=test_sizes[i_data] \n",
    "        no_bipolars = 10\n",
    "        no_amacrines = 5\n",
    "\n",
    "        wheretosave = '/home/ubuntu/Notebooks/Circuit2_Trained_Network_data' + str(no_train) + '_kernel' + str(no_kernels) \\\n",
    "        + '_sd' + str(sd) + '_nol1reg.mat'\n",
    "\n",
    "        ## initialize all variables\n",
    "        bip_bias_init_all = -1.0*np.ones([28])\n",
    "        bip_bias_init_all[0]=-2.0\n",
    "        bip_bias_init_all[1]=-3.0\n",
    "        bip_bias_init_all[3]=-15.0\n",
    "        bip_bias_init_all[8]=-25.0\n",
    "        bip_bias_init_all[9]=-10.0\n",
    "        \n",
    "        bip_bias_init_all[4]=-2.0\n",
    "        bip_bias_init_all[5]=-3.0\n",
    "        bip_bias_init_all[7]=-15.0\n",
    "        bip_bias_init_all[12]=-25.0\n",
    "        bip_bias_init_all[13]=-10.0\n",
    "\n",
    "\n",
    "        bip_bias_init = bip_bias_init_all[0:no_kernels]\n",
    "        bip_bias_init = bip_bias_init.astype(float32)\n",
    "        bipolar_bias = bias_var([no_kernels], bip_bias_init)\n",
    "        \n",
    "        am_bias_init = -5.0 \n",
    "        am_bias = bias_var([1], am_bias_init)\n",
    "        \n",
    "        gc_bias = bias_var([1], gc_bias_init)\n",
    "        \n",
    "        bip_gc_syn_init=tf.random.normal([1, no_bipolars, no_bipolars, no_kernels], mean = 0.0, stddev = sqrt(2.0/(no_kernels*100)), dtype=tf.dtypes.float32, seed=sd)\n",
    "        bip_gc_syn = synapse_var([1, no_bipolars, no_bipolars, no_kernels], bip_gc_syn_init)\n",
    "        \n",
    "        bip_am_syn_inds = np.zeros([no_kernels*100, 6])\n",
    "        for i in range(10):\n",
    "            for j in range(10):\n",
    "                for k in range(no_kernels):\n",
    "                    bip_am_syn_inds[no_kernels*10*(i)+no_kernels*(j)+k]=[0, i, j, k, floor(i/2), floor(j/2)]\n",
    "        bip_am_syn_inds = bip_am_syn_inds.astype(int64)\n",
    "        bip_am_syn_init11 = abs(np.random.normal(0.0, (sqrt(2.0/no_kernels)), size=[no_kernels*100]))\n",
    "        bip_am_syn_init111=bip_am_syn_init11.astype(float32)  \n",
    "        bip_am_syn_val = synapse_var([no_kernels*no_bipolars*no_bipolars], bip_am_syn_init111)\n",
    "        bip_am_syn1 = tf.sparse.SparseTensor(indices=bip_am_syn_inds, values=bip_am_syn_val, dense_shape=[1, no_bipolars, no_bipolars, no_kernels, no_amacrines, no_amacrines])\n",
    "        bip_am_syn = tf.sparse.to_dense(tf.sparse.reorder(bip_am_syn1))        \n",
    "        \n",
    "        am_gc_syn = synapse_var([1, no_amacrines, no_amacrines], am_gc_syn_init1)\n",
    "\n",
    "        stimulus_ = tf.placeholder(\"float32\", name=\"stim_placeholder\")\n",
    "\n",
    "        bipolar_cell_layer = tf.nn.relu(tf.nn.bias_add(bip_conv2d(stimulus_, bipkernels1), bipolar_bias))\n",
    "\n",
    "        biplyr = tf.reshape(bipolar_cell_layer, [-1, no_bipolars*no_bipolars*no_kernels, 1])\n",
    "\n",
    "        tilebip_am_syn=tf.tile(tf.transpose(tf.reshape(tf.abs(bip_am_syn), [1, no_bipolars*no_bipolars*no_kernels, no_amacrines*no_amacrines]), [0, 2, 1]), [1, 1, 1])\n",
    "\n",
    "\n",
    "        amacrine_activation = 3.0*tf.reshape(tf.linalg.matmul(tilebip_am_syn, biplyr), [-1,no_amacrines, no_amacrines])\n",
    "     \n",
    "        amacrine_cell_layer = tf.nn.relu(tf.add(amacrine_activation, am_bias))\n",
    "\n",
    "        gc_activation = tf.multiply(tf.abs(bip_gc_syn), bipolar_cell_layer)\n",
    "\n",
    "        gc_activation_inhib = tf.multiply(tf.abs(am_gc_syn), amacrine_cell_layer)\n",
    "\n",
    "        gc_output = tf.add_n([tf.reduce_sum(gc_activation, [1, 2, 3]), -1.0*tf.reduce_sum(gc_activation_inhib, [1, 2])])\n",
    "\n",
    "        ## training procedure\n",
    "        y_ = tf.placeholder(\"float32\", name=\"output_spikes\")\n",
    "        \n",
    "        batchsize=20\n",
    "\n",
    "        loss = (tf.nn.l2_loss((tf.squeeze(gc_output) - tf.squeeze(y_)), name='loss'))\n",
    "\n",
    "        regularizer=tf.add_n([tf.reduce_sum(tf.abs(bip_am_syn)), tf.reduce_sum(tf.abs(bip_gc_syn)), \\\n",
    "                              0.0*tf.reduce_sum(tf.abs(am_gc_syn))])\n",
    "\n",
    "        objective=tf.add(loss, lambda1*regularizer)\n",
    "        \n",
    "        bip_am_ygrad = tf.gradients(loss, [bip_am_syn])\n",
    "        bip_am_reggrad = tf.gradients(regularizer, [bip_am_syn])\n",
    "        \n",
    "        am_gc_ygrad = tf.gradients(loss, [am_gc_syn])\n",
    "        am_gc_reggrad = tf.gradients(regularizer, [am_gc_syn])\n",
    "        \n",
    "        bip_gc_ygrad = tf.gradients(loss, [bip_gc_syn])\n",
    "        bip_gc_reggrad = tf.gradients(regularizer, [bip_gc_syn])\n",
    "        \n",
    "\n",
    "\n",
    "        algorithm_choice=2\n",
    "        lr_min = 1e-4\n",
    "        lr_max = 1e-5\n",
    "        max_step =500\n",
    "        lr_ = tf.placeholder(\"float32\", name=\"learn_rate\")\n",
    "        \n",
    "        if algorithm_choice==1:\n",
    "            train_step = tf.train.GradientDescentOptimizer(lr_).minimize(objective)\n",
    "        elif algorithm_choice==2:\n",
    "            my_epsilon=1e-8\n",
    "            train_step = tf.train.AdamOptimizer(learning_rate=lr_, epsilon=my_epsilon).minimize(objective)\n",
    "        elif algorithm_choice==3:\n",
    "            momentum_par=0.9\n",
    "            train_step = tf.train.MomentumOptimizer(lr_, momentum_par).minimize(objective)\n",
    "        elif algorithm_choice==4:\n",
    "            train_step = tf.train.AdagradOptimizer(lr_).minimize(objective)\n",
    "        elif algorithm_choice==5:\n",
    "            train_step = tf.train.RMSPropOptimizer(lr_).minimize(objective)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())    \n",
    "\n",
    "        bip_gc_syn_hist=tf.reshape(bip_gc_syn.eval(session=sess), [1, no_bipolars, no_bipolars, no_kernels]) \n",
    "        bip_am_syn_hist=tf.reshape(bip_am_syn.eval(session=sess), [1, no_bipolars, no_bipolars, no_kernels, no_amacrines, no_amacrines]) \n",
    "        am_gc_syn_hist=tf.reshape(am_gc_syn.eval(session=sess), [1, no_amacrines, no_amacrines]) \n",
    "        train_loss_hist = ones([1])\n",
    "        test_loss_hist = ones([1])\n",
    "        \n",
    "        bip_am_ygrad_hist=np.reshape(sess.run(bip_am_ygrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_bipolars, no_bipolars, no_kernels, no_amacrines, no_amacrines]) \n",
    "        bip_am_reggrad_hist=np.reshape(sess.run(bip_am_reggrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_bipolars, no_bipolars, no_kernels, no_amacrines, no_amacrines]) \n",
    "        am_gc_ygrad_hist=np.reshape(sess.run(am_gc_ygrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_amacrines, no_amacrines])\n",
    "        am_gc_reggrad_hist=np.reshape(sess.run(am_gc_reggrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_amacrines, no_amacrines])\n",
    "        bip_gc_ygrad_hist=np.reshape(sess.run(bip_gc_ygrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_bipolars, no_bipolars, no_kernels]) \n",
    "        bip_gc_reggrad_hist=np.reshape(sess.run(bip_gc_reggrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_bipolars, no_bipolars, no_kernels]) \n",
    "\n",
    "        train_fd={stimulus_: x_train[0:50, :, :, :]}\n",
    "        test_fd={stimulus_: x_test[0:50, :, :, :]}\n",
    "        train_output_hist=reshape(gc_output.eval(session=sess, feed_dict=train_fd), [1, 50])\n",
    "        test_output_hist=reshape(gc_output.eval(session=sess, feed_dict=test_fd), [1, 50])\n",
    "        \n",
    "        \n",
    "        check=1.0\n",
    "        step=0\n",
    "        end_flag=0\n",
    "\n",
    "        fd = {stimulus_:x_train[0:100, :, :, :], y_:y_train[0, 0:100]}\n",
    "        train_loss_val = sess.run(loss, feed_dict = fd)\n",
    "        print(train_loss_val)\n",
    "\n",
    "        fd = {stimulus_:x_test[0:100, :, :, :], y_:y_test[0, 0:100]}\n",
    "        test_loss_val = sess.run(loss, feed_dict = fd)\n",
    "        print(test_loss_val)\n",
    "\n",
    "\n",
    "\n",
    "        train_loss_hist=train_loss_val*train_loss_hist\n",
    "        test_loss_hist=test_loss_val*test_loss_hist\n",
    "\n",
    "        \n",
    "        endflag=0\n",
    "        step=0\n",
    "        while endflag == 0:\n",
    "            # learning rate schedule\n",
    "            learn_rate_sch = lr_min + 0.5*(lr_max - lr_min)*(1.0+np.cos(np.pi*(step%max_step/max_step))) \n",
    "            if step>=10*max_step:\n",
    "                learn_rate_sch = lr_min\n",
    "\n",
    "            inds = np.reshape(np.random.permutation(range(no_train)), [-1, batchsize])\n",
    "            for n in range(len(inds)): \n",
    "                fdd = {stimulus_: x_train[inds[n, :], :, :, :], y_: y_train[0, inds[n, :]], lr_: learn_rate_sch} \n",
    "                                                                 \n",
    "                sess.run(train_step, feed_dict=fdd)\n",
    "\n",
    "                        \n",
    "            if (step % 100 ==0):\n",
    "\n",
    "                train_loss_val = sess.run(loss, feed_dict= {stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]})/100.0\n",
    "                test_loss_val = sess.run(loss, feed_dict= {stimulus_: x_test[0:100, :, :, :], y_: y_test[0, 0:100]})/100.0\n",
    "                print(\"step: %d  loss: = %9f\" % (step, train_loss_val))\n",
    "\n",
    "                bip_gc_syn_hist=tf.concat( [bip_gc_syn_hist, tf.reshape(bip_gc_syn.eval(session=sess), [1, no_bipolars, no_bipolars, no_kernels])], 0,  name='bip_gc_syn_concat')\n",
    "                bip_am_syn_hist=tf.concat( [bip_am_syn_hist, tf.reshape(bip_am_syn.eval(session=sess), [1, no_bipolars, no_bipolars, no_kernels, no_amacrines, no_amacrines])], 0,  name='bip_am_syn_concat')\n",
    "                am_gc_syn_hist=tf.concat( [am_gc_syn_hist, tf.reshape(am_gc_syn.eval(session=sess), [1, no_amacrines, no_amacrines])], 0,  name='am_gc_syn_concat')\n",
    "\n",
    "                bip_am_ygrad_hist=tf.concat( [bip_am_ygrad_hist, np.reshape(sess.run(bip_am_ygrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_bipolars, no_bipolars, no_kernels, no_amacrines, no_amacrines])], 0)\n",
    "                bip_am_reggrad_hist=tf.concat( [bip_am_reggrad_hist, np.reshape(sess.run(bip_am_reggrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_bipolars, no_bipolars, no_kernels, no_amacrines, no_amacrines])], 0)\n",
    "                am_gc_ygrad_hist=tf.concat( [am_gc_ygrad_hist, np.reshape(sess.run(am_gc_ygrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_amacrines, no_amacrines])], 0)\n",
    "                am_gc_reggrad_hist=tf.concat( [am_gc_reggrad_hist, np.reshape(sess.run(am_gc_reggrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_amacrines, no_amacrines])], 0)\n",
    "                bip_gc_ygrad_hist=tf.concat( [bip_gc_ygrad_hist, np.reshape(sess.run(bip_gc_ygrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_bipolars, no_bipolars, no_kernels])], 0)\n",
    "                bip_gc_reggrad_hist=tf.concat( [bip_gc_reggrad_hist, np.reshape(sess.run(bip_gc_reggrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_bipolars, no_bipolars, no_kernels])], 0)\n",
    "\n",
    "                train_loss_hist=np.concatenate([train_loss_hist, np.array([train_loss_val])], axis=0)\n",
    "                test_loss_hist=np.concatenate([test_loss_hist, np.array([test_loss_val])], axis=0)\n",
    "                \n",
    "                train_fd={stimulus_: x_train[0:50, :, :, :]}\n",
    "                test_fd={stimulus_: x_test[0:50, :, :, :]}\n",
    "                train_output=reshape(gc_output.eval(session=sess, feed_dict=train_fd), [1, 50])\n",
    "                test_output=reshape(gc_output.eval(session=sess, feed_dict=test_fd), [1, 50])\n",
    "                \n",
    "                train_output_hist=np.concatenate([train_output_hist, train_output], axis=0)\n",
    "                test_output_hist=np.concatenate([test_output_hist, test_output], axis=0)\n",
    "                \n",
    "                #stopping condition\n",
    "                if (step/100)>=5:\n",
    "                    b=np.diff(train_loss_hist[int(step/100-5):int(step/100)])\n",
    "                    a=abs(b)<1.0\n",
    "                    c=b>0.0\n",
    "                    if sum(c)>=3:\n",
    "                        endflag=1\n",
    "            step = step + 1\n",
    "\n",
    "        db = {}\n",
    "\n",
    "        db['bipolar_bias'] = bipolar_bias.eval(session=sess)\n",
    "        db['bip_gc_syn_hist'] = bip_gc_syn_hist.eval(session=sess)\n",
    "        db['bip_am_syn_hist'] = bip_am_syn_hist.eval(session=sess)\n",
    "        db['am_gc_syn_hist'] = am_gc_syn_hist.eval(session=sess)\n",
    "        db['gc_bias'] = gc_bias.eval(session=sess)\n",
    "        \n",
    "        db['bip_am_ygrad_hist'] = bip_am_ygrad_hist.eval(session=sess)\n",
    "        db['bip_am_reggrad_hist'] = bip_am_reggrad_hist.eval(session=sess)\n",
    "        db['am_gc_ygrad_hist'] = am_gc_ygrad_hist.eval(session=sess)\n",
    "        db['am_gc_reggrad_hist'] = am_gc_reggrad_hist.eval(session=sess)\n",
    "        db['bip_gc_ygrad_hist'] = bip_gc_ygrad_hist.eval(session=sess)\n",
    "        db['bip_gc_reggrad_hist'] = bip_gc_reggrad_hist.eval(session=sess)\n",
    "\n",
    "\n",
    "\n",
    "        db['no_train']=no_train\n",
    "        db['no_test']=no_test\n",
    "\n",
    "        db['no_kernels'] = no_kernels\n",
    "        db['no_bipolars']=no_bipolars\n",
    "\n",
    "        db['bipkernels'] = bipkernels\n",
    "        db['randomseed'] = sd\n",
    "\n",
    "        db['train_output_hist'] = train_output_hist\n",
    "        db['test_output_hist'] = test_output_hist\n",
    "\n",
    "        db['algorithm_choice'] = algorithm_choice\n",
    "        db['learn_rate'] = learn_rate\n",
    "        db['lambda'] = lambda1\n",
    "\n",
    "        db['train_loss_hist'] = train_loss_hist\n",
    "        db['test_loss_hist'] = test_loss_hist                          \n",
    "\n",
    "        struct_proj = np.zeros([len(train_loss_hist), 1])\n",
    "\n",
    "        syn_hist = bip_gc_syn_hist.eval(session=sess)\n",
    "        basyn_hist = abs(bip_am_syn_hist.eval(session=sess))\n",
    "        agsyn_hist = abs(am_gc_syn_hist.eval(session=sess))\n",
    "        \n",
    "        truesyn = np.zeros([10, 10, no_kernels])\n",
    "        \n",
    "        truebasyn = np.zeros([no_bipolars, no_bipolars, no_kernels, no_amacrines, no_amacrines])\n",
    "        truebasyn[:, :, 0:3, :, :]=bip_am_syn_init\n",
    "        \n",
    "        trueagsyn=am_gc_syn_init\n",
    "        \n",
    "        norm_factor = (np.sum(np.square(truebasyn)) + np.sum(np.square(trueagsyn)))\n",
    "        for i in range(len(train_loss_hist)):\n",
    "            norm_factor = (np.sum(np.square(basyn_hist[i, :, :, :, :, :])) + np.sum(np.square(agsyn_hist[i, :, :])))\n",
    "            struct_proj[i] = (np.sum(np.multiply((basyn_hist[i, :, :, :, :, :]), truebasyn))+np.sum(np.multiply((agsyn_hist[i, :, :]), trueagsyn)))/norm_factor\n",
    "\n",
    "        db['struct_proj'] = struct_proj\n",
    "\n",
    "        sio.savemat(wheretosave, db)\n",
    "        \n",
    "        print(\"completed data: %d  kernels: = %9f\" % (no_train, no_kernels))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -2.391449     1.3332825   -8.086426    -7.3722534   -9.315186\n",
      "  -3.8841248   -1.7415466    1.4291687   -4.7464905   -5.434143\n",
      "  -2.290863    -3.4580078   -5.93367     -0.36605835  -2.3545837\n",
      "  -5.5501404   -1.0029907   -3.6003723   -4.366028    -2.084198\n",
      "   4.4366455   -7.401825    -1.4229431    0.9442749   -3.6280518\n",
      "   1.058136    -1.1313782  -13.549194    -1.5032043   -6.958954\n",
      "  -1.7838135   -7.2634583   -2.2261353  -10.235657     2.074524\n",
      "  -9.631805     8.288971    -1.2003174  -14.270966    -4.9716797\n",
      "   2.021759    -0.07931519  -1.0352173   -5.935913     3.5401306\n",
      " -11.168549    -6.64563     -0.02102661   4.9253235    3.1720123\n",
      "   0.07427979   1.5976868   -2.5878296   -7.083191    -7.7838745\n",
      "  -0.8669739   -4.14447     -5.8881226   -5.6602173   -7.7986298\n",
      "  -2.825119    -3.526001    -2.4040985    0.3944702   -7.837799\n",
      "  -3.1681824   -0.9395447    3.642517    -4.968109    -4.661957\n",
      "  -7.738922    -5.2638855   -6.007721    -1.8297577    0.7111206\n",
      "  -2.3671112   -0.4719696   -2.0507202   -9.050781    -5.8664246\n",
      "   0.92285156  -3.3198547    1.9862366   -3.1588135    1.9380493\n",
      "  -1.7788696   -5.4555054   -6.644684     0.6535034   -4.5297546\n",
      "  -3.9063416   -1.6047363   -5.8604126   -4.857086    -1.4229431\n",
      "  -2.4763489   -1.0523682   -4.236328     4.626251    -7.255829  ]\n",
      "[[[126.43265   72.242355 240.99197  127.51605   82.0588  ]\n",
      "  [210.84851  160.76022  183.76895  143.03703  118.26631 ]\n",
      "  [297.92523  134.84518  115.840614 110.855484 179.37158 ]\n",
      "  [128.82956  169.30487  207.42978  260.9159   117.912155]\n",
      "  [252.73181   43.48075  180.21648  108.67955  213.4153  ]]\n",
      "\n",
      " [[179.37946  127.73489  106.01742  125.2211    88.66895 ]\n",
      "  [ 84.70929  175.41222  173.24956   66.760605  83.29733 ]\n",
      "  [145.42723  137.0274   138.39517  143.97906  101.86821 ]\n",
      "  [247.80167  202.7181   151.76572  137.61002  124.22986 ]\n",
      "  [ 84.98244  136.81107   93.46443  149.49713  360.48022 ]]\n",
      "\n",
      " [[ 81.7418   202.27701  105.05598  115.34609  194.00983 ]\n",
      "  [222.84653  220.66315  169.10689  220.36305  333.38608 ]\n",
      "  [216.6005   226.63506   66.92818  126.85272   34.929226]\n",
      "  [211.55832  109.90129  104.05333  111.02924  206.40646 ]\n",
      "  [144.70839  178.08644  184.93277   76.233444 101.446594]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[123.9415   156.0837   209.90956  117.040375 160.33221 ]\n",
      "  [151.48083   35.72081  172.47694   73.41402   78.210045]\n",
      "  [186.43704  128.11052  253.42255   99.507324 268.9193  ]\n",
      "  [104.92412  193.37889  114.4433   136.45862  194.86667 ]\n",
      "  [162.84863  146.89214  101.77847  231.91216  291.76767 ]]\n",
      "\n",
      " [[179.55211  141.06226   87.75461   92.37622  262.33707 ]\n",
      "  [ 59.684265 187.92519  161.61441   83.69883  128.64536 ]\n",
      "  [311.8802   159.53987  110.33551  252.95917  135.54248 ]\n",
      "  [257.768     64.346756 186.5951   171.89923  145.04291 ]\n",
      "  [120.923645  77.7856   309.74207  102.19548  139.09491 ]]\n",
      "\n",
      " [[103.00197  118.953636 308.7841   125.85477  192.51024 ]\n",
      "  [211.81439  117.04782  138.3412   140.75803  363.3478  ]\n",
      "  [190.48035  129.66348  119.59505   90.54145  138.93181 ]\n",
      "  [122.85133  196.85147  176.46147  148.83066   80.30628 ]\n",
      "  [175.55441  108.44884   98.78412  114.74898   91.6756  ]]]\n"
     ]
    }
   ],
   "source": [
    "fd=feed_dict= {stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}\n",
    "train_output=gc_output.eval(session=sess, feed_dict=fd)\n",
    "print(train_output)\n",
    "\n",
    "fd=feed_dict= {stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}\n",
    "train_output=amacrine_cell_layer.eval(session=sess, feed_dict=fd)\n",
    "print(train_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p27)",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
