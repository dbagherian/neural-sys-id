{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from __future__ import division\n",
    "import random\n",
    "import scipy\n",
    "import h5py\n",
    "import hdf5storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random seed will dictate the random initialization\n",
    "sd=30000\n",
    "np.random.seed(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98000, 100, 100, 1)\n",
      "(2000, 100, 100, 1)\n",
      "(1, 10, 10, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2800.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxker=28\n",
    "traindatapath='/home/ubuntu/Notebooks/Circuit2_Training_Data.h5'\n",
    "data = hdf5storage.loadmat(traindatapath)\n",
    "\n",
    "\n",
    "x_train = data['x_train']\n",
    "x_test = data['x_test']\n",
    "y_train = data['y_train']\n",
    "y_test = reshape(data['y_test'], [1, 2000])\n",
    "\n",
    "gc_bias_init = data['gc_bias']\n",
    "bipkernels = data['bipkernels']\n",
    "bip_gc_syn_init = data['bip_gc_syn']\n",
    "bip_am_syn_init = data['bip_am_syn']\n",
    "am_gc_syn_init = data['am_gc_syn']\n",
    "\n",
    "#sparsity params for weight matrix initializations\n",
    "init_sparsity = 0.0 \n",
    "init_sparsity_bg = 0.01\n",
    "\n",
    "\n",
    "bip_gc_syn_mask1 = np.random.rand(maxker*100, 1)\n",
    "bip_gc_syn_mask1 = reshape(bip_gc_syn_mask1, [1, 10, 10, maxker])\n",
    "bip_gc_syn_mask1 = bip_gc_syn_mask1 >(1.0 - init_sparsity_bg)\n",
    "\n",
    "\n",
    "bip_gc_syn_init_full = np.zeros([1, 10, 10, maxker])\n",
    "bip_gc_syn_mask_true = reshape(bip_gc_syn_init, [1, 10, 10, 3])>0.0\n",
    "bip_gc_syn_init_full[:, :, :, 0:3] = bip_gc_syn_mask_true\n",
    "\n",
    "bip_gc_syn_mask = np.maximum(bip_gc_syn_mask1, bip_gc_syn_init_full)\n",
    "\n",
    "bip_gc_syn_init11 = tf.random_uniform([1, 10, 10, maxker], minval=0.1, maxval=0.2, dtype=tf.float32)\n",
    "bip_gc_syn_init1=bip_gc_syn_init11\n",
    "\n",
    "bip_am_syn_mask = np.zeros([10, 10, maxker, 5, 5])\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        for k in range(maxker):\n",
    "            bip_am_syn_mask[i, j, k, int(floor(i/2)), int(floor(j/2))] = 1.0\n",
    "bip_am_syn_mask = bip_am_syn_mask.astype(float32)\n",
    "\n",
    "\n",
    "bip_am_syn_inds = np.zeros([maxker*100, 6])\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        for k in range(maxker):\n",
    "            bip_am_syn_inds[maxker*10*(i)+28*(j)+k]=[0, i, j, k, floor(i/2), floor(j/2)]\n",
    "bip_am_syn_inds = bip_am_syn_inds.astype(int64)\n",
    "\n",
    "\n",
    "bip_am_syn_init11 = np.random.normal(0.0, (sqrt(2.0)/112.0), size=[maxker*100])\n",
    "bip_am_syn_init11=bip_am_syn_init11.astype(float32)        \n",
    "\n",
    "am_gc_syn_init1 = tf.random_uniform([1, 5, 5], minval=0.1, maxval=0.2, dtype=tf.float32)\n",
    " \n",
    "\n",
    "print(shape(x_train))\n",
    "print(shape(x_test))\n",
    "print(shape(bip_gc_syn_init))\n",
    "\n",
    "sum(bip_am_syn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 98000)\n",
      "(1, 2000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f20086b6b10>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(shape(y_train))\n",
    "print(shape(y_test))\n",
    "plt.figure()\n",
    "plt.plot(squeeze(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_var(shape, initial_val):\n",
    "    initial = tf.constant(initial_val, shape=shape)\n",
    "    return tf.constant(initial_val) #initial\n",
    "\n",
    "def bip_conv2d(x, W):\n",
    "    padsize=10 \n",
    "    paddedx=tf.pad(x, [[0, 0], [padsize, padsize], [padsize, padsize], [0, 0]], 'CONSTANT')\n",
    "    outconv=tf.nn.conv2d(paddedx, W, strides=[1, 10, 10, 1], padding='SAME') #250 for movingdot and noise\n",
    "    return outconv[:, 1:11, 1:11, :]\n",
    "\n",
    "def synapse_var(shape, initial_val):\n",
    "#      initial=tf.constant(initial_val, shape=shape)\n",
    "#     initial = tf.random_uniform(shape, minval=0.1, maxval=0.8, dtype=tf.float32)\n",
    "    return tf.Variable(initial_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f2001601dd0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 28 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## create layer 1 convolutional kernels (difference of gaussians)\n",
    "\n",
    "def difference_of_gaussians(ctr_sigma, surr_sigma, ctr_strength, surr_strength, x, y):\n",
    "    \n",
    "    center=0.4*(1/ctr_sigma)*exp(-0.5*square(sqrt(square(x)+square(y))/ctr_sigma))\n",
    "    \n",
    "    surround=0.4*(1/surr_sigma)*exp(-0.5*square(sqrt(square(x)+square(y))/surr_sigma))\n",
    "    \n",
    "    kernel = ctr_strength*center - surr_strength*surround\n",
    "    \n",
    "    maxk = amax(abs(kernel)) #normalization factor\n",
    "    \n",
    "    return kernel/maxk\n",
    "\n",
    "\n",
    "x = np.linspace(-5, 5, 11)\n",
    "y = np.linspace(-5, 5, 11)\n",
    "xv, yv = np.meshgrid(x, y)\n",
    "\n",
    "bipkernels = np.zeros([11, 11, 1, maxker])\n",
    "\n",
    "\n",
    "kernel1 = difference_of_gaussians(3, 6, 13, 12.9, xv, yv) \n",
    "kernel2 = difference_of_gaussians(5, 6, 18, 18, xv, yv)\n",
    "kernel3 = difference_of_gaussians(2, 4, 20, 14, xv, yv)\n",
    "kernel4 = difference_of_gaussians(3, 6, 13, 0, xv, yv)\n",
    "kernel5 = difference_of_gaussians(4, 6, 13, 0, xv, yv) \n",
    "kernel6 = difference_of_gaussians(2, 4, 20, 0, xv, yv)\n",
    "\n",
    "kernel7 = difference_of_gaussians(3, 6, 13, 20, xv, yv)\n",
    "kernel8 = difference_of_gaussians(5, 6, 18, 20, xv, yv) \n",
    "kernel9 = difference_of_gaussians(2, 4, 20, 24, xv, yv)\n",
    "\n",
    "kernel10 = difference_of_gaussians(5, 8, 13, 20, xv, yv)\n",
    "kernel11 = difference_of_gaussians(2, 8, 15, 15, xv, yv)\n",
    "kernel12 = difference_of_gaussians(3, 8, 20, 12, xv, yv)\n",
    "kernel13 = difference_of_gaussians(5, 8, 20, 18, xv, yv)\n",
    "kernel14 = difference_of_gaussians(2, 8, 13, 18, xv, yv)\n",
    "\n",
    "\n",
    "bipkernels[:, :, 0, 0]=kernel1\n",
    "bipkernels[:, :, 0, 1]=kernel2\n",
    "bipkernels[:, :, 0, 2]=kernel3\n",
    "bipkernels[:, :, 0, 3]=kernel4\n",
    "bipkernels[:, :, 0, 4]=-1.0*kernel1\n",
    "bipkernels[:, :, 0, 5]=-1.0*kernel2\n",
    "bipkernels[:, :, 0, 6]=-1.0*kernel3\n",
    "bipkernels[:, :, 0, 7]=-1.0*kernel4\n",
    "bipkernels[:, :, 0, 8]=kernel5\n",
    "bipkernels[:, :, 0, 9]=kernel6\n",
    "bipkernels[:, :, 0, 10]=kernel7\n",
    "bipkernels[:, :, 0, 11]=kernel8\n",
    "bipkernels[:, :, 0, 12]=-1.0*kernel5\n",
    "bipkernels[:, :, 0, 13]=-1.0*kernel6\n",
    "bipkernels[:, :, 0, 14]=-1.0*kernel7\n",
    "bipkernels[:, :, 0, 15]=-1.0*kernel8\n",
    "bipkernels[:, :, 0, 16]=kernel9\n",
    "bipkernels[:, :, 0, 17]=kernel10\n",
    "bipkernels[:, :, 0, 18]=kernel11\n",
    "bipkernels[:, :, 0, 19]=kernel12\n",
    "bipkernels[:, :, 0, 20]=-1.0*kernel9\n",
    "bipkernels[:, :, 0, 21]=-1.0*kernel10\n",
    "bipkernels[:, :, 0, 22]=-1.0*kernel11\n",
    "bipkernels[:, :, 0, 23]=-1.0*kernel12\n",
    "bipkernels[:, :, 0, 24]=kernel13\n",
    "bipkernels[:, :, 0, 25]=kernel14\n",
    "bipkernels[:, :, 0, 26]=-1.0*kernel13\n",
    "bipkernels[:, :, 0, 27]=-1.0*kernel14\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(4, 4, 1)\n",
    "plt.imshow(kernel1, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 2)\n",
    "plt.imshow(kernel2, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 3)\n",
    "plt.imshow(kernel3, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 4)\n",
    "plt.imshow(kernel4, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 5)\n",
    "plt.imshow(kernel5, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 6)\n",
    "plt.imshow(kernel6, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 7)\n",
    "plt.imshow(kernel7, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 8)\n",
    "plt.imshow(kernel8, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 9)\n",
    "plt.imshow(kernel9, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 10)\n",
    "plt.imshow(kernel10, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 11)\n",
    "plt.imshow(kernel11, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 12)\n",
    "plt.imshow(kernel12, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 13)\n",
    "plt.imshow(kernel13, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 14)\n",
    "plt.imshow(kernel14, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90761.33\n",
      "100538.164\n",
      "step: 0  loss: = 906.600937\n",
      "step: 100  loss: = 787.125625\n",
      "step: 200  loss: = 570.873359\n",
      "step: 300  loss: = 312.133984\n",
      "step: 400  loss: = 141.789902\n",
      "step: 500  loss: = 77.060488\n",
      "step: 600  loss: = 72.803896\n",
      "step: 700  loss: = 64.632666\n",
      "step: 800  loss: = 54.524731\n",
      "step: 900  loss: = 45.200762\n",
      "step: 1000  loss: = 36.854248\n",
      "step: 1100  loss: = 35.803574\n",
      "step: 1200  loss: = 33.335901\n",
      "step: 1300  loss: = 28.939253\n",
      "step: 1400  loss: = 23.605337\n",
      "step: 1500  loss: = 18.866896\n",
      "step: 1600  loss: = 18.313175\n",
      "step: 1700  loss: = 17.054845\n",
      "step: 1800  loss: = 14.949138\n",
      "step: 1900  loss: = 12.597913\n",
      "step: 2000  loss: = 10.664016\n",
      "step: 2100  loss: = 10.445200\n",
      "step: 2200  loss: =  9.943916\n",
      "step: 2300  loss: =  9.090223\n",
      "step: 2400  loss: =  8.110783\n",
      "step: 2500  loss: =  7.277394\n",
      "step: 2600  loss: =  7.177250\n",
      "step: 2700  loss: =  6.953016\n",
      "step: 2800  loss: =  6.570938\n",
      "step: 2900  loss: =  6.097966\n",
      "step: 3000  loss: =  5.657791\n",
      "step: 3100  loss: =  5.606224\n",
      "step: 3200  loss: =  5.485772\n",
      "step: 3300  loss: =  5.262208\n",
      "step: 3400  loss: =  5.007287\n",
      "step: 3500  loss: =  4.806689\n",
      "step: 3600  loss: =  4.788232\n",
      "step: 3700  loss: =  4.763176\n",
      "step: 3800  loss: =  4.719526\n",
      "step: 3900  loss: =  4.646101\n",
      "step: 4000  loss: =  4.549847\n",
      "step: 4100  loss: =  4.534580\n",
      "step: 4200  loss: =  4.497111\n",
      "step: 4300  loss: =  4.408076\n",
      "step: 4400  loss: =  4.283796\n",
      "step: 4500  loss: =  4.140522\n",
      "step: 4600  loss: =  4.122258\n",
      "step: 4700  loss: =  4.084247\n",
      "step: 4800  loss: =  4.043326\n",
      "step: 4900  loss: =  4.016269\n",
      "step: 5000  loss: =  3.993439\n",
      "step: 5100  loss: =  3.982763\n",
      "step: 5200  loss: =  3.967870\n",
      "step: 5300  loss: =  3.954312\n",
      "step: 5400  loss: =  3.938370\n",
      "step: 5500  loss: =  3.933666\n",
      "step: 5600  loss: =  3.913906\n",
      "step: 5700  loss: =  3.898266\n",
      "step: 5800  loss: =  3.900085\n",
      "step: 5900  loss: =  3.902910\n",
      "step: 6000  loss: =  3.905259\n",
      "step: 6100  loss: =  3.901544\n",
      "step: 6200  loss: =  3.892718\n",
      "completed data: 60  kernels: =  3.000000\n",
      "132133.38\n",
      "132005.36\n",
      "step: 0  loss: = 1318.836719\n",
      "step: 100  loss: = 1033.823359\n",
      "step: 200  loss: = 586.845820\n",
      "step: 300  loss: = 206.086816\n",
      "step: 400  loss: = 80.114438\n",
      "step: 500  loss: = 59.297812\n",
      "step: 600  loss: = 57.708921\n",
      "step: 700  loss: = 53.940303\n",
      "step: 800  loss: = 47.036699\n",
      "step: 900  loss: = 38.580029\n",
      "step: 1000  loss: = 31.419797\n",
      "step: 1100  loss: = 30.604675\n",
      "step: 1200  loss: = 28.750239\n",
      "step: 1300  loss: = 25.661243\n",
      "step: 1400  loss: = 22.285767\n",
      "step: 1500  loss: = 19.616512\n",
      "step: 1600  loss: = 19.312490\n",
      "step: 1700  loss: = 18.615387\n",
      "step: 1800  loss: = 17.462469\n",
      "step: 1900  loss: = 16.096768\n",
      "step: 2000  loss: = 14.830123\n",
      "step: 2100  loss: = 14.682262\n",
      "step: 2200  loss: = 14.332187\n",
      "step: 2300  loss: = 13.680796\n",
      "step: 2400  loss: = 12.757598\n",
      "step: 2500  loss: = 11.588202\n",
      "step: 2600  loss: = 11.419888\n",
      "step: 2700  loss: = 11.003235\n",
      "step: 2800  loss: = 10.136468\n",
      "step: 2900  loss: =  8.904264\n",
      "step: 3000  loss: =  7.640140\n",
      "step: 3100  loss: =  7.464797\n",
      "step: 3200  loss: =  7.070989\n",
      "step: 3300  loss: =  6.384594\n",
      "step: 3400  loss: =  5.602353\n",
      "step: 3500  loss: =  4.958399\n",
      "step: 3600  loss: =  4.889352\n",
      "step: 3700  loss: =  4.732356\n",
      "step: 3800  loss: =  4.468472\n",
      "step: 3900  loss: =  4.137099\n",
      "step: 4000  loss: =  3.823491\n",
      "step: 4100  loss: =  3.789917\n",
      "step: 4200  loss: =  3.718649\n",
      "step: 4300  loss: =  3.614225\n",
      "step: 4400  loss: =  3.516683\n",
      "step: 4500  loss: =  3.466311\n",
      "step: 4600  loss: =  3.459153\n",
      "step: 4700  loss: =  3.435306\n",
      "step: 4800  loss: =  3.367273\n",
      "step: 4900  loss: =  3.258877\n",
      "step: 5000  loss: =  3.192578\n",
      "step: 5100  loss: =  3.138160\n",
      "step: 5200  loss: =  3.152718\n",
      "step: 5300  loss: =  3.215624\n",
      "step: 5400  loss: =  3.259529\n",
      "step: 5500  loss: =  3.335989\n",
      "step: 5600  loss: =  3.426322\n",
      "completed data: 60  kernels: =  8.000000\n",
      "206414.86\n",
      "195246.22\n",
      "step: 0  loss: = 2057.103906\n",
      "step: 100  loss: = 1315.645313\n",
      "step: 200  loss: = 466.362969\n",
      "step: 300  loss: = 129.985615\n",
      "step: 400  loss: = 88.744365\n",
      "step: 500  loss: = 67.275488\n",
      "step: 600  loss: = 64.830176\n",
      "step: 700  loss: = 59.282256\n",
      "step: 800  loss: = 50.571978\n",
      "step: 900  loss: = 42.269526\n",
      "step: 1000  loss: = 37.056812\n",
      "step: 1100  loss: = 36.557917\n",
      "step: 1200  loss: = 35.499719\n",
      "step: 1300  loss: = 33.975352\n",
      "step: 1400  loss: = 32.588010\n",
      "step: 1500  loss: = 31.459958\n",
      "step: 1600  loss: = 31.314058\n",
      "step: 1700  loss: = 30.956050\n",
      "step: 1800  loss: = 30.278843\n",
      "step: 1900  loss: = 29.186299\n",
      "step: 2000  loss: = 27.979570\n",
      "step: 2100  loss: = 27.801389\n",
      "step: 2200  loss: = 27.353757\n",
      "step: 2300  loss: = 26.502449\n",
      "step: 2400  loss: = 25.221421\n",
      "step: 2500  loss: = 23.649272\n",
      "step: 2600  loss: = 23.408123\n",
      "step: 2700  loss: = 22.833958\n",
      "step: 2800  loss: = 21.469377\n",
      "step: 2900  loss: = 19.673142\n",
      "step: 3000  loss: = 17.730392\n",
      "step: 3100  loss: = 17.435243\n",
      "step: 3200  loss: = 16.715686\n",
      "step: 3300  loss: = 15.388252\n",
      "step: 3400  loss: = 13.323030\n",
      "step: 3500  loss: = 11.153125\n",
      "step: 3600  loss: = 10.902877\n",
      "step: 3700  loss: = 10.342198\n",
      "step: 3800  loss: =  9.340686\n",
      "step: 3900  loss: =  8.009277\n",
      "step: 4000  loss: =  6.797710\n",
      "step: 4100  loss: =  6.665640\n",
      "step: 4200  loss: =  6.391594\n",
      "step: 4300  loss: =  5.827983\n",
      "step: 4400  loss: =  5.082393\n",
      "step: 4500  loss: =  4.357727\n",
      "step: 4600  loss: =  4.279519\n",
      "step: 4700  loss: =  4.115283\n",
      "step: 4800  loss: =  3.846379\n",
      "step: 4900  loss: =  3.510113\n",
      "step: 5000  loss: =  3.283010\n",
      "step: 5100  loss: =  3.169420\n",
      "step: 5200  loss: =  3.083915\n",
      "step: 5300  loss: =  3.020806\n",
      "step: 5400  loss: =  2.997687\n",
      "step: 5500  loss: =  2.982292\n",
      "step: 5600  loss: =  2.921266\n",
      "step: 5700  loss: =  2.865840\n",
      "step: 5800  loss: =  2.842639\n",
      "step: 5900  loss: =  2.897211\n",
      "step: 6000  loss: =  2.973318\n",
      "step: 6100  loss: =  3.036573\n",
      "step: 6200  loss: =  3.091096\n",
      "step: 6300  loss: =  3.127221\n",
      "completed data: 60  kernels: = 16.000000\n",
      "168799.84\n",
      "173492.17\n",
      "step: 0  loss: = 1674.324063\n",
      "step: 100  loss: = 547.719023\n",
      "step: 200  loss: = 110.864160\n",
      "step: 300  loss: = 76.292417\n",
      "step: 400  loss: = 51.299287\n",
      "step: 500  loss: = 41.923579\n",
      "step: 600  loss: = 41.348018\n",
      "step: 700  loss: = 40.329087\n",
      "step: 800  loss: = 39.386624\n",
      "step: 900  loss: = 38.857046\n",
      "step: 1000  loss: = 38.266831\n",
      "step: 1100  loss: = 38.152603\n",
      "step: 1200  loss: = 37.848442\n",
      "step: 1300  loss: = 37.116113\n",
      "step: 1400  loss: = 35.954019\n",
      "step: 1500  loss: = 34.581062\n",
      "step: 1600  loss: = 34.357246\n",
      "step: 1700  loss: = 33.793711\n",
      "step: 1800  loss: = 32.492688\n",
      "step: 1900  loss: = 30.565461\n",
      "step: 2000  loss: = 28.233689\n",
      "step: 2100  loss: = 27.907471\n",
      "step: 2200  loss: = 27.105166\n",
      "step: 2300  loss: = 25.466025\n",
      "step: 2400  loss: = 23.280247\n",
      "step: 2500  loss: = 20.706663\n",
      "step: 2600  loss: = 20.341984\n",
      "step: 2700  loss: = 19.525410\n",
      "step: 2800  loss: = 17.968049\n",
      "step: 2900  loss: = 15.761222\n",
      "step: 3000  loss: = 13.562775\n",
      "step: 3100  loss: = 13.258247\n",
      "step: 3200  loss: = 12.463182\n",
      "step: 3300  loss: = 10.949602\n",
      "step: 3400  loss: =  9.184675\n",
      "step: 3500  loss: =  7.827482\n",
      "step: 3600  loss: =  7.645760\n",
      "step: 3700  loss: =  7.274452\n",
      "step: 3800  loss: =  6.759329\n",
      "step: 3900  loss: =  6.190934\n",
      "step: 4000  loss: =  5.779863\n",
      "step: 4100  loss: =  5.785649\n",
      "step: 4200  loss: =  5.746934\n",
      "step: 4300  loss: =  5.681375\n",
      "step: 4400  loss: =  5.430549\n",
      "step: 4500  loss: =  5.038001\n",
      "step: 4600  loss: =  4.991122\n",
      "step: 4700  loss: =  4.875946\n",
      "step: 4800  loss: =  4.686550\n",
      "step: 4900  loss: =  4.480508\n",
      "step: 5000  loss: =  4.287735\n",
      "step: 5100  loss: =  3.962257\n",
      "step: 5200  loss: =  3.715039\n",
      "step: 5300  loss: =  3.519944\n",
      "step: 5400  loss: =  3.440555\n",
      "step: 5500  loss: =  3.394174\n",
      "step: 5600  loss: =  3.373845\n",
      "step: 5700  loss: =  3.352182\n",
      "step: 5800  loss: =  3.371349\n",
      "step: 5900  loss: =  3.389420\n",
      "step: 6000  loss: =  3.391406\n",
      "step: 6100  loss: =  3.389820\n",
      "step: 6200  loss: =  3.367033\n",
      "completed data: 60  kernels: = 28.000000\n",
      "147113.7\n",
      "147449.38\n",
      "step: 0  loss: = 1462.976250\n",
      "step: 100  loss: = 689.354609\n",
      "step: 200  loss: = 112.117891\n",
      "step: 300  loss: = 41.212959\n",
      "step: 400  loss: = 19.194972\n",
      "step: 500  loss: =  7.635829\n",
      "step: 600  loss: =  6.628116\n",
      "step: 700  loss: =  4.752245\n",
      "step: 800  loss: =  3.062440\n",
      "step: 900  loss: =  2.001209\n",
      "step: 1000  loss: =  1.547932\n",
      "step: 1100  loss: =  1.521213\n",
      "step: 1200  loss: =  1.474886\n",
      "step: 1300  loss: =  1.382433\n",
      "step: 1400  loss: =  1.295992\n",
      "step: 1500  loss: =  1.218839\n",
      "step: 1600  loss: =  1.209749\n",
      "step: 1700  loss: =  1.186399\n",
      "step: 1800  loss: =  1.159368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1900  loss: =  1.119696\n",
      "step: 2000  loss: =  1.077324\n",
      "step: 2100  loss: =  1.076325\n",
      "step: 2200  loss: =  1.068903\n",
      "step: 2300  loss: =  1.060232\n",
      "step: 2400  loss: =  1.055493\n",
      "step: 2500  loss: =  1.077285\n",
      "step: 2600  loss: =  1.078442\n",
      "step: 2700  loss: =  1.076683\n",
      "step: 2800  loss: =  1.075094\n",
      "step: 2900  loss: =  1.063824\n",
      "step: 3000  loss: =  1.045768\n",
      "step: 3100  loss: =  1.044686\n",
      "step: 3200  loss: =  1.039047\n",
      "step: 3300  loss: =  1.032616\n",
      "step: 3400  loss: =  1.020092\n",
      "step: 3500  loss: =  1.011448\n",
      "step: 3600  loss: =  1.010396\n",
      "step: 3700  loss: =  1.005650\n",
      "step: 3800  loss: =  0.999718\n",
      "step: 3900  loss: =  0.995678\n",
      "step: 4000  loss: =  0.993050\n",
      "step: 4100  loss: =  0.994921\n",
      "step: 4200  loss: =  0.993672\n",
      "step: 4300  loss: =  0.991434\n",
      "step: 4400  loss: =  0.988960\n",
      "step: 4500  loss: =  0.985859\n",
      "step: 4600  loss: =  0.986708\n",
      "step: 4700  loss: =  0.985243\n",
      "step: 4800  loss: =  0.984226\n",
      "step: 4900  loss: =  0.979380\n",
      "step: 5000  loss: =  0.981954\n",
      "step: 5100  loss: =  0.978908\n",
      "step: 5200  loss: =  0.978467\n",
      "step: 5300  loss: =  0.979910\n",
      "step: 5400  loss: =  0.982624\n",
      "step: 5500  loss: =  0.981063\n",
      "step: 5600  loss: =  0.982073\n",
      "step: 5700  loss: =  0.984266\n",
      "step: 5800  loss: =  0.986190\n",
      "completed data: 340  kernels: =  3.000000\n",
      "100134.69\n",
      "103105.58\n",
      "step: 0  loss: = 990.165625\n",
      "step: 100  loss: = 208.330488\n",
      "step: 200  loss: = 55.009805\n",
      "step: 300  loss: = 24.812373\n",
      "step: 400  loss: =  7.385870\n",
      "step: 500  loss: =  3.230362\n",
      "step: 600  loss: =  2.924758\n",
      "step: 700  loss: =  2.323221\n",
      "step: 800  loss: =  1.751626\n",
      "step: 900  loss: =  1.374438\n",
      "step: 1000  loss: =  1.200950\n",
      "step: 1100  loss: =  1.192601\n",
      "step: 1200  loss: =  1.167367\n",
      "step: 1300  loss: =  1.115350\n",
      "step: 1400  loss: =  1.066918\n",
      "step: 1500  loss: =  1.041911\n",
      "step: 1600  loss: =  1.043535\n",
      "step: 1700  loss: =  1.036638\n",
      "step: 1800  loss: =  1.009417\n",
      "step: 1900  loss: =  0.984307\n",
      "step: 2000  loss: =  0.957379\n",
      "step: 2100  loss: =  0.959651\n",
      "step: 2200  loss: =  0.954231\n",
      "step: 2300  loss: =  0.949502\n",
      "step: 2400  loss: =  0.941199\n",
      "step: 2500  loss: =  0.917306\n",
      "step: 2600  loss: =  0.917915\n",
      "step: 2700  loss: =  0.912773\n",
      "step: 2800  loss: =  0.903770\n",
      "step: 2900  loss: =  0.888501\n",
      "step: 3000  loss: =  0.887290\n",
      "step: 3100  loss: =  0.890598\n",
      "step: 3200  loss: =  0.888253\n",
      "step: 3300  loss: =  0.881903\n",
      "step: 3400  loss: =  0.886448\n",
      "step: 3500  loss: =  0.876547\n",
      "step: 3600  loss: =  0.876593\n",
      "step: 3700  loss: =  0.873715\n",
      "step: 3800  loss: =  0.874194\n",
      "step: 3900  loss: =  0.867367\n",
      "step: 4000  loss: =  0.863342\n",
      "step: 4100  loss: =  0.864487\n",
      "step: 4200  loss: =  0.861350\n",
      "step: 4300  loss: =  0.855694\n",
      "step: 4400  loss: =  0.848950\n",
      "step: 4500  loss: =  0.846438\n",
      "step: 4600  loss: =  0.850234\n",
      "step: 4700  loss: =  0.851454\n",
      "step: 4800  loss: =  0.849085\n",
      "step: 4900  loss: =  0.839546\n",
      "step: 5000  loss: =  0.838962\n",
      "step: 5100  loss: =  0.837462\n",
      "step: 5200  loss: =  0.836122\n",
      "step: 5300  loss: =  0.839422\n",
      "step: 5400  loss: =  0.836859\n",
      "step: 5500  loss: =  0.829842\n",
      "step: 5600  loss: =  0.837448\n",
      "step: 5700  loss: =  0.833582\n",
      "step: 5800  loss: =  0.835769\n",
      "step: 5900  loss: =  0.831132\n",
      "step: 6000  loss: =  0.833359\n",
      "step: 6100  loss: =  0.829034\n",
      "step: 6200  loss: =  0.837357\n",
      "step: 6300  loss: =  0.830299\n",
      "step: 6400  loss: =  0.830674\n",
      "step: 6500  loss: =  0.833544\n",
      "step: 6600  loss: =  0.828234\n",
      "step: 6700  loss: =  0.830822\n",
      "completed data: 340  kernels: =  8.000000\n",
      "206241.52\n",
      "248030.14\n",
      "step: 0  loss: = 2025.076250\n",
      "step: 100  loss: = 173.343613\n",
      "step: 200  loss: = 79.521855\n",
      "step: 300  loss: = 27.658516\n",
      "step: 400  loss: =  6.925350\n",
      "step: 500  loss: =  2.306261\n",
      "step: 600  loss: =  2.000696\n",
      "step: 700  loss: =  1.478846\n",
      "step: 800  loss: =  1.189188\n",
      "step: 900  loss: =  1.068507\n",
      "step: 1000  loss: =  0.906964\n",
      "step: 1100  loss: =  0.906757\n",
      "step: 1200  loss: =  0.893830\n",
      "step: 1300  loss: =  0.824447\n",
      "step: 1400  loss: =  0.767491\n",
      "step: 1500  loss: =  0.733807\n",
      "step: 1600  loss: =  0.736798\n",
      "step: 1700  loss: =  0.721137\n",
      "step: 1800  loss: =  0.693020\n",
      "step: 1900  loss: =  0.676027\n",
      "step: 2000  loss: =  0.651602\n",
      "step: 2100  loss: =  0.650667\n",
      "step: 2200  loss: =  0.644857\n",
      "step: 2300  loss: =  0.634135\n",
      "step: 2400  loss: =  0.628187\n",
      "step: 2500  loss: =  0.619517\n",
      "step: 2600  loss: =  0.629592\n",
      "step: 2700  loss: =  0.629774\n",
      "step: 2800  loss: =  0.618480\n",
      "step: 2900  loss: =  0.619915\n",
      "step: 3000  loss: =  0.616941\n",
      "step: 3100  loss: =  0.620253\n",
      "completed data: 340  kernels: = 16.000000\n",
      "192932.05\n",
      "208182.19\n",
      "step: 0  loss: = 1849.676094\n",
      "step: 100  loss: = 106.267520\n",
      "step: 200  loss: = 42.019131\n",
      "step: 300  loss: =  9.122310\n",
      "step: 400  loss: =  2.534265\n",
      "step: 500  loss: =  1.440107\n",
      "step: 600  loss: =  1.389538\n",
      "step: 700  loss: =  1.276033\n",
      "step: 800  loss: =  1.130483\n",
      "step: 900  loss: =  0.959731\n",
      "step: 1000  loss: =  0.827583\n",
      "step: 1100  loss: =  0.822401\n",
      "step: 1200  loss: =  0.779664\n",
      "step: 1300  loss: =  0.773702\n",
      "step: 1400  loss: =  0.694024\n",
      "step: 1500  loss: =  0.665565\n",
      "step: 1600  loss: =  0.673543\n",
      "step: 1700  loss: =  0.684223\n",
      "step: 1800  loss: =  0.665782\n",
      "step: 1900  loss: =  0.658887\n",
      "step: 2000  loss: =  0.645341\n",
      "step: 2100  loss: =  0.660034\n",
      "step: 2200  loss: =  0.642049\n",
      "step: 2300  loss: =  0.643642\n",
      "step: 2400  loss: =  0.662181\n",
      "step: 2500  loss: =  0.651410\n",
      "step: 2600  loss: =  0.653619\n",
      "completed data: 340  kernels: = 28.000000\n",
      "61729.145\n",
      "73454.71\n",
      "step: 0  loss: = 609.029297\n",
      "step: 100  loss: = 103.043301\n",
      "step: 200  loss: = 34.987712\n",
      "step: 300  loss: =  6.394368\n",
      "step: 400  loss: =  2.337208\n",
      "step: 500  loss: =  1.404238\n",
      "step: 600  loss: =  1.355463\n",
      "step: 700  loss: =  1.262915\n",
      "step: 800  loss: =  1.145632\n",
      "step: 900  loss: =  1.052973\n",
      "step: 1000  loss: =  0.969391\n",
      "step: 1100  loss: =  0.957763\n",
      "step: 1200  loss: =  0.936913\n",
      "step: 1300  loss: =  0.931565\n",
      "step: 1400  loss: =  0.923108\n",
      "step: 1500  loss: =  0.913347\n",
      "step: 1600  loss: =  0.915410\n",
      "step: 1700  loss: =  0.912405\n",
      "step: 1800  loss: =  0.909117\n",
      "step: 1900  loss: =  0.902877\n",
      "step: 2000  loss: =  0.902445\n",
      "step: 2100  loss: =  0.902895\n",
      "step: 2200  loss: =  0.901284\n",
      "step: 2300  loss: =  0.901667\n",
      "step: 2400  loss: =  0.906796\n",
      "step: 2500  loss: =  0.912630\n",
      "step: 2600  loss: =  0.910946\n",
      "completed data: 700  kernels: =  3.000000\n",
      "110959.734\n",
      "117563.625\n",
      "step: 0  loss: = 1084.082969\n",
      "step: 100  loss: = 50.976514\n",
      "step: 200  loss: = 20.633743\n",
      "step: 300  loss: =  3.987343\n",
      "step: 400  loss: =  1.728945\n",
      "step: 500  loss: =  1.242146\n",
      "step: 600  loss: =  1.214104\n",
      "step: 700  loss: =  1.196445\n",
      "step: 800  loss: =  1.136137\n",
      "step: 900  loss: =  1.053508\n",
      "step: 1000  loss: =  1.025484\n",
      "step: 1100  loss: =  1.013254\n",
      "step: 1200  loss: =  1.009630\n",
      "step: 1300  loss: =  0.988629\n",
      "step: 1400  loss: =  0.953770\n",
      "step: 1500  loss: =  0.944429\n",
      "step: 1600  loss: =  0.937625\n",
      "step: 1700  loss: =  0.932392\n",
      "step: 1800  loss: =  0.926489\n",
      "step: 1900  loss: =  0.896539\n",
      "step: 2000  loss: =  0.909448\n",
      "step: 2100  loss: =  0.917283\n",
      "step: 2200  loss: =  0.911883\n",
      "step: 2300  loss: =  0.904242\n",
      "step: 2400  loss: =  0.921602\n",
      "step: 2500  loss: =  0.913605\n",
      "step: 2600  loss: =  0.909917\n",
      "step: 2700  loss: =  0.907237\n",
      "step: 2800  loss: =  0.895910\n",
      "step: 2900  loss: =  0.899413\n",
      "step: 3000  loss: =  0.896827\n",
      "step: 3100  loss: =  0.899875\n",
      "step: 3200  loss: =  0.898571\n",
      "step: 3300  loss: =  0.900700\n",
      "step: 3400  loss: =  0.892241\n",
      "step: 3500  loss: =  0.897817\n",
      "step: 3600  loss: =  0.894711\n",
      "step: 3700  loss: =  0.893685\n",
      "step: 3800  loss: =  0.888286\n",
      "step: 3900  loss: =  0.872706\n",
      "step: 4000  loss: =  0.892698\n",
      "step: 4100  loss: =  0.892983\n",
      "step: 4200  loss: =  0.892342\n",
      "step: 4300  loss: =  0.881056\n",
      "step: 4400  loss: =  0.883061\n",
      "step: 4500  loss: =  0.895774\n",
      "step: 4600  loss: =  0.893269\n",
      "step: 4700  loss: =  0.890614\n",
      "step: 4800  loss: =  0.879262\n",
      "step: 4900  loss: =  0.897262\n",
      "step: 5000  loss: =  0.884353\n",
      "step: 5100  loss: =  0.886945\n",
      "step: 5200  loss: =  0.875346\n",
      "step: 5300  loss: =  0.883585\n",
      "step: 5400  loss: =  0.897339\n",
      "step: 5500  loss: =  0.880118\n",
      "step: 5600  loss: =  0.872577\n",
      "completed data: 700  kernels: =  8.000000\n",
      "164702.34\n",
      "161829.12\n",
      "step: 0  loss: = 1579.375781\n",
      "step: 100  loss: = 69.745562\n",
      "step: 200  loss: = 18.154949\n",
      "step: 300  loss: =  3.428518\n",
      "step: 400  loss: =  1.457484\n",
      "step: 500  loss: =  1.083591\n",
      "step: 600  loss: =  1.062175\n",
      "step: 700  loss: =  0.988820\n",
      "step: 800  loss: =  0.871215\n",
      "step: 900  loss: =  0.823811\n",
      "step: 1000  loss: =  0.774646\n",
      "step: 1100  loss: =  0.769990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1200  loss: =  0.758227\n",
      "step: 1300  loss: =  0.744133\n",
      "step: 1400  loss: =  0.734927\n",
      "step: 1500  loss: =  0.753542\n",
      "step: 1600  loss: =  0.752607\n",
      "step: 1700  loss: =  0.769413\n",
      "step: 1800  loss: =  0.752476\n",
      "step: 1900  loss: =  0.787823\n",
      "step: 2000  loss: =  0.753505\n",
      "step: 2100  loss: =  0.747035\n",
      "step: 2200  loss: =  0.743865\n",
      "step: 2300  loss: =  0.748402\n",
      "step: 2400  loss: =  0.784137\n",
      "step: 2500  loss: =  0.750234\n",
      "step: 2600  loss: =  0.751050\n",
      "step: 2700  loss: =  0.750142\n",
      "step: 2800  loss: =  0.733373\n",
      "completed data: 700  kernels: = 16.000000\n",
      "208816.7\n",
      "234310.38\n",
      "step: 0  loss: = 1923.450000\n",
      "step: 100  loss: = 94.152090\n",
      "step: 200  loss: = 13.475408\n",
      "step: 300  loss: =  2.358350\n",
      "step: 400  loss: =  1.314632\n",
      "step: 500  loss: =  0.966291\n",
      "step: 600  loss: =  0.999237\n",
      "step: 700  loss: =  0.955525\n",
      "step: 800  loss: =  0.856224\n",
      "step: 900  loss: =  0.826904\n",
      "step: 1000  loss: =  0.815485\n",
      "step: 1100  loss: =  0.832569\n",
      "step: 1200  loss: =  0.847845\n",
      "step: 1300  loss: =  0.814580\n",
      "step: 1400  loss: =  0.818726\n",
      "step: 1500  loss: =  0.852037\n",
      "step: 1600  loss: =  0.852081\n",
      "completed data: 700  kernels: = 28.000000\n",
      "70580.26\n",
      "74323.305\n",
      "step: 0  loss: = 686.000391\n",
      "step: 100  loss: = 35.078489\n",
      "step: 200  loss: =  7.187864\n",
      "step: 300  loss: =  1.488397\n",
      "step: 400  loss: =  1.033284\n",
      "step: 500  loss: =  0.879026\n",
      "step: 600  loss: =  0.865277\n",
      "step: 700  loss: =  0.843093\n",
      "step: 800  loss: =  0.814779\n",
      "step: 900  loss: =  0.798750\n",
      "step: 1000  loss: =  0.797578\n",
      "step: 1100  loss: =  0.792772\n",
      "step: 1200  loss: =  0.788273\n",
      "step: 1300  loss: =  0.772517\n",
      "step: 1400  loss: =  0.771723\n",
      "step: 1500  loss: =  0.772901\n",
      "step: 1600  loss: =  0.773056\n",
      "step: 1700  loss: =  0.770630\n",
      "step: 1800  loss: =  0.772295\n",
      "step: 1900  loss: =  0.757010\n",
      "step: 2000  loss: =  0.770451\n",
      "completed data: 1400  kernels: =  3.000000\n",
      "41606.85\n",
      "48842.566\n",
      "step: 0  loss: = 390.226484\n",
      "step: 100  loss: = 25.706665\n",
      "step: 200  loss: =  4.381001\n",
      "step: 300  loss: =  1.393487\n",
      "step: 400  loss: =  0.995530\n",
      "step: 500  loss: =  0.949839\n",
      "step: 600  loss: =  0.936512\n",
      "step: 700  loss: =  0.942917\n",
      "step: 800  loss: =  0.964165\n",
      "step: 900  loss: =  0.942712\n",
      "step: 1000  loss: =  0.915404\n",
      "step: 1100  loss: =  0.916896\n",
      "step: 1200  loss: =  0.913963\n",
      "step: 1300  loss: =  0.905644\n",
      "step: 1400  loss: =  0.882367\n",
      "step: 1500  loss: =  0.893825\n",
      "step: 1600  loss: =  0.899617\n",
      "step: 1700  loss: =  0.909266\n",
      "step: 1800  loss: =  0.898313\n",
      "step: 1900  loss: =  0.881466\n",
      "completed data: 1400  kernels: =  8.000000\n",
      "91106.38\n",
      "101431.79\n",
      "step: 0  loss: = 822.438906\n",
      "step: 100  loss: = 31.635901\n",
      "step: 200  loss: =  3.663145\n",
      "step: 300  loss: =  1.298536\n",
      "step: 400  loss: =  0.930286\n",
      "step: 500  loss: =  0.890726\n",
      "step: 600  loss: =  0.879648\n",
      "step: 700  loss: =  0.869607\n",
      "step: 800  loss: =  0.864120\n",
      "step: 900  loss: =  0.890701\n",
      "step: 1000  loss: =  0.880383\n",
      "step: 1100  loss: =  0.871365\n",
      "step: 1200  loss: =  0.867892\n",
      "step: 1300  loss: =  0.869060\n",
      "step: 1400  loss: =  0.856629\n",
      "step: 1500  loss: =  0.880960\n",
      "step: 1600  loss: =  0.872692\n",
      "step: 1700  loss: =  0.873892\n",
      "step: 1800  loss: =  0.869557\n",
      "step: 1900  loss: =  0.850182\n",
      "step: 2000  loss: =  0.881206\n",
      "step: 2100  loss: =  0.869985\n",
      "step: 2200  loss: =  0.862382\n",
      "step: 2300  loss: =  0.855077\n",
      "step: 2400  loss: =  0.848967\n",
      "step: 2500  loss: =  0.877140\n",
      "step: 2600  loss: =  0.864264\n",
      "step: 2700  loss: =  0.865288\n",
      "step: 2800  loss: =  0.879951\n",
      "step: 2900  loss: =  0.865859\n",
      "step: 3000  loss: =  0.880231\n",
      "completed data: 1400  kernels: = 16.000000\n",
      "171491.78\n",
      "159125.38\n",
      "step: 0  loss: = 1424.511875\n",
      "step: 100  loss: = 17.515313\n",
      "step: 200  loss: =  2.540733\n",
      "step: 300  loss: =  1.142454\n",
      "step: 400  loss: =  1.075173\n",
      "step: 500  loss: =  0.935825\n",
      "step: 600  loss: =  0.963227\n",
      "step: 700  loss: =  0.934098\n",
      "step: 800  loss: =  1.003418\n",
      "step: 900  loss: =  0.946871\n",
      "step: 1000  loss: =  0.951716\n",
      "step: 1100  loss: =  0.937564\n",
      "step: 1200  loss: =  0.915333\n",
      "step: 1300  loss: =  0.922405\n",
      "step: 1400  loss: =  0.934893\n",
      "step: 1500  loss: =  0.956607\n",
      "step: 1600  loss: =  0.937676\n",
      "step: 1700  loss: =  0.944241\n",
      "completed data: 1400  kernels: = 28.000000\n",
      "63064.242\n",
      "77809.59\n",
      "step: 0  loss: = 595.773906\n",
      "step: 100  loss: = 18.373784\n",
      "step: 200  loss: =  1.656417\n",
      "step: 300  loss: =  1.087194\n",
      "step: 400  loss: =  0.878664\n",
      "step: 500  loss: =  0.888970\n",
      "step: 600  loss: =  0.884824\n",
      "step: 700  loss: =  0.876423\n",
      "step: 800  loss: =  0.873847\n",
      "step: 900  loss: =  0.870900\n",
      "step: 1000  loss: =  0.872768\n",
      "step: 1100  loss: =  0.871044\n",
      "step: 1200  loss: =  0.885563\n",
      "step: 1300  loss: =  0.878767\n",
      "step: 1400  loss: =  0.868219\n",
      "step: 1500  loss: =  0.880492\n",
      "step: 1600  loss: =  0.873975\n",
      "step: 1700  loss: =  0.871252\n",
      "step: 1800  loss: =  0.878411\n",
      "step: 1900  loss: =  0.875193\n",
      "step: 2000  loss: =  0.877304\n",
      "step: 2100  loss: =  0.877077\n",
      "step: 2200  loss: =  0.874984\n",
      "step: 2300  loss: =  0.885905\n",
      "step: 2400  loss: =  0.872136\n",
      "step: 2500  loss: =  0.873730\n",
      "step: 2600  loss: =  0.875316\n",
      "step: 2700  loss: =  0.878655\n",
      "step: 2800  loss: =  0.873731\n",
      "completed data: 2800  kernels: =  3.000000\n",
      "53954.293\n",
      "57156.832\n",
      "step: 0  loss: = 482.296055\n",
      "step: 100  loss: =  9.018749\n",
      "step: 200  loss: =  1.481347\n",
      "step: 300  loss: =  0.987121\n",
      "step: 400  loss: =  0.973136\n",
      "step: 500  loss: =  0.933675\n",
      "step: 600  loss: =  0.929836\n",
      "step: 700  loss: =  0.912644\n",
      "step: 800  loss: =  0.892827\n",
      "step: 900  loss: =  0.865823\n",
      "step: 1000  loss: =  0.899387\n",
      "step: 1100  loss: =  0.907377\n",
      "step: 1200  loss: =  0.936914\n",
      "step: 1300  loss: =  0.937659\n",
      "step: 1400  loss: =  0.918344\n",
      "completed data: 2800  kernels: =  8.000000\n",
      "212845.83\n",
      "211267.77\n",
      "step: 0  loss: = 1828.979063\n",
      "step: 100  loss: = 13.248104\n",
      "step: 200  loss: =  1.539632\n",
      "step: 300  loss: =  1.024997\n",
      "step: 400  loss: =  0.990280\n",
      "step: 500  loss: =  0.982523\n",
      "step: 600  loss: =  0.988784\n",
      "step: 700  loss: =  0.969800\n",
      "step: 800  loss: =  1.003126\n",
      "step: 900  loss: =  1.007339\n",
      "step: 1000  loss: =  1.008021\n",
      "step: 1100  loss: =  1.003331\n",
      "completed data: 2800  kernels: = 16.000000\n",
      "140531.19\n",
      "154086.61\n",
      "step: 0  loss: = 935.623047\n",
      "step: 100  loss: =  5.928198\n",
      "step: 200  loss: =  1.247644\n",
      "step: 300  loss: =  1.121222\n",
      "step: 400  loss: =  1.031058\n",
      "step: 500  loss: =  1.022121\n",
      "step: 600  loss: =  1.019542\n",
      "step: 700  loss: =  1.056232\n",
      "step: 800  loss: =  1.045092\n",
      "step: 900  loss: =  1.227554\n",
      "step: 1000  loss: =  0.986481\n",
      "step: 1100  loss: =  1.019523\n",
      "step: 1200  loss: =  1.030078\n",
      "step: 1300  loss: =  1.045735\n",
      "step: 1400  loss: =  1.171887\n",
      "completed data: 2800  kernels: = 28.000000\n",
      "139714.53\n",
      "143156.3\n",
      "step: 0  loss: = 1279.775078\n",
      "step: 100  loss: =  4.458267\n",
      "step: 200  loss: =  1.098035\n",
      "step: 300  loss: =  0.936019\n",
      "step: 400  loss: =  0.910054\n",
      "step: 500  loss: =  0.913206\n",
      "step: 600  loss: =  0.915464\n",
      "step: 700  loss: =  0.914186\n",
      "step: 800  loss: =  0.913054\n",
      "step: 900  loss: =  0.894605\n",
      "step: 1000  loss: =  0.911898\n",
      "step: 1100  loss: =  0.904433\n",
      "step: 1200  loss: =  0.917456\n",
      "step: 1300  loss: =  0.910815\n",
      "step: 1400  loss: =  0.924018\n",
      "step: 1500  loss: =  0.903233\n",
      "step: 1600  loss: =  0.910578\n",
      "step: 1700  loss: =  0.910901\n",
      "step: 1800  loss: =  0.902574\n",
      "step: 1900  loss: =  0.924640\n",
      "completed data: 5500  kernels: =  3.000000\n",
      "62059.945\n",
      "63408.695\n",
      "step: 0  loss: = 502.447500\n",
      "step: 100  loss: =  3.034892\n",
      "step: 200  loss: =  1.006353\n",
      "step: 300  loss: =  0.944337\n",
      "step: 400  loss: =  0.892810\n",
      "step: 500  loss: =  0.897060\n",
      "step: 600  loss: =  0.887233\n",
      "step: 700  loss: =  0.884189\n",
      "step: 800  loss: =  0.888068\n",
      "step: 900  loss: =  0.867381\n",
      "step: 1000  loss: =  0.877835\n",
      "step: 1100  loss: =  0.884057\n",
      "step: 1200  loss: =  0.888733\n",
      "step: 1300  loss: =  0.909086\n",
      "completed data: 5500  kernels: =  8.000000\n",
      "265673.62\n",
      "265296.53\n",
      "step: 0  loss: = 2027.170781\n",
      "step: 100  loss: =  2.925283\n",
      "step: 200  loss: =  1.103727\n",
      "step: 300  loss: =  0.989685\n",
      "step: 400  loss: =  0.948676\n",
      "step: 500  loss: =  0.962277\n",
      "step: 600  loss: =  0.963415\n",
      "step: 700  loss: =  0.935734\n",
      "step: 800  loss: =  0.970900\n",
      "step: 900  loss: =  0.955185\n",
      "step: 1000  loss: =  0.971432\n",
      "completed data: 5500  kernels: = 16.000000\n",
      "296790.28\n",
      "328069.38\n",
      "step: 0  loss: = 1638.732656\n",
      "step: 100  loss: =  2.702097\n",
      "step: 200  loss: =  1.082883\n",
      "step: 300  loss: =  0.976626\n",
      "step: 400  loss: =  1.085922\n",
      "step: 500  loss: =  1.003564\n",
      "step: 600  loss: =  0.970541\n",
      "step: 700  loss: =  0.946851\n",
      "step: 800  loss: =  1.058763\n",
      "step: 900  loss: =  0.974131\n",
      "step: 1000  loss: =  1.012183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1100  loss: =  1.045967\n",
      "step: 1200  loss: =  0.940503\n",
      "step: 1300  loss: =  1.010814\n",
      "completed data: 5500  kernels: = 28.000000\n",
      "50759.69\n",
      "71028.34\n",
      "step: 0  loss: = 401.179336\n",
      "step: 100  loss: =  1.472515\n",
      "step: 200  loss: =  1.025870\n",
      "step: 300  loss: =  0.960902\n",
      "step: 400  loss: =  0.961560\n",
      "step: 500  loss: =  0.965563\n",
      "step: 600  loss: =  0.959412\n",
      "step: 700  loss: =  0.951688\n",
      "step: 800  loss: =  0.950056\n",
      "step: 900  loss: =  0.953716\n",
      "step: 1000  loss: =  0.953670\n",
      "step: 1100  loss: =  0.955128\n",
      "step: 1200  loss: =  0.957468\n",
      "step: 1300  loss: =  0.977640\n",
      "step: 1400  loss: =  0.960708\n",
      "completed data: 11000  kernels: =  3.000000\n",
      "71621.336\n",
      "80588.92\n",
      "step: 0  loss: = 473.811680\n",
      "step: 100  loss: =  1.192201\n",
      "step: 200  loss: =  0.907570\n",
      "step: 300  loss: =  0.853495\n",
      "step: 400  loss: =  0.830141\n",
      "step: 500  loss: =  0.834479\n",
      "step: 600  loss: =  0.840843\n",
      "step: 700  loss: =  0.824005\n",
      "step: 800  loss: =  0.872129\n",
      "step: 900  loss: =  0.834649\n",
      "step: 1000  loss: =  0.824585\n",
      "completed data: 11000  kernels: =  8.000000\n",
      "167397.98\n",
      "207227.44\n",
      "step: 0  loss: = 846.584766\n",
      "step: 100  loss: =  1.441407\n",
      "step: 200  loss: =  0.970401\n",
      "step: 300  loss: =  0.992210\n",
      "step: 400  loss: =  0.951200\n",
      "step: 500  loss: =  0.993987\n",
      "step: 600  loss: =  0.968427\n",
      "step: 700  loss: =  0.993826\n",
      "step: 800  loss: =  0.974975\n",
      "step: 900  loss: =  1.056676\n",
      "step: 1000  loss: =  0.995733\n",
      "step: 1100  loss: =  0.963507\n",
      "step: 1200  loss: =  0.956552\n",
      "step: 1300  loss: =  0.933781\n",
      "step: 1400  loss: =  0.952045\n",
      "step: 1500  loss: =  0.993800\n",
      "step: 1600  loss: =  1.012083\n",
      "step: 1700  loss: =  1.007288\n",
      "step: 1800  loss: =  0.928710\n",
      "completed data: 11000  kernels: = 16.000000\n",
      "177539.2\n",
      "193535.16\n",
      "step: 0  loss: = 399.697852\n",
      "step: 100  loss: =  1.117023\n",
      "step: 200  loss: =  1.020204\n",
      "step: 300  loss: =  1.029271\n",
      "step: 400  loss: =  0.966053\n",
      "step: 500  loss: =  1.026251\n",
      "step: 600  loss: =  1.024287\n",
      "step: 700  loss: =  0.989976\n",
      "step: 800  loss: =  1.037642\n",
      "step: 900  loss: =  1.020066\n",
      "step: 1000  loss: =  1.066575\n",
      "step: 1100  loss: =  1.075107\n",
      "step: 1200  loss: =  1.004996\n",
      "step: 1300  loss: =  1.089562\n",
      "completed data: 11000  kernels: = 28.000000\n"
     ]
    }
   ],
   "source": [
    "kernels = [3, 8, 16, 28]\n",
    "lambdas = [1e1, 1e1, 1e1, 1e1] # knowledge 0.2\n",
    "\n",
    "datas = [60, 340, 700, 1400, 2800, 5500, 11000, 22000, 98000]\n",
    "training_epochs = [6000, 4000, 4000, 3000, 3000, 2000, 2000, 1000, 500]\n",
    "test_sizes = [2080, 2080, 2080, 2080, 2080, 2080, 2080, 2080, 2080, 2000]\n",
    "\n",
    "learn_rate = 1e-3\n",
    "learn_rate_late = 1e-4\n",
    "\n",
    "\n",
    "for i_data in range(7):\n",
    "    \n",
    "    for i_kernel in range(4): \n",
    "        \n",
    "        if i_kernel > 0: \n",
    "            del stimulus_\n",
    "            del bipolar_cell_layer\n",
    "            del gc_activation\n",
    "            del gc_output\n",
    "            del bipolar_bias\n",
    "            del bipkernels1\n",
    "        \n",
    "        no_train=datas[i_data]\n",
    "        epochs = training_epochs[i_data]\n",
    "        no_kernels = kernels[i_kernel]\n",
    "        lambda1 = lambdas[i_kernel]\n",
    "        bipkernels1 = bipkernels[:, :, :, 0:no_kernels]\n",
    "        bip_gc_syn_init = bip_gc_syn_init1[:, :, :, 0:no_kernels]\n",
    "        bip_am_syn_mask1 = bip_am_syn_mask[ :, :, 0:no_kernels, :, :]\n",
    "        \n",
    "        \n",
    "        no_test=test_sizes[i_data] \n",
    "        no_bipolars = 10\n",
    "        no_amacrines = 5\n",
    "\n",
    "        wheretosave = '/home/ubuntu/Notebooks/Circuit2_Trained_Network_data' + str(no_train) + '_kernel' + str(no_kernels) \\\n",
    "        + '_sd' + str(sd) + '_nosignconstraint.mat'\n",
    "\n",
    "        ## initialize all variables\n",
    "\n",
    "        bip_bias_init_all = -1.0*np.ones([28])\n",
    "        bip_bias_init_all[0]=-2.0\n",
    "        bip_bias_init_all[1]=-3.0\n",
    "        bip_bias_init_all[3]=-15.0\n",
    "        bip_bias_init_all[8]=-25.0\n",
    "        bip_bias_init_all[9]=-10.0\n",
    "        \n",
    "        bip_bias_init_all[4]=-2.0\n",
    "        bip_bias_init_all[5]=-3.0\n",
    "        bip_bias_init_all[7]=-15.0\n",
    "        bip_bias_init_all[12]=-25.0\n",
    "        bip_bias_init_all[13]=-10.0\n",
    "\n",
    "        \n",
    "\n",
    "        bip_bias_init = bip_bias_init_all[0:no_kernels]\n",
    "        bip_bias_init = bip_bias_init.astype(float32)\n",
    "        bipolar_bias = bias_var([no_kernels], bip_bias_init)\n",
    "        \n",
    "        am_bias_init = -5.0 \n",
    "        am_bias = bias_var([1], am_bias_init)\n",
    "        \n",
    "        gc_bias = bias_var([1], gc_bias_init)\n",
    "        \n",
    "        bip_gc_syn_init=tf.random.normal([1, no_bipolars, no_bipolars, no_kernels], mean = 0.0, stddev = sqrt(2.0/(no_kernels*100)), dtype=tf.dtypes.float32, seed=sd)\n",
    "        bip_gc_syn = synapse_var([1, no_bipolars, no_bipolars, no_kernels], bip_gc_syn_init)\n",
    "        \n",
    "        bip_am_syn_inds = np.zeros([no_kernels*100, 6])\n",
    "        for i in range(10):\n",
    "            for j in range(10):\n",
    "                for k in range(no_kernels):\n",
    "                    bip_am_syn_inds[no_kernels*10*(i)+no_kernels*(j)+k]=[0, i, j, k, floor(i/2), floor(j/2)]\n",
    "        bip_am_syn_inds = bip_am_syn_inds.astype(int64)\n",
    "        bip_am_syn_init11 = np.random.normal(0.0, (sqrt(2.0/no_kernels)), size=[no_kernels*100])\n",
    "        bip_am_syn_init111=bip_am_syn_init11.astype(float32)  \n",
    "        bip_am_syn_val = synapse_var([no_kernels*no_bipolars*no_bipolars], bip_am_syn_init111)\n",
    "        bip_am_syn1 = tf.sparse.SparseTensor(indices=bip_am_syn_inds, values=bip_am_syn_val, dense_shape=[1, no_bipolars, no_bipolars, no_kernels, no_amacrines, no_amacrines])\n",
    "        bip_am_syn = tf.sparse.to_dense(tf.sparse.reorder(bip_am_syn1))        \n",
    "\n",
    "        am_gc_syn = synapse_var([1, no_amacrines, no_amacrines], am_gc_syn_init1)\n",
    "\n",
    "        stimulus_ = tf.placeholder(\"float32\", name=\"stim_placeholder\")\n",
    "\n",
    "        bipolar_cell_layer = tf.nn.relu(tf.nn.bias_add(bip_conv2d(stimulus_, bipkernels1), bipolar_bias))\n",
    "\n",
    "        biplyr = tf.reshape(bipolar_cell_layer, [-1, no_bipolars*no_bipolars*no_kernels, 1])\n",
    "\n",
    "        tilebip_am_syn=tf.tile(tf.transpose(tf.reshape(bip_am_syn, [1, no_bipolars*no_bipolars*no_kernels, no_amacrines*no_amacrines]), [0, 2, 1]), [1, 1, 1])\n",
    "\n",
    "        amacrine_activation = 3.0*tf.reshape(tf.linalg.matmul(tilebip_am_syn, biplyr), [-1,no_amacrines, no_amacrines])\n",
    "     \n",
    "        amacrine_cell_layer = tf.nn.relu(tf.add(amacrine_activation, am_bias))\n",
    "\n",
    "        gc_activation = tf.multiply(bip_gc_syn, bipolar_cell_layer)\n",
    "\n",
    "        gc_activation_inhib = tf.multiply(am_gc_syn, amacrine_cell_layer)\n",
    "\n",
    "        gc_output = tf.add_n([tf.reduce_sum(gc_activation, [1, 2, 3]), tf.reduce_sum(gc_activation_inhib, [1, 2])])\n",
    "\n",
    "        ## training procedure\n",
    "        y_ = tf.placeholder(\"float32\", name=\"output_spikes\")\n",
    "        \n",
    "        batchsize=20\n",
    "\n",
    "        loss = (tf.nn.l2_loss((tf.squeeze(gc_output) - tf.squeeze(y_)), name='loss'))\n",
    "\n",
    "        regularizer=tf.add_n([tf.reduce_sum(tf.abs(bip_am_syn)), tf.reduce_sum(tf.abs(bip_gc_syn)), \\\n",
    "                              0.0*tf.reduce_sum(tf.abs(am_gc_syn))])\n",
    "\n",
    "        objective=tf.add(loss, lambda1*regularizer)\n",
    "        \n",
    "        bip_am_ygrad = tf.gradients(loss, [bip_am_syn])\n",
    "        bip_am_reggrad = tf.gradients(regularizer, [bip_am_syn])\n",
    "        \n",
    "        am_gc_ygrad = tf.gradients(loss, [am_gc_syn])\n",
    "        am_gc_reggrad = tf.gradients(regularizer, [am_gc_syn])\n",
    "        \n",
    "        bip_gc_ygrad = tf.gradients(loss, [bip_gc_syn])\n",
    "        bip_gc_reggrad = tf.gradients(regularizer, [bip_gc_syn])\n",
    "        \n",
    "\n",
    "\n",
    "        algorithm_choice=2\n",
    "        lr_min = 1e-4\n",
    "        lr_max = 1e-5\n",
    "        max_step =500\n",
    "        lr_ = tf.placeholder(\"float32\", name=\"learn_rate\")\n",
    "        \n",
    "        if algorithm_choice==1:\n",
    "            train_step = tf.train.GradientDescentOptimizer(lr_).minimize(objective)\n",
    "        elif algorithm_choice==2:\n",
    "            my_epsilon=1e-8\n",
    "            train_step = tf.train.AdamOptimizer(learning_rate=lr_, epsilon=my_epsilon).minimize(objective)\n",
    "        elif algorithm_choice==3:\n",
    "            momentum_par=0.9\n",
    "            train_step = tf.train.MomentumOptimizer(lr_, momentum_par).minimize(objective)\n",
    "        elif algorithm_choice==4:\n",
    "            train_step = tf.train.AdagradOptimizer(lr_).minimize(objective)\n",
    "        elif algorithm_choice==5:\n",
    "            train_step = tf.train.RMSPropOptimizer(lr_).minimize(objective)\n",
    "            \n",
    "\n",
    "        sess.run(tf.global_variables_initializer())    \n",
    "\n",
    "        bip_gc_syn_hist=tf.reshape(bip_gc_syn.eval(session=sess), [1, no_bipolars, no_bipolars, no_kernels]) \n",
    "        bip_am_syn_hist=tf.reshape(bip_am_syn.eval(session=sess), [1, no_bipolars, no_bipolars, no_kernels, no_amacrines, no_amacrines]) \n",
    "        am_gc_syn_hist=tf.reshape(am_gc_syn.eval(session=sess), [1, no_amacrines, no_amacrines]) \n",
    "        train_loss_hist = ones([1])\n",
    "        test_loss_hist = ones([1])\n",
    "        \n",
    "        bip_am_ygrad_hist=np.reshape(sess.run(bip_am_ygrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_bipolars, no_bipolars, no_kernels, no_amacrines, no_amacrines]) \n",
    "        bip_am_reggrad_hist=np.reshape(sess.run(bip_am_reggrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_bipolars, no_bipolars, no_kernels, no_amacrines, no_amacrines]) \n",
    "        am_gc_ygrad_hist=np.reshape(sess.run(am_gc_ygrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_amacrines, no_amacrines])\n",
    "        am_gc_reggrad_hist=np.reshape(sess.run(am_gc_reggrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_amacrines, no_amacrines])\n",
    "        bip_gc_ygrad_hist=np.reshape(sess.run(bip_gc_ygrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_bipolars, no_bipolars, no_kernels]) \n",
    "        bip_gc_reggrad_hist=np.reshape(sess.run(bip_gc_reggrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_bipolars, no_bipolars, no_kernels]) \n",
    "\n",
    "        train_fd={stimulus_: x_train[0:50, :, :, :]}\n",
    "        test_fd={stimulus_: x_test[0:50, :, :, :]}\n",
    "        train_output_hist=reshape(gc_output.eval(session=sess, feed_dict=train_fd), [1, 50])\n",
    "        test_output_hist=reshape(gc_output.eval(session=sess, feed_dict=test_fd), [1, 50])\n",
    "        \n",
    "        \n",
    "        check=1.0\n",
    "        step=0\n",
    "        end_flag=0\n",
    "\n",
    "        fd = {stimulus_:x_train[0:100, :, :, :], y_:y_train[0, 0:100]}\n",
    "        train_loss_val = sess.run(loss, feed_dict = fd)\n",
    "        print(train_loss_val)\n",
    "\n",
    "        fd = {stimulus_:x_test[0:100, :, :, :], y_:y_test[0, 0:100]}\n",
    "        test_loss_val = sess.run(loss, feed_dict = fd)\n",
    "        print(test_loss_val)\n",
    "\n",
    "\n",
    "\n",
    "        train_loss_hist=train_loss_val*train_loss_hist\n",
    "        test_loss_hist=test_loss_val*test_loss_hist\n",
    "\n",
    "        \n",
    "        endflag=0\n",
    "        step=0\n",
    "        while endflag == 0:\n",
    "            #learning rate schedule\n",
    "            learn_rate_sch = lr_min + 0.5*(lr_max - lr_min)*(1.0+np.cos(np.pi*(step%max_step/max_step))) \n",
    "            if step>=10*max_step:\n",
    "                learn_rate_sch = lr_min\n",
    "\n",
    "            inds = np.reshape(np.random.permutation(range(no_train)), [-1, batchsize])\n",
    "            for n in range(len(inds)): \n",
    "                fdd = {stimulus_: x_train[inds[n, :], :, :, :], y_: y_train[0, inds[n, :]], lr_: learn_rate_sch} \n",
    "                                                                 \n",
    "                sess.run(train_step, feed_dict=fdd)\n",
    "\n",
    "                        \n",
    "            if (step % 100 ==0):\n",
    "\n",
    "                train_loss_val = sess.run(loss, feed_dict= {stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]})/100.0\n",
    "                test_loss_val = sess.run(loss, feed_dict= {stimulus_: x_test[0:100, :, :, :], y_: y_test[0, 0:100]})/100.0\n",
    "                print(\"step: %d  loss: = %9f\" % (step, train_loss_val))\n",
    "\n",
    "                bip_gc_syn_hist=tf.concat( [bip_gc_syn_hist, tf.reshape(bip_gc_syn.eval(session=sess), [1, no_bipolars, no_bipolars, no_kernels])], 0,  name='bip_gc_syn_concat')\n",
    "                bip_am_syn_hist=tf.concat( [bip_am_syn_hist, tf.reshape(bip_am_syn.eval(session=sess), [1, no_bipolars, no_bipolars, no_kernels, no_amacrines, no_amacrines])], 0,  name='bip_am_syn_concat')\n",
    "                am_gc_syn_hist=tf.concat( [am_gc_syn_hist, tf.reshape(am_gc_syn.eval(session=sess), [1, no_amacrines, no_amacrines])], 0,  name='am_gc_syn_concat')\n",
    "\n",
    "                bip_am_ygrad_hist=tf.concat( [bip_am_ygrad_hist, np.reshape(sess.run(bip_am_ygrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_bipolars, no_bipolars, no_kernels, no_amacrines, no_amacrines])], 0)\n",
    "                bip_am_reggrad_hist=tf.concat( [bip_am_reggrad_hist, np.reshape(sess.run(bip_am_reggrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_bipolars, no_bipolars, no_kernels, no_amacrines, no_amacrines])], 0)\n",
    "                am_gc_ygrad_hist=tf.concat( [am_gc_ygrad_hist, np.reshape(sess.run(am_gc_ygrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_amacrines, no_amacrines])], 0)\n",
    "                am_gc_reggrad_hist=tf.concat( [am_gc_reggrad_hist, np.reshape(sess.run(am_gc_reggrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_amacrines, no_amacrines])], 0)\n",
    "                bip_gc_ygrad_hist=tf.concat( [bip_gc_ygrad_hist, np.reshape(sess.run(bip_gc_ygrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_bipolars, no_bipolars, no_kernels])], 0)\n",
    "                bip_gc_reggrad_hist=tf.concat( [bip_gc_reggrad_hist, np.reshape(sess.run(bip_gc_reggrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_bipolars, no_bipolars, no_kernels])], 0)\n",
    "\n",
    "                train_loss_hist=np.concatenate([train_loss_hist, np.array([train_loss_val])], axis=0)\n",
    "                test_loss_hist=np.concatenate([test_loss_hist, np.array([test_loss_val])], axis=0)\n",
    "                \n",
    "                train_fd={stimulus_: x_train[0:50, :, :, :]}\n",
    "                test_fd={stimulus_: x_test[0:50, :, :, :]}\n",
    "                train_output=reshape(gc_output.eval(session=sess, feed_dict=train_fd), [1, 50])\n",
    "                test_output=reshape(gc_output.eval(session=sess, feed_dict=test_fd), [1, 50])\n",
    "                \n",
    "                train_output_hist=np.concatenate([train_output_hist, train_output], axis=0)\n",
    "                test_output_hist=np.concatenate([test_output_hist, test_output], axis=0)\n",
    "                \n",
    "                #stopping condition\n",
    "                if (step/100)>=5:\n",
    "                    b=np.diff(train_loss_hist[int(step/100-5):int(step/100)])\n",
    "                    a=abs(b)<1.0\n",
    "                    c=b>0.0\n",
    "                    if sum(c)>=3:\n",
    "                        endflag=1\n",
    "            step = step + 1\n",
    "\n",
    "        db = {}\n",
    "\n",
    "        db['bipolar_bias'] = bipolar_bias.eval(session=sess)\n",
    "        db['bip_gc_syn_hist'] = bip_gc_syn_hist.eval(session=sess)\n",
    "        db['bip_am_syn_hist'] = bip_am_syn_hist.eval(session=sess)\n",
    "        db['am_gc_syn_hist'] = am_gc_syn_hist.eval(session=sess)\n",
    "        db['gc_bias'] = gc_bias.eval(session=sess)\n",
    "        \n",
    "        db['bip_am_ygrad_hist'] = bip_am_ygrad_hist.eval(session=sess)\n",
    "        db['bip_am_reggrad_hist'] = bip_am_reggrad_hist.eval(session=sess)\n",
    "        db['am_gc_ygrad_hist'] = am_gc_ygrad_hist.eval(session=sess)\n",
    "        db['am_gc_reggrad_hist'] = am_gc_reggrad_hist.eval(session=sess)\n",
    "        db['bip_gc_ygrad_hist'] = bip_gc_ygrad_hist.eval(session=sess)\n",
    "        db['bip_gc_reggrad_hist'] = bip_gc_reggrad_hist.eval(session=sess)\n",
    "\n",
    "\n",
    "\n",
    "        db['no_train']=no_train\n",
    "        db['no_test']=no_test\n",
    "\n",
    "        db['no_kernels'] = no_kernels\n",
    "        db['no_bipolars']=no_bipolars\n",
    "\n",
    "        db['bipkernels'] = bipkernels\n",
    "        db['randomseed'] = sd\n",
    "\n",
    "        db['train_output_hist'] = train_output_hist\n",
    "        db['test_output_hist'] = test_output_hist\n",
    "\n",
    "        db['algorithm_choice'] = algorithm_choice\n",
    "        db['learn_rate'] = learn_rate\n",
    "        db['lambda'] = lambda1\n",
    "\n",
    "        db['train_loss_hist'] = train_loss_hist\n",
    "        db['test_loss_hist'] = test_loss_hist                          \n",
    "\n",
    "        struct_proj = np.zeros([len(train_loss_hist), 1])\n",
    "\n",
    "        syn_hist = bip_gc_syn_hist.eval(session=sess)\n",
    "        basyn_hist = bip_am_syn_hist.eval(session=sess)\n",
    "        agsyn_hist = am_gc_syn_hist.eval(session=sess)\n",
    "        \n",
    "        truesyn = np.zeros([10, 10, no_kernels])\n",
    "        \n",
    "        truebasyn = np.zeros([no_bipolars, no_bipolars, no_kernels, no_amacrines, no_amacrines])\n",
    "        truebasyn[:, :, 0:3, :, :]=bip_am_syn_init\n",
    "        \n",
    "        trueagsyn=am_gc_syn_init\n",
    "        \n",
    "        norm_factor = (np.sum(np.square(truebasyn)) + np.sum(np.square(trueagsyn)))\n",
    "        for i in range(len(train_loss_hist)):\n",
    "            norm_factor = (np.sum(np.square(basyn_hist[i, :, :, :, :, :])) + np.sum(np.square(agsyn_hist[i, :, :])))\n",
    "            struct_proj[i] = (np.sum(np.multiply((basyn_hist[i, :, :, :, :, :]), truebasyn))+np.sum(np.multiply((agsyn_hist[i, :, :]), trueagsyn)))/norm_factor\n",
    "\n",
    "        db['struct_proj'] = struct_proj\n",
    "\n",
    "        sio.savemat(wheretosave, db)\n",
    "        \n",
    "        print(\"completed data: %d  kernels: = %9f\" % (no_train, no_kernels))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -1.4228064    0.6416717   -7.954151    -7.1236005   -8.948158\n",
      "  -3.4573846   -1.6151704    0.8720955   -5.1968794   -5.343743\n",
      "  -4.679883    -5.180412    -6.1150255   -0.5158843   -2.3922982\n",
      "  -6.2199106   -0.44152528  -2.5907264   -4.4703307   -5.9195175\n",
      "   1.0567964   -7.220103    -0.75077546   0.13780445  -3.358344\n",
      "   0.52748483  -1.9693136  -13.426476    -2.278798    -5.4695296\n",
      "  -4.5002356   -6.812718    -2.0966468   -8.701207     1.0634787\n",
      "  -8.707906     5.1430945   -1.2696398  -12.205326    -3.801906\n",
      "   0.29879776  -0.28056496  -1.2217234   -5.2528863    1.0899161\n",
      "  -9.754969    -6.4723225   -1.5638349    3.5947518    0.46535856\n",
      "  -0.35739198   0.07319409  -2.579593    -5.58231     -6.6857977\n",
      "  -1.7683878   -2.1197047   -6.2449265   -5.120393    -7.956409\n",
      "  -5.783134    -4.958048    -1.6125269    1.1362497   -5.9084387\n",
      "  -2.8033814   -1.1789311    1.4857316   -4.264687    -4.833137\n",
      "  -8.886445    -3.803897    -5.945573    -4.5016975   -0.21387231\n",
      "  -4.860885    -1.700955    -3.135984    -7.847861    -5.249995\n",
      "   1.5730579   -4.060959     1.0790215   -3.6008167    1.1754119\n",
      "  -2.9484162   -4.039862    -7.2570324    1.7819848   -3.9551134\n",
      "  -4.2186275   -1.4278649   -5.0105314   -5.4567986   -2.1144204\n",
      "  -3.0883818   -1.9037377   -4.134474     2.4039447   -7.087973  ]\n",
      "[[[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "fd=feed_dict= {stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}\n",
    "train_output=gc_output.eval(session=sess, feed_dict=fd)\n",
    "print(train_output)\n",
    "\n",
    "fd=feed_dict= {stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}\n",
    "train_output=amacrine_cell_layer.eval(session=sess, feed_dict=fd)\n",
    "print(train_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p27)",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
