{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from __future__ import division\n",
    "import random\n",
    "import scipy\n",
    "import h5py\n",
    "import hdf5storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random seed will dictate the random initialization\n",
    "sd=30000\n",
    "np.random.seed(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxker=28 #Maximum number of convolutional kernels to use in layer 1\n",
    "\n",
    "\n",
    "traindatapath='/home/ubuntu/Notebooks/Circuit2_Training_Data.h5'\n",
    "data = hdf5storage.loadmat(traindatapath)\n",
    "\n",
    "\n",
    "x_train = data['x_train']\n",
    "x_test = data['x_test']\n",
    "y_train = data['y_train']\n",
    "y_test = reshape(data['y_test'], [1, 2000])\n",
    "\n",
    "gc_bias_init = data['gc_bias']\n",
    "bipkernels = data['bipkernels']\n",
    "bip_gc_syn_init = data['bip_gc_syn']\n",
    "bip_am_syn_init = data['bip_am_syn']\n",
    "am_gc_syn_init = data['am_gc_syn']\n",
    "\n",
    "#sparsity params for weight matrix initializations\n",
    "init_sparsity = 0.0 \n",
    "init_sparsity_bg = 0.01\n",
    "\n",
    "\n",
    "bip_gc_syn_mask1 = np.random.rand(maxker*100, 1)\n",
    "bip_gc_syn_mask1 = reshape(bip_gc_syn_mask1, [1, 10, 10, maxker])\n",
    "bip_gc_syn_mask1 = bip_gc_syn_mask1 >(1.0 - init_sparsity_bg)\n",
    "\n",
    "\n",
    "bip_gc_syn_init_full = np.zeros([1, 10, 10, maxker])\n",
    "bip_gc_syn_mask_true = reshape(bip_gc_syn_init, [1, 10, 10, 3])>0.0\n",
    "bip_gc_syn_init_full[:, :, :, 0:3] = bip_gc_syn_mask_true\n",
    "\n",
    "bip_gc_syn_mask = np.maximum(bip_gc_syn_mask1, bip_gc_syn_init_full)\n",
    "\n",
    "bip_gc_syn_init11 = tf.random_uniform([1, 10, 10, maxker], minval=0.1, maxval=0.2, dtype=tf.float32)\n",
    "# bip_gc_syn_init1 = tf.multiply(bip_gc_syn_init11, bip_gc_syn_mask)\n",
    "bip_gc_syn_init1=bip_gc_syn_init11\n",
    "\n",
    "bip_am_syn_mask = np.zeros([10, 10, maxker, 5, 5])\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        for k in range(maxker):\n",
    "            bip_am_syn_mask[i, j, k, int(floor(i/2)), int(floor(j/2))] = 1.0\n",
    "bip_am_syn_mask = bip_am_syn_mask.astype(float32)\n",
    "\n",
    "bip_am_syn_inds = np.zeros([maxker*100, 6])\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        for k in range(maxker):\n",
    "            bip_am_syn_inds[maxker*10*(i)+28*(j)+k]=[0, i, j, k, floor(i/2), floor(j/2)]\n",
    "bip_am_syn_inds = bip_am_syn_inds.astype(int64)\n",
    "\n",
    "\n",
    "bip_am_syn_init11 = abs(np.random.normal(0.0, (sqrt(2.0)/112.0), size=[maxker*100]))\n",
    "bip_am_syn_init11=bip_am_syn_init11.astype(float32)        \n",
    "\n",
    "am_gc_syn_init1 = tf.random_uniform([1, 5, 5], minval=0.1, maxval=0.2, dtype=tf.float32)\n",
    " \n",
    "\n",
    "print(shape(x_train))\n",
    "print(shape(x_test))\n",
    "print(shape(bip_gc_syn_init))\n",
    "\n",
    "sum(bip_am_syn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shape(y_train))\n",
    "print(shape(y_test))\n",
    "plt.figure()\n",
    "plt.plot(squeeze(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_var(shape, initial_val):\n",
    "    initial = tf.constant(initial_val, shape=shape)\n",
    "    return tf.constant(initial_val) #initial\n",
    "\n",
    "def bip_conv2d(x, W):\n",
    "    padsize=10 \n",
    "    paddedx=tf.pad(x, [[0, 0], [padsize, padsize], [padsize, padsize], [0, 0]], 'CONSTANT')\n",
    "    outconv=tf.nn.conv2d(paddedx, W, strides=[1, 10, 10, 1], padding='SAME') #250 for movingdot and noise\n",
    "    return outconv[:, 1:11, 1:11, :]\n",
    "\n",
    "def synapse_var(shape, initial_val):\n",
    "#      initial=tf.constant(initial_val, shape=shape)\n",
    "#     initial = tf.random_uniform(shape, minval=0.1, maxval=0.8, dtype=tf.float32)\n",
    "    return tf.Variable(initial_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create layer 1 convolutional kernels (difference of gaussians)\n",
    "\n",
    "def difference_of_gaussians(ctr_sigma, surr_sigma, ctr_strength, surr_strength, x, y):\n",
    "    \n",
    "    center=0.4*(1/ctr_sigma)*exp(-0.5*square(sqrt(square(x)+square(y))/ctr_sigma))\n",
    "    \n",
    "    surround=0.4*(1/surr_sigma)*exp(-0.5*square(sqrt(square(x)+square(y))/surr_sigma))\n",
    "    \n",
    "    kernel = ctr_strength*center - surr_strength*surround\n",
    "    \n",
    "    maxk = amax(abs(kernel)) #normalization factor\n",
    "    \n",
    "    return kernel/maxk\n",
    "\n",
    "\n",
    "x = np.linspace(-5, 5, 11)\n",
    "y = np.linspace(-5, 5, 11)\n",
    "xv, yv = np.meshgrid(x, y)\n",
    "\n",
    "bipkernels = np.zeros([11, 11, 1, maxker])\n",
    "\n",
    "\n",
    "\n",
    "kernel1 = difference_of_gaussians(3, 6, 13, 12.9, xv, yv) \n",
    "kernel2 = difference_of_gaussians(5, 6, 18, 18, xv, yv)\n",
    "kernel3 = difference_of_gaussians(2, 4, 20, 14, xv, yv)\n",
    "kernel4 = difference_of_gaussians(3, 6, 13, 0, xv, yv)\n",
    "kernel5 = difference_of_gaussians(4, 6, 13, 0, xv, yv) \n",
    "kernel6 = difference_of_gaussians(2, 4, 20, 0, xv, yv)\n",
    "\n",
    "kernel7 = difference_of_gaussians(3, 6, 13, 20, xv, yv)\n",
    "kernel8 = difference_of_gaussians(5, 6, 18, 20, xv, yv) \n",
    "kernel9 = difference_of_gaussians(2, 4, 20, 24, xv, yv)\n",
    "\n",
    "kernel10 = difference_of_gaussians(5, 8, 13, 20, xv, yv)\n",
    "kernel11 = difference_of_gaussians(2, 8, 15, 15, xv, yv)\n",
    "kernel12 = difference_of_gaussians(3, 8, 20, 12, xv, yv)\n",
    "kernel13 = difference_of_gaussians(5, 8, 20, 18, xv, yv)\n",
    "kernel14 = difference_of_gaussians(2, 8, 13, 18, xv, yv)\n",
    "\n",
    "\n",
    "bipkernels[:, :, 0, 0]=kernel1\n",
    "bipkernels[:, :, 0, 1]=kernel2\n",
    "bipkernels[:, :, 0, 2]=kernel3\n",
    "bipkernels[:, :, 0, 3]=kernel4\n",
    "bipkernels[:, :, 0, 4]=-1.0*kernel1\n",
    "bipkernels[:, :, 0, 5]=-1.0*kernel2\n",
    "bipkernels[:, :, 0, 6]=-1.0*kernel3\n",
    "bipkernels[:, :, 0, 7]=-1.0*kernel4\n",
    "bipkernels[:, :, 0, 8]=kernel5\n",
    "bipkernels[:, :, 0, 9]=kernel6\n",
    "bipkernels[:, :, 0, 10]=kernel7\n",
    "bipkernels[:, :, 0, 11]=kernel8\n",
    "bipkernels[:, :, 0, 12]=-1.0*kernel5\n",
    "bipkernels[:, :, 0, 13]=-1.0*kernel6\n",
    "bipkernels[:, :, 0, 14]=-1.0*kernel7\n",
    "bipkernels[:, :, 0, 15]=-1.0*kernel8\n",
    "bipkernels[:, :, 0, 16]=kernel9\n",
    "bipkernels[:, :, 0, 17]=kernel10\n",
    "bipkernels[:, :, 0, 18]=kernel11\n",
    "bipkernels[:, :, 0, 19]=kernel12\n",
    "bipkernels[:, :, 0, 20]=-1.0*kernel9\n",
    "bipkernels[:, :, 0, 21]=-1.0*kernel10\n",
    "bipkernels[:, :, 0, 22]=-1.0*kernel11\n",
    "bipkernels[:, :, 0, 23]=-1.0*kernel12\n",
    "bipkernels[:, :, 0, 24]=kernel13\n",
    "bipkernels[:, :, 0, 25]=kernel14\n",
    "bipkernels[:, :, 0, 26]=-1.0*kernel13\n",
    "bipkernels[:, :, 0, 27]=-1.0*kernel14\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(4, 4, 1)\n",
    "plt.imshow(kernel1, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 2)\n",
    "plt.imshow(kernel2, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 3)\n",
    "plt.imshow(kernel3, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 4)\n",
    "plt.imshow(kernel4, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 5)\n",
    "plt.imshow(kernel5, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 6)\n",
    "plt.imshow(kernel6, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 7)\n",
    "plt.imshow(kernel7, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 8)\n",
    "plt.imshow(kernel8, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 9)\n",
    "plt.imshow(kernel9, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 10)\n",
    "plt.imshow(kernel10, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 11)\n",
    "plt.imshow(kernel11, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 12)\n",
    "plt.imshow(kernel12, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 13)\n",
    "plt.imshow(kernel13, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 14)\n",
    "plt.imshow(kernel14, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kernels = [3, 8, 16, 28]\n",
    "lambdas = [1e1, 1e1, 1e1, 1e1] # knowledge 0.2\n",
    "\n",
    "datas = [60, 340, 700, 1400, 2800, 5500, 11000, 22000, 98000]\n",
    "training_epochs = [6000, 4000, 4000, 3000, 3000, 2000, 2000, 1000, 500]\n",
    "test_sizes = [2080, 2080, 2080, 2080, 2080, 2080, 2080, 2080, 2080, 2000]\n",
    "\n",
    "learn_rate = 1e-3\n",
    "learn_rate_late = 1e-4\n",
    "\n",
    "\n",
    "for i_data in range(7):\n",
    "    \n",
    "    for i_kernel in range(4): \n",
    "        \n",
    "        if i_kernel > 0: \n",
    "            del stimulus_\n",
    "            del bipolar_cell_layer\n",
    "            del gc_activation\n",
    "            del gc_output\n",
    "            del bipolar_bias\n",
    "            del bipkernels1\n",
    "        \n",
    "        no_train=datas[i_data]\n",
    "        epochs = training_epochs[i_data]\n",
    "        no_kernels = kernels[i_kernel]\n",
    "        lambda1 = lambdas[i_kernel]\n",
    "        bipkernels1 = bipkernels[:, :, :, 0:no_kernels]\n",
    "        bip_gc_syn_init = bip_gc_syn_init1[:, :, :, 0:no_kernels]\n",
    "        bip_am_syn_mask1 = bip_am_syn_mask[ :, :, 0:no_kernels, :, :]\n",
    "        \n",
    "        \n",
    "        no_test=test_sizes[i_data] \n",
    "        no_bipolars = 10\n",
    "        no_amacrines = 5\n",
    "\n",
    "        wheretosave = '/home/ubuntu/Notebooks/Circuit2_Trained_Network_data' + str(no_train) + '_kernel' + str(no_kernels) \\\n",
    "        + '_sd' + str(sd) + '.mat'\n",
    "\n",
    "        ## initialize all variables\n",
    "        bip_bias_init_all = -1.0*np.ones([28])\n",
    "        bip_bias_init_all[0]=-2.0\n",
    "        bip_bias_init_all[1]=-3.0\n",
    "        bip_bias_init_all[3]=-15.0\n",
    "        bip_bias_init_all[8]=-25.0\n",
    "        bip_bias_init_all[9]=-10.0\n",
    "        \n",
    "        bip_bias_init_all[4]=-2.0\n",
    "        bip_bias_init_all[5]=-3.0\n",
    "        bip_bias_init_all[7]=-15.0\n",
    "        bip_bias_init_all[12]=-25.0\n",
    "        bip_bias_init_all[13]=-10.0\n",
    "\n",
    "        \n",
    "        bip_bias_init = bip_bias_init_all[0:no_kernels]\n",
    "        bip_bias_init = bip_bias_init.astype(float32)\n",
    "        bipolar_bias = bias_var([no_kernels], bip_bias_init)\n",
    "        \n",
    "        am_bias_init = -5.0 \n",
    "        am_bias = bias_var([1], am_bias_init)\n",
    "        \n",
    "        gc_bias = bias_var([1], gc_bias_init)\n",
    "        \n",
    "        bip_gc_syn_init=tf.random.normal([1, no_bipolars, no_bipolars, no_kernels], mean = 0.0, stddev = sqrt(2.0/(no_kernels*100)), dtype=tf.dtypes.float32, seed=sd)\n",
    "        bip_gc_syn = synapse_var([1, no_bipolars, no_bipolars, no_kernels], bip_gc_syn_init)\n",
    "        \n",
    "        bip_am_syn_inds = np.zeros([no_kernels*100, 6])\n",
    "        for i in range(10):\n",
    "            for j in range(10):\n",
    "                for k in range(no_kernels):\n",
    "                    bip_am_syn_inds[no_kernels*10*(i)+no_kernels*(j)+k]=[0, i, j, k, floor(i/2), floor(j/2)]\n",
    "        bip_am_syn_inds = bip_am_syn_inds.astype(int64)\n",
    "        bip_am_syn_init11 = abs(np.random.normal(0.0, (sqrt(2.0/no_kernels)), size=[no_kernels*100]))\n",
    "        bip_am_syn_init111=bip_am_syn_init11.astype(float32)  \n",
    "        bip_am_syn_val = synapse_var([no_kernels*no_bipolars*no_bipolars], bip_am_syn_init111)\n",
    "        bip_am_syn1 = tf.sparse.SparseTensor(indices=bip_am_syn_inds, values=bip_am_syn_val, dense_shape=[1, no_bipolars, no_bipolars, no_kernels, no_amacrines, no_amacrines])\n",
    "        bip_am_syn = tf.sparse.to_dense(tf.sparse.reorder(bip_am_syn1))        \n",
    "        \n",
    "        am_gc_syn = synapse_var([1, no_amacrines, no_amacrines], am_gc_syn_init1)\n",
    "\n",
    "        stimulus_ = tf.placeholder(\"float32\", name=\"stim_placeholder\")\n",
    "\n",
    "        bipolar_cell_layer = tf.nn.relu(tf.nn.bias_add(bip_conv2d(stimulus_, bipkernels1), bipolar_bias))\n",
    "\n",
    "        biplyr = tf.reshape(bipolar_cell_layer, [-1, no_bipolars*no_bipolars*no_kernels, 1])\n",
    "\n",
    "        tilebip_am_syn=tf.tile(tf.transpose(tf.reshape(tf.abs(bip_am_syn), [1, no_bipolars*no_bipolars*no_kernels, no_amacrines*no_amacrines]), [0, 2, 1]), [1, 1, 1])\n",
    "\n",
    "        amacrine_activation = 3.0*tf.reshape(tf.linalg.matmul(tilebip_am_syn, biplyr), [-1,no_amacrines, no_amacrines])\n",
    "     \n",
    "        amacrine_cell_layer = tf.nn.relu(tf.add(amacrine_activation, am_bias))\n",
    "\n",
    "        gc_activation = tf.multiply(tf.abs(bip_gc_syn), bipolar_cell_layer)\n",
    "\n",
    "        gc_activation_inhib = tf.multiply(tf.abs(am_gc_syn), amacrine_cell_layer)\n",
    "\n",
    "        gc_output = tf.add_n([tf.reduce_sum(gc_activation, [1, 2, 3]), -1.0*tf.reduce_sum(gc_activation_inhib, [1, 2])])\n",
    "\n",
    "        ## training procedure\n",
    "        y_ = tf.placeholder(\"float32\", name=\"output_spikes\")\n",
    "        \n",
    "        batchsize=20\n",
    "\n",
    "        loss = (tf.nn.l2_loss((tf.squeeze(gc_output) - tf.squeeze(y_)), name='loss'))\n",
    "\n",
    "        regularizer=tf.add_n([tf.reduce_sum(tf.abs(bip_am_syn)), tf.reduce_sum(tf.abs(bip_gc_syn)), \\\n",
    "                              0.0*tf.reduce_sum(tf.abs(am_gc_syn))])\n",
    "\n",
    "        objective=tf.add(loss, lambda1*regularizer)\n",
    "        \n",
    "        bip_am_ygrad = tf.gradients(loss, [bip_am_syn])\n",
    "        bip_am_reggrad = tf.gradients(regularizer, [bip_am_syn])\n",
    "        \n",
    "        am_gc_ygrad = tf.gradients(loss, [am_gc_syn])\n",
    "        am_gc_reggrad = tf.gradients(regularizer, [am_gc_syn])\n",
    "        \n",
    "        bip_gc_ygrad = tf.gradients(loss, [bip_gc_syn])\n",
    "        bip_gc_reggrad = tf.gradients(regularizer, [bip_gc_syn])\n",
    "        \n",
    "\n",
    "\n",
    "        algorithm_choice=2\n",
    "        lr_min = 1e-4\n",
    "        lr_max = 1e-5\n",
    "        max_step =500\n",
    "        lr_ = tf.placeholder(\"float32\", name=\"learn_rate\")\n",
    "        \n",
    "        if algorithm_choice==1:\n",
    "            train_step = tf.train.GradientDescentOptimizer(lr_).minimize(objective)\n",
    "        elif algorithm_choice==2:\n",
    "            my_epsilon=1e-8\n",
    "            train_step = tf.train.AdamOptimizer(learning_rate=lr_, epsilon=my_epsilon).minimize(objective)\n",
    "        elif algorithm_choice==3:\n",
    "            momentum_par=0.9\n",
    "            train_step = tf.train.MomentumOptimizer(lr_, momentum_par).minimize(objective)\n",
    "        elif algorithm_choice==4:\n",
    "            train_step = tf.train.AdagradOptimizer(lr_).minimize(objective)\n",
    "        elif algorithm_choice==5:\n",
    "            train_step = tf.train.RMSPropOptimizer(lr_).minimize(objective)\n",
    "            \n",
    "\n",
    "        sess.run(tf.global_variables_initializer())    \n",
    "\n",
    "        bip_gc_syn_hist=tf.reshape(bip_gc_syn.eval(session=sess), [1, no_bipolars, no_bipolars, no_kernels]) \n",
    "        bip_am_syn_hist=tf.reshape(bip_am_syn.eval(session=sess), [1, no_bipolars, no_bipolars, no_kernels, no_amacrines, no_amacrines]) \n",
    "        am_gc_syn_hist=tf.reshape(am_gc_syn.eval(session=sess), [1, no_amacrines, no_amacrines]) \n",
    "        train_loss_hist = ones([1])\n",
    "        test_loss_hist = ones([1])\n",
    "        \n",
    "        bip_am_ygrad_hist=np.reshape(sess.run(bip_am_ygrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_bipolars, no_bipolars, no_kernels, no_amacrines, no_amacrines]) \n",
    "        bip_am_reggrad_hist=np.reshape(sess.run(bip_am_reggrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_bipolars, no_bipolars, no_kernels, no_amacrines, no_amacrines]) \n",
    "        am_gc_ygrad_hist=np.reshape(sess.run(am_gc_ygrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_amacrines, no_amacrines])\n",
    "        am_gc_reggrad_hist=np.reshape(sess.run(am_gc_reggrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_amacrines, no_amacrines])\n",
    "        bip_gc_ygrad_hist=np.reshape(sess.run(bip_gc_ygrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_bipolars, no_bipolars, no_kernels]) \n",
    "        bip_gc_reggrad_hist=np.reshape(sess.run(bip_gc_reggrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_bipolars, no_bipolars, no_kernels]) \n",
    "\n",
    "        train_fd={stimulus_: x_train[0:50, :, :, :]}\n",
    "        test_fd={stimulus_: x_test[0:50, :, :, :]}\n",
    "        train_output_hist=reshape(gc_output.eval(session=sess, feed_dict=train_fd), [1, 50])\n",
    "        test_output_hist=reshape(gc_output.eval(session=sess, feed_dict=test_fd), [1, 50])\n",
    "        \n",
    "        \n",
    "        check=1.0\n",
    "        step=0\n",
    "        end_flag=0\n",
    "\n",
    "        fd = {stimulus_:x_train[0:100, :, :, :], y_:y_train[0, 0:100]}\n",
    "        train_loss_val = sess.run(loss, feed_dict = fd)\n",
    "        print(train_loss_val)\n",
    "\n",
    "        fd = {stimulus_:x_test[0:100, :, :, :], y_:y_test[0, 0:100]}\n",
    "        test_loss_val = sess.run(loss, feed_dict = fd)\n",
    "        print(test_loss_val)\n",
    "\n",
    "\n",
    "\n",
    "        train_loss_hist=train_loss_val*train_loss_hist\n",
    "        test_loss_hist=test_loss_val*test_loss_hist\n",
    "\n",
    "        \n",
    "        endflag=0\n",
    "        step=0\n",
    "        while endflag == 0:\n",
    "            #learning rate schedule\n",
    "            learn_rate_sch = lr_min + 0.5*(lr_max - lr_min)*(1.0+np.cos(np.pi*(step%max_step/max_step))) \n",
    "            if step>=10*max_step:\n",
    "                learn_rate_sch = lr_min\n",
    "\n",
    "            inds = np.reshape(np.random.permutation(range(no_train)), [-1, batchsize])\n",
    "            for n in range(len(inds)): \n",
    "                fdd = {stimulus_: x_train[inds[n, :], :, :, :], y_: y_train[0, inds[n, :]], lr_: learn_rate_sch} \n",
    "                                                                 \n",
    "                sess.run(train_step, feed_dict=fdd)\n",
    "\n",
    "                        \n",
    "            if (step % 100 ==0):\n",
    "\n",
    "                train_loss_val = sess.run(loss, feed_dict= {stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]})/100.0\n",
    "                test_loss_val = sess.run(loss, feed_dict= {stimulus_: x_test[0:100, :, :, :], y_: y_test[0, 0:100]})/100.0\n",
    "                print(\"step: %d  loss: = %9f\" % (step, train_loss_val))\n",
    "\n",
    "                bip_gc_syn_hist=tf.concat( [bip_gc_syn_hist, tf.reshape(bip_gc_syn.eval(session=sess), [1, no_bipolars, no_bipolars, no_kernels])], 0,  name='bip_gc_syn_concat')\n",
    "                bip_am_syn_hist=tf.concat( [bip_am_syn_hist, tf.reshape(bip_am_syn.eval(session=sess), [1, no_bipolars, no_bipolars, no_kernels, no_amacrines, no_amacrines])], 0,  name='bip_am_syn_concat')\n",
    "                am_gc_syn_hist=tf.concat( [am_gc_syn_hist, tf.reshape(am_gc_syn.eval(session=sess), [1, no_amacrines, no_amacrines])], 0,  name='am_gc_syn_concat')\n",
    "\n",
    "                bip_am_ygrad_hist=tf.concat( [bip_am_ygrad_hist, np.reshape(sess.run(bip_am_ygrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_bipolars, no_bipolars, no_kernels, no_amacrines, no_amacrines])], 0)\n",
    "                bip_am_reggrad_hist=tf.concat( [bip_am_reggrad_hist, np.reshape(sess.run(bip_am_reggrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_bipolars, no_bipolars, no_kernels, no_amacrines, no_amacrines])], 0)\n",
    "                am_gc_ygrad_hist=tf.concat( [am_gc_ygrad_hist, np.reshape(sess.run(am_gc_ygrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_amacrines, no_amacrines])], 0)\n",
    "                am_gc_reggrad_hist=tf.concat( [am_gc_reggrad_hist, np.reshape(sess.run(am_gc_reggrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_amacrines, no_amacrines])], 0)\n",
    "                bip_gc_ygrad_hist=tf.concat( [bip_gc_ygrad_hist, np.reshape(sess.run(bip_gc_ygrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_bipolars, no_bipolars, no_kernels])], 0)\n",
    "                bip_gc_reggrad_hist=tf.concat( [bip_gc_reggrad_hist, np.reshape(sess.run(bip_gc_reggrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_bipolars, no_bipolars, no_kernels])], 0)\n",
    "\n",
    "                train_loss_hist=np.concatenate([train_loss_hist, np.array([train_loss_val])], axis=0)\n",
    "                test_loss_hist=np.concatenate([test_loss_hist, np.array([test_loss_val])], axis=0)\n",
    "                \n",
    "                train_fd={stimulus_: x_train[0:50, :, :, :]}\n",
    "                test_fd={stimulus_: x_test[0:50, :, :, :]}\n",
    "                train_output=reshape(gc_output.eval(session=sess, feed_dict=train_fd), [1, 50])\n",
    "                test_output=reshape(gc_output.eval(session=sess, feed_dict=test_fd), [1, 50])\n",
    "                \n",
    "                train_output_hist=np.concatenate([train_output_hist, train_output], axis=0)\n",
    "                test_output_hist=np.concatenate([test_output_hist, test_output], axis=0)\n",
    "                \n",
    "                #stopping condition: stop when test loss begins to bounce up and down\n",
    "                if (step/100)>=5:\n",
    "                    b=np.diff(test_loss_hist[int(step/100-5):int(step/100)])\n",
    "                    a=abs(b)<1.0\n",
    "                    c=b>0.0\n",
    "                    if sum(c)>=3:\n",
    "                        endflag=1\n",
    "            step = step + 1\n",
    "\n",
    "\n",
    "        db = {}\n",
    "\n",
    "        db['bipolar_bias'] = bipolar_bias.eval(session=sess)\n",
    "        db['bip_gc_syn_hist'] = bip_gc_syn_hist.eval(session=sess)\n",
    "        db['bip_am_syn_hist'] = bip_am_syn_hist.eval(session=sess)\n",
    "        db['am_gc_syn_hist'] = am_gc_syn_hist.eval(session=sess)\n",
    "        db['gc_bias'] = gc_bias.eval(session=sess)\n",
    "        \n",
    "        db['bip_am_ygrad_hist'] = bip_am_ygrad_hist.eval(session=sess)\n",
    "        db['bip_am_reggrad_hist'] = bip_am_reggrad_hist.eval(session=sess)\n",
    "        db['am_gc_ygrad_hist'] = am_gc_ygrad_hist.eval(session=sess)\n",
    "        db['am_gc_reggrad_hist'] = am_gc_reggrad_hist.eval(session=sess)\n",
    "        db['bip_gc_ygrad_hist'] = bip_gc_ygrad_hist.eval(session=sess)\n",
    "        db['bip_gc_reggrad_hist'] = bip_gc_reggrad_hist.eval(session=sess)\n",
    "\n",
    "\n",
    "\n",
    "        db['no_train']=no_train\n",
    "        db['no_test']=no_test\n",
    "\n",
    "        db['no_kernels'] = no_kernels\n",
    "        db['no_bipolars']=no_bipolars\n",
    "\n",
    "        db['bipkernels'] = bipkernels\n",
    "        db['randomseed'] = sd\n",
    "\n",
    "        db['train_output_hist'] = train_output_hist\n",
    "        db['test_output_hist'] = test_output_hist\n",
    "\n",
    "        db['algorithm_choice'] = algorithm_choice\n",
    "        db['learn_rate'] = learn_rate\n",
    "        db['lambda'] = lambda1\n",
    "\n",
    "        db['train_loss_hist'] = train_loss_hist\n",
    "        db['test_loss_hist'] = test_loss_hist                          \n",
    "\n",
    "        struct_proj = np.zeros([len(train_loss_hist), 1])\n",
    "\n",
    "        syn_hist = bip_gc_syn_hist.eval(session=sess)\n",
    "        basyn_hist = abs(bip_am_syn_hist.eval(session=sess))\n",
    "        agsyn_hist = abs(am_gc_syn_hist.eval(session=sess))\n",
    "        \n",
    "        truesyn = np.zeros([10, 10, no_kernels])\n",
    "        \n",
    "        truebasyn = np.zeros([no_bipolars, no_bipolars, no_kernels, no_amacrines, no_amacrines])\n",
    "        truebasyn[:, :, 0:3, :, :]=bip_am_syn_init\n",
    "        \n",
    "        trueagsyn=am_gc_syn_init\n",
    "        \n",
    "        norm_factor = (np.sum(np.square(truebasyn)) + np.sum(np.square(trueagsyn)))\n",
    "        for i in range(len(train_loss_hist)):\n",
    "            norm_factor = (np.sum(np.square(basyn_hist[i, :, :, :, :, :])) + np.sum(np.square(agsyn_hist[i, :, :])))\n",
    "            struct_proj[i] = (np.sum(np.multiply((basyn_hist[i, :, :, :, :, :]), truebasyn))+np.sum(np.multiply((agsyn_hist[i, :, :]), trueagsyn)))/norm_factor\n",
    "\n",
    "        db['struct_proj'] = struct_proj\n",
    "\n",
    "        sio.savemat(wheretosave, db)\n",
    "        \n",
    "        print(\"completed data: %d  kernels: = %9f\" % (no_train, no_kernels))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd=feed_dict= {stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}\n",
    "train_output=gc_output.eval(session=sess, feed_dict=fd)\n",
    "print(train_output)\n",
    "\n",
    "fd=feed_dict= {stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}\n",
    "train_output=amacrine_cell_layer.eval(session=sess, feed_dict=fd)\n",
    "print(train_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p27)",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
