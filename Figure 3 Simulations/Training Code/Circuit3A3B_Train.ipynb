{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from __future__ import division\n",
    "import random\n",
    "import scipy\n",
    "import h5py\n",
    "import hdf5storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random seed will dictate the random initialization\n",
    "sd=30000\n",
    "np.random.seed(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98000, 100, 100, 1)\n",
      "(2000, 100, 100, 1)\n",
      "(1, 10, 10, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2800.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxker=28\n",
    "\n",
    "# Select the appropriate training data file\n",
    "\n",
    "traindatapath='/home/ubuntu/Notebooks/Circuit3A_Training_Data.h5'\n",
    "# traindatapath='/home/ubuntu/Notebooks/Circuit3B_Training_Data.h5' \n",
    "\n",
    "\n",
    "data = hdf5storage.loadmat(traindatapath)\n",
    "\n",
    "\n",
    "x_train = data['x_train']\n",
    "x_test = data['x_test']\n",
    "y_train = data['y_train']\n",
    "y_test = reshape(data['y_test'], [1, 2000])\n",
    "\n",
    "gc_bias_init = data['gc_bias']\n",
    "bipkernels = data['bipkernels']\n",
    "bip_gc_syn_init = data['bip_gc_syn']\n",
    "bip_am_syn_init = data['bip_am_syn']\n",
    "am_gc_syn_init = data['am_gc_syn']\n",
    "\n",
    "#sparsity params for weight matrix initializations\n",
    "init_sparsity = 0.0 \n",
    "init_sparsity_bg = 0.01\n",
    "\n",
    "\n",
    "bip_gc_syn_mask1 = np.random.rand(maxker*100, 1)\n",
    "bip_gc_syn_mask1 = reshape(bip_gc_syn_mask1, [1, 10, 10, maxker])\n",
    "bip_gc_syn_mask1 = bip_gc_syn_mask1 >(1.0 - init_sparsity_bg)\n",
    "\n",
    "\n",
    "bip_gc_syn_init_full = np.zeros([1, 10, 10, maxker])\n",
    "bip_gc_syn_mask_true = reshape(bip_gc_syn_init, [1, 10, 10, 3])>0.0\n",
    "bip_gc_syn_init_full[:, :, :, 0:3] = bip_gc_syn_mask_true\n",
    "\n",
    "bip_gc_syn_mask = np.maximum(bip_gc_syn_mask1, bip_gc_syn_init_full)\n",
    "\n",
    "bip_gc_syn_init11 = tf.random_uniform([1, 10, 10, maxker], minval=1.0, maxval=2.0, dtype=tf.float32)\n",
    "bip_gc_syn_init1=bip_gc_syn_init11\n",
    "\n",
    "bip_am_syn_mask = np.zeros([10, 10, maxker, 5, 5])\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        for k in range(maxker):\n",
    "            bip_am_syn_mask[i, j, k, int(floor(i/2)), int(floor(j/2))] = 1.0\n",
    "bip_am_syn_mask = bip_am_syn_mask.astype(float32)\n",
    "\n",
    "\n",
    "bip_am_syn_inds = np.zeros([maxker*100, 6])\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        for k in range(maxker):\n",
    "            bip_am_syn_inds[maxker*10*(i)+28*(j)+k]=[0, i, j, k, floor(i/2), floor(j/2)]\n",
    "bip_am_syn_inds = bip_am_syn_inds.astype(int64)\n",
    "\n",
    "\n",
    "bip_am_syn_init11 = abs(np.random.normal(0.0, (sqrt(2.0)/112.0), size=[maxker*100]))\n",
    "bip_am_syn_init11=bip_am_syn_init11.astype(float32)        \n",
    "\n",
    "am_gc_syn_init1 = tf.random_uniform([1, 5, 5], minval=0.1, maxval=0.2, dtype=tf.float32)\n",
    " \n",
    "\n",
    "print(shape(x_train))\n",
    "print(shape(x_test))\n",
    "print(shape(bip_gc_syn_init))\n",
    "\n",
    "sum(bip_am_syn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 98000)\n",
      "(1, 2000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6eb5cc2b90>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(shape(y_train))\n",
    "print(shape(y_test))\n",
    "plt.figure()\n",
    "plt.plot(squeeze(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_var(shape, initial_val):\n",
    "    initial = tf.constant(initial_val, shape=shape)\n",
    "    return tf.constant(initial_val) #initial\n",
    "\n",
    "def bip_conv2d(x, W):\n",
    "    padsize=10 \n",
    "    paddedx=tf.pad(x, [[0, 0], [padsize, padsize], [padsize, padsize], [0, 0]], 'CONSTANT')\n",
    "    outconv=tf.nn.conv2d(paddedx, W, strides=[1, 10, 10, 1], padding='SAME') #250 for movingdot and noise\n",
    "    return outconv[:, 1:11, 1:11, :]\n",
    "\n",
    "def synapse_var(shape, initial_val):\n",
    "#      initial=tf.constant(initial_val, shape=shape)\n",
    "#     initial = tf.random_uniform(shape, minval=0.1, maxval=0.8, dtype=tf.float32)\n",
    "    return tf.Variable(initial_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f6eb4c3fe90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 28 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## create layer 1 convolutional kernels (difference of gaussians)\n",
    "\n",
    "def difference_of_gaussians(ctr_sigma, surr_sigma, ctr_strength, surr_strength, x, y):\n",
    "    \n",
    "    center=0.4*(1/ctr_sigma)*exp(-0.5*square(sqrt(square(x)+square(y))/ctr_sigma))\n",
    "    \n",
    "    surround=0.4*(1/surr_sigma)*exp(-0.5*square(sqrt(square(x)+square(y))/surr_sigma))\n",
    "    \n",
    "    kernel = ctr_strength*center - surr_strength*surround\n",
    "    \n",
    "    maxk = amax(abs(kernel)) #normalization factor\n",
    "    \n",
    "    return kernel/maxk\n",
    "\n",
    "\n",
    "x = np.linspace(-5, 5, 11)\n",
    "y = np.linspace(-5, 5, 11)\n",
    "xv, yv = np.meshgrid(x, y)\n",
    "\n",
    "bipkernels = np.zeros([11, 11, 1, maxker])\n",
    "\n",
    "\n",
    "kernel1 = difference_of_gaussians(3, 6, 13, 12.9, xv, yv) \n",
    "kernel2 = difference_of_gaussians(5, 6, 18, 18, xv, yv)\n",
    "kernel3 = difference_of_gaussians(2, 4, 20, 14, xv, yv)\n",
    "kernel4 = difference_of_gaussians(3, 6, 13, 0, xv, yv)\n",
    "kernel5 = difference_of_gaussians(4, 6, 13, 0, xv, yv) \n",
    "kernel6 = difference_of_gaussians(2, 4, 20, 0, xv, yv)\n",
    "\n",
    "kernel7 = difference_of_gaussians(3, 6, 13, 20, xv, yv)\n",
    "kernel8 = difference_of_gaussians(5, 6, 18, 20, xv, yv) \n",
    "kernel9 = difference_of_gaussians(2, 4, 20, 24, xv, yv)\n",
    "\n",
    "kernel10 = difference_of_gaussians(5, 8, 13, 20, xv, yv)\n",
    "kernel11 = difference_of_gaussians(2, 8, 15, 15, xv, yv)\n",
    "kernel12 = difference_of_gaussians(3, 8, 20, 12, xv, yv)\n",
    "kernel13 = difference_of_gaussians(5, 8, 20, 18, xv, yv)\n",
    "kernel14 = difference_of_gaussians(2, 8, 13, 18, xv, yv)\n",
    "\n",
    "\n",
    "bipkernels[:, :, 0, 0]=kernel1\n",
    "bipkernels[:, :, 0, 1]=kernel2\n",
    "bipkernels[:, :, 0, 2]=kernel3\n",
    "bipkernels[:, :, 0, 3]=kernel4\n",
    "bipkernels[:, :, 0, 4]=-1.0*kernel1\n",
    "bipkernels[:, :, 0, 5]=-1.0*kernel2\n",
    "bipkernels[:, :, 0, 6]=-1.0*kernel3\n",
    "bipkernels[:, :, 0, 7]=-1.0*kernel4\n",
    "bipkernels[:, :, 0, 8]=kernel5\n",
    "bipkernels[:, :, 0, 9]=kernel6\n",
    "bipkernels[:, :, 0, 10]=kernel7\n",
    "bipkernels[:, :, 0, 11]=kernel8\n",
    "bipkernels[:, :, 0, 12]=-1.0*kernel5\n",
    "bipkernels[:, :, 0, 13]=-1.0*kernel6\n",
    "bipkernels[:, :, 0, 14]=-1.0*kernel7\n",
    "bipkernels[:, :, 0, 15]=-1.0*kernel8\n",
    "bipkernels[:, :, 0, 16]=kernel9\n",
    "bipkernels[:, :, 0, 17]=kernel10\n",
    "bipkernels[:, :, 0, 18]=kernel11\n",
    "bipkernels[:, :, 0, 19]=kernel12\n",
    "bipkernels[:, :, 0, 20]=-1.0*kernel9\n",
    "bipkernels[:, :, 0, 21]=-1.0*kernel10\n",
    "bipkernels[:, :, 0, 22]=-1.0*kernel11\n",
    "bipkernels[:, :, 0, 23]=-1.0*kernel12\n",
    "bipkernels[:, :, 0, 24]=kernel13\n",
    "bipkernels[:, :, 0, 25]=kernel14\n",
    "bipkernels[:, :, 0, 26]=-1.0*kernel13\n",
    "bipkernels[:, :, 0, 27]=-1.0*kernel14\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(4, 4, 1)\n",
    "plt.imshow(kernel1, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 2)\n",
    "plt.imshow(kernel2, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 3)\n",
    "plt.imshow(kernel3, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 4)\n",
    "plt.imshow(kernel4, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 5)\n",
    "plt.imshow(kernel5, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 6)\n",
    "plt.imshow(kernel6, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 7)\n",
    "plt.imshow(kernel7, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 8)\n",
    "plt.imshow(kernel8, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 9)\n",
    "plt.imshow(kernel9, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 10)\n",
    "plt.imshow(kernel10, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 11)\n",
    "plt.imshow(kernel11, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 12)\n",
    "plt.imshow(kernel12, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 13)\n",
    "plt.imshow(kernel13, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(4, 4, 14)\n",
    "plt.imshow(kernel14, cmap=plt.get_cmap('RdBu'))\n",
    "plt.clim(-1.0, 1.0)\n",
    "plt.colorbar()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40557.617\n",
      "44305.066\n",
      "step: 0  loss: = 405.045781\n",
      "step: 100  loss: = 341.375117\n",
      "step: 200  loss: = 225.964687\n",
      "step: 300  loss: = 92.580859\n",
      "step: 400  loss: = 20.957356\n",
      "step: 500  loss: =  6.311478\n",
      "step: 600  loss: =  5.652542\n",
      "step: 700  loss: =  4.491799\n",
      "step: 800  loss: =  3.279804\n",
      "step: 900  loss: =  2.476377\n",
      "step: 1000  loss: =  2.100222\n",
      "step: 1100  loss: =  2.066532\n",
      "step: 1200  loss: =  1.993726\n",
      "step: 1300  loss: =  1.882496\n",
      "step: 1400  loss: =  1.770811\n",
      "step: 1500  loss: =  1.688259\n",
      "step: 1600  loss: =  1.679344\n",
      "step: 1700  loss: =  1.659423\n",
      "step: 1800  loss: =  1.628029\n",
      "step: 1900  loss: =  1.597695\n",
      "step: 2000  loss: =  1.578922\n",
      "step: 2100  loss: =  1.577201\n",
      "step: 2200  loss: =  1.573769\n",
      "step: 2300  loss: =  1.569413\n",
      "step: 2400  loss: =  1.566800\n",
      "step: 2500  loss: =  1.567256\n",
      "step: 2600  loss: =  1.567518\n",
      "step: 2700  loss: =  1.568314\n",
      "step: 2800  loss: =  1.570559\n",
      "step: 2900  loss: =  1.575044\n",
      "completed data: 60  kernels: =  3.000000\n",
      "82247.01\n",
      "84920.08\n",
      "step: 0  loss: = 820.650156\n",
      "step: 100  loss: = 614.277734\n",
      "step: 200  loss: = 296.350762\n",
      "step: 300  loss: = 55.905640\n",
      "step: 400  loss: =  6.437810\n",
      "step: 500  loss: =  2.668441\n",
      "step: 600  loss: =  2.518148\n",
      "step: 700  loss: =  2.237546\n",
      "step: 800  loss: =  1.908713\n",
      "step: 900  loss: =  1.642956\n",
      "step: 1000  loss: =  1.490602\n",
      "step: 1100  loss: =  1.475691\n",
      "step: 1200  loss: =  1.441424\n",
      "step: 1300  loss: =  1.403054\n",
      "step: 1400  loss: =  1.386731\n",
      "step: 1500  loss: =  1.367204\n",
      "step: 1600  loss: =  1.362734\n",
      "step: 1700  loss: =  1.355737\n",
      "step: 1800  loss: =  1.363364\n",
      "step: 1900  loss: =  1.355041\n",
      "step: 2000  loss: =  1.338989\n",
      "step: 2100  loss: =  1.331890\n",
      "step: 2200  loss: =  1.316104\n",
      "step: 2300  loss: =  1.261223\n",
      "step: 2400  loss: =  1.105904\n",
      "step: 2500  loss: =  1.009011\n",
      "step: 2600  loss: =  1.000840\n",
      "step: 2700  loss: =  0.983223\n",
      "step: 2800  loss: =  0.954213\n",
      "step: 2900  loss: =  0.871801\n",
      "step: 3000  loss: =  0.850342\n",
      "step: 3100  loss: =  0.848787\n",
      "step: 3200  loss: =  0.852261\n",
      "step: 3300  loss: =  0.851790\n",
      "step: 3400  loss: =  0.843910\n",
      "step: 3500  loss: =  0.851787\n",
      "step: 3600  loss: =  0.850981\n",
      "step: 3700  loss: =  0.854127\n",
      "step: 3800  loss: =  0.861216\n",
      "step: 3900  loss: =  0.852165\n",
      "step: 4000  loss: =  0.857714\n",
      "completed data: 60  kernels: =  8.000000\n",
      "232782.83\n",
      "235258.73\n",
      "step: 0  loss: = 2319.735312\n",
      "step: 100  loss: = 1455.397031\n",
      "step: 200  loss: = 438.124687\n",
      "step: 300  loss: = 31.229932\n",
      "step: 400  loss: =  3.890795\n",
      "step: 500  loss: =  2.257875\n",
      "step: 600  loss: =  2.176740\n",
      "step: 700  loss: =  2.010607\n",
      "step: 800  loss: =  1.775314\n",
      "step: 900  loss: =  1.582811\n",
      "step: 1000  loss: =  1.462068\n",
      "step: 1100  loss: =  1.451607\n",
      "step: 1200  loss: =  1.431391\n",
      "step: 1300  loss: =  1.415269\n",
      "step: 1400  loss: =  1.426712\n",
      "step: 1500  loss: =  1.447965\n",
      "step: 1600  loss: =  1.451423\n",
      "step: 1700  loss: =  1.456353\n",
      "step: 1800  loss: =  1.471284\n",
      "completed data: 60  kernels: = 16.000000\n",
      "499131.0\n",
      "502019.2\n",
      "step: 0  loss: = 4961.501562\n",
      "step: 100  loss: = 2170.184531\n",
      "step: 200  loss: = 226.493691\n",
      "step: 300  loss: =  5.344391\n",
      "step: 400  loss: =  2.020067\n",
      "step: 500  loss: =  1.497401\n",
      "step: 600  loss: =  1.456786\n",
      "step: 700  loss: =  1.371248\n",
      "step: 800  loss: =  1.253708\n",
      "step: 900  loss: =  1.165186\n",
      "step: 1000  loss: =  1.136304\n",
      "step: 1100  loss: =  1.135225\n",
      "step: 1200  loss: =  1.132913\n",
      "step: 1300  loss: =  1.130123\n",
      "step: 1400  loss: =  1.132813\n",
      "step: 1500  loss: =  1.138310\n",
      "step: 1600  loss: =  1.137725\n",
      "step: 1700  loss: =  1.142215\n",
      "step: 1800  loss: =  1.041494\n",
      "step: 1900  loss: =  1.017753\n",
      "completed data: 60  kernels: = 28.000000\n",
      "38764.83\n",
      "43044.484\n",
      "step: 0  loss: = 384.720195\n",
      "step: 100  loss: = 124.895986\n",
      "step: 200  loss: =  6.786468\n",
      "step: 300  loss: =  2.185597\n",
      "step: 400  loss: =  1.691731\n",
      "step: 500  loss: =  1.430732\n",
      "step: 600  loss: =  1.404867\n",
      "step: 700  loss: =  1.283179\n",
      "step: 800  loss: =  1.214371\n",
      "step: 900  loss: =  1.196284\n",
      "step: 1000  loss: =  1.053443\n",
      "step: 1100  loss: =  1.053090\n",
      "step: 1200  loss: =  1.054636\n",
      "step: 1300  loss: =  1.052814\n",
      "step: 1400  loss: =  0.871221\n",
      "step: 1500  loss: =  0.844730\n",
      "step: 1600  loss: =  0.844526\n",
      "step: 1700  loss: =  0.481388\n",
      "step: 1800  loss: =  0.323431\n",
      "step: 1900  loss: =  0.305178\n",
      "step: 2000  loss: =  0.304238\n",
      "step: 2100  loss: =  0.304231\n",
      "step: 2200  loss: =  0.304241\n",
      "step: 2300  loss: =  0.305182\n",
      "step: 2400  loss: =  0.274762\n",
      "step: 2500  loss: =  0.274817\n",
      "step: 2600  loss: =  0.274798\n",
      "step: 2700  loss: =  0.274922\n",
      "completed data: 340  kernels: =  3.000000\n",
      "86859.36\n",
      "89633.414\n",
      "step: 0  loss: = 858.560469\n",
      "step: 100  loss: = 130.672695\n",
      "step: 200  loss: =  3.189520\n",
      "step: 300  loss: =  1.828326\n",
      "step: 400  loss: =  1.599834\n",
      "step: 500  loss: =  1.330923\n",
      "step: 600  loss: =  1.236458\n",
      "step: 700  loss: =  1.076594\n",
      "step: 800  loss: =  1.067373\n",
      "step: 900  loss: =  1.018845\n",
      "step: 1000  loss: =  0.971963\n",
      "step: 1100  loss: =  0.965661\n",
      "step: 1200  loss: =  0.956836\n",
      "step: 1300  loss: =  0.944572\n",
      "step: 1400  loss: =  0.935990\n",
      "step: 1500  loss: =  0.927461\n",
      "step: 1600  loss: =  0.923763\n",
      "step: 1700  loss: =  0.923184\n",
      "step: 1800  loss: =  0.921545\n",
      "step: 1900  loss: =  0.921476\n",
      "step: 2000  loss: =  0.919390\n",
      "step: 2100  loss: =  0.917801\n",
      "step: 2200  loss: =  0.918726\n",
      "step: 2300  loss: =  0.917532\n",
      "step: 2400  loss: =  0.919856\n",
      "step: 2500  loss: =  0.916693\n",
      "step: 2600  loss: =  0.917336\n",
      "step: 2700  loss: =  0.918257\n",
      "step: 2800  loss: =  0.918228\n",
      "step: 2900  loss: =  0.920762\n",
      "completed data: 340  kernels: =  8.000000\n",
      "205449.06\n",
      "209431.22\n",
      "step: 0  loss: = 2011.057500\n",
      "step: 100  loss: = 44.686782\n",
      "step: 200  loss: =  1.999722\n",
      "step: 300  loss: =  1.627353\n",
      "step: 400  loss: =  1.394673\n",
      "step: 500  loss: =  0.748944\n",
      "step: 600  loss: =  0.730609\n",
      "step: 700  loss: =  0.767269\n",
      "step: 800  loss: =  0.903100\n",
      "step: 900  loss: =  0.861327\n",
      "step: 1000  loss: =  0.803307\n",
      "step: 1100  loss: =  0.800881\n",
      "step: 1200  loss: =  0.797530\n",
      "step: 1300  loss: =  0.786078\n",
      "step: 1400  loss: =  0.777119\n",
      "step: 1500  loss: =  0.774359\n",
      "step: 1600  loss: =  0.771893\n",
      "step: 1700  loss: =  0.774114\n",
      "step: 1800  loss: =  0.774176\n",
      "step: 1900  loss: =  0.772584\n",
      "step: 2000  loss: =  0.759221\n",
      "step: 2100  loss: =  0.754972\n",
      "step: 2200  loss: =  0.755625\n",
      "step: 2300  loss: =  0.760060\n",
      "step: 2400  loss: =  0.751649\n",
      "step: 2500  loss: =  0.742134\n",
      "step: 2600  loss: =  0.736401\n",
      "step: 2700  loss: =  0.740108\n",
      "step: 2800  loss: =  0.745061\n",
      "step: 2900  loss: =  0.752911\n",
      "step: 3000  loss: =  0.744009\n",
      "step: 3100  loss: =  0.735211\n",
      "completed data: 340  kernels: = 16.000000\n",
      "530768.0\n",
      "537231.2\n",
      "step: 0  loss: = 5141.767500\n",
      "step: 100  loss: = 14.697585\n",
      "step: 200  loss: =  1.814357\n",
      "step: 300  loss: =  1.530671\n",
      "step: 400  loss: =  1.466275\n",
      "step: 500  loss: =  1.077268\n",
      "step: 600  loss: =  0.984745\n",
      "step: 700  loss: =  1.009299\n",
      "step: 800  loss: =  1.082143\n",
      "step: 900  loss: =  1.035720\n",
      "step: 1000  loss: =  0.984451\n",
      "step: 1100  loss: =  0.972197\n",
      "step: 1200  loss: =  0.967888\n",
      "step: 1300  loss: =  0.961968\n",
      "step: 1400  loss: =  0.963268\n",
      "step: 1500  loss: =  0.961422\n",
      "step: 1600  loss: =  0.941255\n",
      "step: 1700  loss: =  0.945843\n",
      "step: 1800  loss: =  0.961017\n",
      "step: 1900  loss: =  0.962674\n",
      "step: 2000  loss: =  0.953518\n",
      "step: 2100  loss: =  0.931527\n",
      "completed data: 340  kernels: = 28.000000\n",
      "38030.055\n",
      "41773.355\n",
      "step: 0  loss: = 374.440820\n",
      "step: 100  loss: = 21.867986\n",
      "step: 200  loss: =  1.912682\n",
      "step: 300  loss: =  1.465769\n",
      "step: 400  loss: =  1.353932\n",
      "step: 500  loss: =  1.341171\n",
      "step: 600  loss: =  1.340348\n",
      "step: 700  loss: =  1.217538\n",
      "step: 800  loss: =  1.060806\n",
      "step: 900  loss: =  0.868665\n",
      "step: 1000  loss: =  0.748133\n",
      "step: 1100  loss: =  0.739337\n",
      "step: 1200  loss: =  0.285966\n",
      "step: 1300  loss: =  0.161232\n",
      "step: 1400  loss: =  0.103933\n",
      "step: 1500  loss: =  0.072093\n",
      "step: 1600  loss: =  0.069841\n",
      "step: 1700  loss: =  0.067923\n",
      "step: 1800  loss: =  0.038686\n",
      "step: 1900  loss: =  0.030630\n",
      "step: 2000  loss: =  0.012281\n",
      "step: 2100  loss: =  0.010743\n",
      "step: 2200  loss: =  0.008905\n",
      "step: 2300  loss: =  0.007782\n",
      "step: 2400  loss: =  0.008929\n",
      "step: 2500  loss: =  0.006161\n",
      "step: 2600  loss: =  0.006109\n",
      "step: 2700  loss: =  0.005960\n",
      "step: 2800  loss: =  0.005573\n",
      "step: 2900  loss: =  0.004439\n",
      "step: 3000  loss: =  0.003118\n",
      "step: 3100  loss: =  0.003194\n",
      "step: 3200  loss: =  0.003024\n",
      "step: 3300  loss: =  0.002420\n",
      "step: 3400  loss: =  0.001768\n",
      "step: 3500  loss: =  0.001407\n",
      "step: 3600  loss: =  0.001485\n",
      "step: 3700  loss: =  0.001382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3800  loss: =  0.001212\n",
      "step: 3900  loss: =  0.001012\n",
      "step: 4000  loss: =  0.000791\n",
      "step: 4100  loss: =  0.000738\n",
      "step: 4200  loss: =  0.000673\n",
      "step: 4300  loss: =  0.000565\n",
      "step: 4400  loss: =  0.000501\n",
      "step: 4500  loss: =  0.000417\n",
      "step: 4600  loss: =  0.000435\n",
      "step: 4700  loss: =  0.000425\n",
      "step: 4800  loss: =  0.000424\n",
      "step: 4900  loss: =  0.000505\n",
      "step: 5000  loss: =  0.000419\n",
      "step: 5100  loss: =  0.000460\n",
      "step: 5200  loss: =  0.000421\n",
      "step: 5300  loss: =  0.000518\n",
      "step: 5400  loss: =  0.000419\n",
      "step: 5500  loss: =  0.000454\n",
      "step: 5600  loss: =  0.000465\n",
      "step: 5700  loss: =  0.000403\n",
      "step: 5800  loss: =  0.000438\n",
      "completed data: 700  kernels: =  3.000000\n",
      "85322.82\n",
      "87507.88\n",
      "step: 0  loss: = 832.931250\n",
      "step: 100  loss: =  7.600471\n",
      "step: 200  loss: =  1.841272\n",
      "step: 300  loss: =  1.373819\n",
      "step: 400  loss: =  1.074415\n",
      "step: 500  loss: =  1.029267\n",
      "step: 600  loss: =  1.018447\n",
      "step: 700  loss: =  1.008806\n",
      "step: 800  loss: =  1.010405\n",
      "step: 900  loss: =  1.012495\n",
      "step: 1000  loss: =  1.011934\n",
      "step: 1100  loss: =  1.002463\n",
      "step: 1200  loss: =  0.999749\n",
      "step: 1300  loss: =  1.004678\n",
      "step: 1400  loss: =  1.004504\n",
      "step: 1500  loss: =  1.013424\n",
      "step: 1600  loss: =  1.003454\n",
      "step: 1700  loss: =  0.999484\n",
      "step: 1800  loss: =  1.001395\n",
      "step: 1900  loss: =  1.005587\n",
      "step: 2000  loss: =  1.008656\n",
      "step: 2100  loss: =  1.003128\n",
      "step: 2200  loss: =  1.002290\n",
      "completed data: 700  kernels: =  8.000000\n",
      "213506.78\n",
      "216788.52\n",
      "step: 0  loss: = 2046.078906\n",
      "step: 100  loss: =  2.461822\n",
      "step: 200  loss: =  1.419418\n",
      "step: 300  loss: =  1.325221\n",
      "step: 400  loss: =  1.072065\n",
      "step: 500  loss: =  1.054356\n",
      "step: 600  loss: =  1.035300\n",
      "step: 700  loss: =  1.030592\n",
      "step: 800  loss: =  1.033849\n",
      "step: 900  loss: =  1.031684\n",
      "step: 1000  loss: =  1.043545\n",
      "step: 1100  loss: =  1.019567\n",
      "step: 1200  loss: =  1.021225\n",
      "step: 1300  loss: =  1.020640\n",
      "step: 1400  loss: =  1.035004\n",
      "step: 1500  loss: =  1.043615\n",
      "step: 1600  loss: =  1.019189\n",
      "step: 1700  loss: =  1.021287\n",
      "completed data: 700  kernels: = 16.000000\n",
      "537836.56\n",
      "542704.2\n",
      "step: 0  loss: = 5038.923750\n",
      "step: 100  loss: =  1.620286\n",
      "step: 200  loss: =  1.490511\n",
      "step: 300  loss: =  1.071671\n",
      "step: 400  loss: =  1.054891\n",
      "step: 500  loss: =  0.946176\n",
      "step: 600  loss: =  0.920939\n",
      "step: 700  loss: =  0.913120\n",
      "step: 800  loss: =  0.899956\n",
      "step: 900  loss: =  0.891337\n",
      "step: 1000  loss: =  0.894413\n",
      "step: 1100  loss: =  0.855326\n",
      "step: 1200  loss: =  0.841744\n",
      "step: 1300  loss: =  0.855514\n",
      "step: 1400  loss: =  0.865898\n",
      "step: 1500  loss: =  0.878689\n",
      "step: 1600  loss: =  0.834886\n",
      "step: 1700  loss: =  0.836009\n",
      "completed data: 700  kernels: = 28.000000\n",
      "42463.55\n",
      "46618.652\n",
      "step: 0  loss: = 412.713555\n",
      "step: 100  loss: =  3.137677\n",
      "step: 200  loss: =  1.583582\n",
      "step: 300  loss: =  1.419876\n",
      "step: 400  loss: =  0.635205\n",
      "step: 500  loss: =  0.204542\n",
      "step: 600  loss: =  0.165118\n",
      "step: 700  loss: =  0.102086\n",
      "step: 800  loss: =  0.018883\n",
      "step: 900  loss: =  0.016294\n",
      "step: 1000  loss: =  0.008804\n",
      "step: 1100  loss: =  0.007845\n",
      "step: 1200  loss: =  0.007222\n",
      "step: 1300  loss: =  0.006405\n",
      "step: 1400  loss: =  0.004718\n",
      "step: 1500  loss: =  0.002379\n",
      "step: 1600  loss: =  0.002181\n",
      "step: 1700  loss: =  0.002083\n",
      "step: 1800  loss: =  0.001366\n",
      "step: 1900  loss: =  0.000681\n",
      "step: 2000  loss: =  0.000455\n",
      "step: 2100  loss: =  0.000459\n",
      "step: 2200  loss: =  0.000472\n",
      "step: 2300  loss: =  0.000471\n",
      "step: 2400  loss: =  0.000493\n",
      "step: 2500  loss: =  0.000437\n",
      "step: 2600  loss: =  0.000481\n",
      "completed data: 1400  kernels: =  3.000000\n",
      "87178.195\n",
      "90627.9\n",
      "step: 0  loss: = 831.598516\n",
      "step: 100  loss: =  2.371986\n",
      "step: 200  loss: =  1.244460\n",
      "step: 300  loss: =  1.079905\n",
      "step: 400  loss: =  1.035249\n",
      "step: 500  loss: =  1.035411\n",
      "step: 600  loss: =  1.023517\n",
      "step: 700  loss: =  1.022566\n",
      "step: 800  loss: =  1.028295\n",
      "step: 900  loss: =  1.025402\n",
      "step: 1000  loss: =  1.035571\n",
      "step: 1100  loss: =  1.022643\n",
      "step: 1200  loss: =  1.024549\n",
      "step: 1300  loss: =  1.026223\n",
      "step: 1400  loss: =  1.026934\n",
      "step: 1500  loss: =  1.034855\n",
      "completed data: 1400  kernels: =  8.000000\n",
      "223189.78\n",
      "225388.33\n",
      "step: 0  loss: = 2057.374219\n",
      "step: 100  loss: =  1.783743\n",
      "step: 200  loss: =  1.173355\n",
      "step: 300  loss: =  1.078842\n",
      "step: 400  loss: =  1.057181\n",
      "step: 500  loss: =  1.053036\n",
      "step: 600  loss: =  1.042258\n",
      "step: 700  loss: =  1.036526\n",
      "step: 800  loss: =  1.040321\n",
      "step: 900  loss: =  1.058907\n",
      "step: 1000  loss: =  1.056921\n",
      "step: 1100  loss: =  1.042984\n",
      "step: 1200  loss: =  1.038404\n",
      "step: 1300  loss: =  1.043641\n",
      "step: 1400  loss: =  1.047482\n",
      "step: 1500  loss: =  1.056310\n",
      "step: 1600  loss: =  1.042760\n",
      "step: 1700  loss: =  1.038679\n",
      "completed data: 1400  kernels: = 16.000000\n",
      "517224.8\n",
      "524833.5\n",
      "step: 0  loss: = 4518.874063\n",
      "step: 100  loss: =  1.544741\n",
      "step: 200  loss: =  1.123320\n",
      "step: 300  loss: =  1.063877\n",
      "step: 400  loss: =  1.052989\n",
      "step: 500  loss: =  1.057208\n",
      "step: 600  loss: =  1.040277\n",
      "step: 700  loss: =  1.026654\n",
      "step: 800  loss: =  1.045905\n",
      "step: 900  loss: =  1.070816\n",
      "step: 1000  loss: =  1.053538\n",
      "step: 1100  loss: =  1.030275\n",
      "step: 1200  loss: =  1.020641\n",
      "step: 1300  loss: =  1.030253\n",
      "step: 1400  loss: =  1.054926\n",
      "step: 1500  loss: =  1.064165\n",
      "step: 1600  loss: =  1.037167\n",
      "step: 1700  loss: =  1.013480\n",
      "completed data: 1400  kernels: = 28.000000\n",
      "36368.703\n",
      "39660.0\n",
      "step: 0  loss: = 340.545430\n",
      "step: 100  loss: =  1.805432\n",
      "step: 200  loss: =  1.523854\n",
      "step: 300  loss: =  0.279321\n",
      "step: 400  loss: =  0.021716\n",
      "step: 500  loss: =  0.007336\n",
      "step: 600  loss: =  0.006471\n",
      "step: 700  loss: =  0.004729\n",
      "step: 800  loss: =  0.002531\n",
      "step: 900  loss: =  0.000605\n",
      "step: 1000  loss: =  0.000479\n",
      "step: 1100  loss: =  0.000479\n",
      "step: 1200  loss: =  0.000531\n",
      "step: 1300  loss: =  0.000503\n",
      "step: 1400  loss: =  0.000536\n",
      "step: 1500  loss: =  0.000465\n",
      "step: 1600  loss: =  0.000475\n",
      "step: 1700  loss: =  0.000490\n",
      "step: 1800  loss: =  0.000505\n",
      "step: 1900  loss: =  0.000530\n",
      "completed data: 2800  kernels: =  3.000000\n",
      "88142.99\n",
      "90890.81\n",
      "step: 0  loss: = 802.463438\n",
      "step: 100  loss: =  1.535456\n",
      "step: 200  loss: =  1.065906\n",
      "step: 300  loss: =  1.002483\n",
      "step: 400  loss: =  1.003611\n",
      "step: 500  loss: =  1.008230\n",
      "step: 600  loss: =  0.992851\n",
      "step: 700  loss: =  0.992188\n",
      "step: 800  loss: =  0.995848\n",
      "step: 900  loss: =  0.999199\n",
      "step: 1000  loss: =  1.012232\n",
      "step: 1100  loss: =  0.993324\n",
      "step: 1200  loss: =  0.989356\n",
      "completed data: 2800  kernels: =  8.000000\n",
      "215945.06\n",
      "217996.6\n",
      "step: 0  loss: = 1821.450938\n",
      "step: 100  loss: =  1.242197\n",
      "step: 200  loss: =  1.090476\n",
      "step: 300  loss: =  1.044114\n",
      "step: 400  loss: =  1.043265\n",
      "step: 500  loss: =  1.045006\n",
      "step: 600  loss: =  1.029583\n",
      "step: 700  loss: =  1.029468\n",
      "step: 800  loss: =  1.036618\n",
      "step: 900  loss: =  1.049690\n",
      "step: 1000  loss: =  1.049458\n",
      "step: 1100  loss: =  1.035109\n",
      "step: 1200  loss: =  1.031055\n",
      "step: 1300  loss: =  1.029433\n",
      "step: 1400  loss: =  1.053903\n",
      "step: 1500  loss: =  1.042563\n",
      "step: 1600  loss: =  1.033072\n",
      "step: 1700  loss: =  1.027996\n",
      "step: 1800  loss: =  1.040912\n",
      "step: 1900  loss: =  1.071450\n",
      "step: 2000  loss: =  1.047115\n",
      "step: 2100  loss: =  1.029616\n",
      "step: 2200  loss: =  1.028667\n",
      "step: 2300  loss: =  1.025479\n",
      "step: 2400  loss: =  1.045814\n",
      "step: 2500  loss: =  1.041284\n",
      "step: 2600  loss: =  1.032268\n",
      "step: 2700  loss: =  1.026756\n",
      "step: 2800  loss: =  1.038356\n",
      "step: 2900  loss: =  1.055979\n",
      "step: 3000  loss: =  1.050546\n",
      "step: 3100  loss: =  1.033423\n",
      "step: 3200  loss: =  1.030206\n",
      "step: 3300  loss: =  1.037812\n",
      "step: 3400  loss: =  1.043061\n",
      "step: 3500  loss: =  1.049666\n",
      "step: 3600  loss: =  1.033889\n",
      "step: 3700  loss: =  1.024773\n",
      "completed data: 2800  kernels: = 16.000000\n",
      "482655.47\n",
      "490463.1\n",
      "step: 0  loss: = 3623.250000\n",
      "step: 100  loss: =  1.344965\n",
      "step: 200  loss: =  1.087926\n",
      "step: 300  loss: =  1.087217\n",
      "step: 400  loss: =  1.114772\n",
      "step: 500  loss: =  1.109963\n",
      "step: 600  loss: =  1.059923\n",
      "step: 700  loss: =  1.048221\n",
      "step: 800  loss: =  1.080361\n",
      "step: 900  loss: =  1.137816\n",
      "step: 1000  loss: =  1.102610\n",
      "step: 1100  loss: =  1.052430\n",
      "step: 1200  loss: =  1.048189\n",
      "step: 1300  loss: =  1.094045\n",
      "step: 1400  loss: =  1.184066\n",
      "step: 1500  loss: =  1.099402\n",
      "step: 1600  loss: =  1.059114\n",
      "step: 1700  loss: =  1.061240\n",
      "step: 1800  loss: =  1.076727\n",
      "step: 1900  loss: =  1.105976\n",
      "step: 2000  loss: =  1.112718\n",
      "step: 2100  loss: =  1.056606\n",
      "completed data: 2800  kernels: = 28.000000\n",
      "41388.887\n",
      "46690.223\n",
      "step: 0  loss: = 369.531445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 100  loss: =  1.647094\n",
      "step: 200  loss: =  0.355805\n",
      "step: 300  loss: =  0.017244\n",
      "step: 400  loss: =  0.003103\n",
      "step: 500  loss: =  0.000480\n",
      "step: 600  loss: =  0.000476\n",
      "step: 700  loss: =  0.000471\n",
      "step: 800  loss: =  0.000480\n",
      "step: 900  loss: =  0.000517\n",
      "step: 1000  loss: =  0.000463\n",
      "step: 1100  loss: =  0.000451\n",
      "step: 1200  loss: =  0.000444\n",
      "step: 1300  loss: =  0.000465\n",
      "step: 1400  loss: =  0.000473\n",
      "step: 1500  loss: =  0.000480\n",
      "step: 1600  loss: =  0.000473\n",
      "step: 1700  loss: =  0.000492\n",
      "completed data: 5500  kernels: =  3.000000\n",
      "85815.08\n",
      "89373.66\n",
      "step: 0  loss: = 710.357031\n",
      "step: 100  loss: =  1.123627\n",
      "step: 200  loss: =  0.992466\n",
      "step: 300  loss: =  0.991336\n",
      "step: 400  loss: =  0.993093\n",
      "step: 500  loss: =  1.003790\n",
      "step: 600  loss: =  0.978749\n",
      "step: 700  loss: =  0.977553\n",
      "step: 800  loss: =  0.990114\n",
      "step: 900  loss: =  0.999619\n",
      "step: 1000  loss: =  1.000933\n",
      "step: 1100  loss: =  0.978244\n",
      "step: 1200  loss: =  0.980263\n",
      "completed data: 5500  kernels: =  8.000000\n",
      "215519.86\n",
      "218757.36\n",
      "step: 0  loss: = 1537.691562\n",
      "step: 100  loss: =  1.104316\n",
      "step: 200  loss: =  1.022564\n",
      "step: 300  loss: =  1.027251\n",
      "step: 400  loss: =  1.040341\n",
      "step: 500  loss: =  1.043519\n",
      "step: 600  loss: =  1.013465\n",
      "step: 700  loss: =  1.012402\n",
      "completed data: 5500  kernels: = 16.000000\n",
      "563682.56\n",
      "561488.9\n",
      "step: 0  loss: = 3356.159375\n",
      "step: 100  loss: =  1.103802\n",
      "step: 200  loss: =  1.044679\n",
      "step: 300  loss: =  1.084966\n",
      "step: 400  loss: =  1.157861\n",
      "step: 500  loss: =  1.100695\n",
      "step: 600  loss: =  1.029404\n",
      "step: 700  loss: =  1.037917\n",
      "step: 800  loss: =  1.057000\n",
      "step: 900  loss: =  1.124465\n",
      "step: 1000  loss: =  1.095263\n",
      "step: 1100  loss: =  1.028391\n",
      "completed data: 5500  kernels: = 28.000000\n",
      "40893.797\n",
      "44878.918\n",
      "step: 0  loss: = 323.141406\n",
      "step: 100  loss: =  0.999516\n",
      "step: 200  loss: =  0.022263\n",
      "step: 300  loss: =  0.001355\n",
      "step: 400  loss: =  0.000560\n",
      "step: 500  loss: =  0.000522\n",
      "step: 600  loss: =  0.000504\n",
      "step: 700  loss: =  0.000483\n",
      "step: 800  loss: =  0.000555\n",
      "step: 900  loss: =  0.000585\n",
      "step: 1000  loss: =  0.000493\n",
      "step: 1100  loss: =  0.000504\n",
      "step: 1200  loss: =  0.000576\n",
      "step: 1300  loss: =  0.000539\n",
      "completed data: 11000  kernels: =  3.000000\n",
      "84523.984\n",
      "87961.86\n",
      "step: 0  loss: = 568.242305\n",
      "step: 100  loss: =  1.029175\n",
      "step: 200  loss: =  0.985690\n",
      "step: 300  loss: =  0.991740\n",
      "step: 400  loss: =  0.998399\n",
      "step: 500  loss: =  1.005823\n",
      "step: 600  loss: =  0.980401\n",
      "step: 700  loss: =  0.985051\n",
      "completed data: 11000  kernels: =  8.000000\n",
      "216001.45\n",
      "219318.1\n",
      "step: 0  loss: = 1069.262656\n",
      "step: 100  loss: =  1.054297\n",
      "step: 200  loss: =  1.016472\n",
      "step: 300  loss: =  1.029799\n",
      "step: 400  loss: =  1.046260\n",
      "step: 500  loss: =  1.039886\n",
      "step: 600  loss: =  1.010489\n",
      "step: 700  loss: =  1.016412\n",
      "step: 800  loss: =  1.022140\n",
      "step: 900  loss: =  1.042172\n",
      "step: 1000  loss: =  1.040595\n",
      "step: 1100  loss: =  1.009953\n",
      "completed data: 11000  kernels: = 16.000000\n",
      "495336.8\n",
      "494368.78\n",
      "step: 0  loss: = 1486.446875\n",
      "step: 100  loss: =  1.053462\n",
      "step: 200  loss: =  1.039475\n",
      "step: 300  loss: =  1.086556\n",
      "step: 400  loss: =  1.095171\n",
      "step: 500  loss: =  1.091874\n",
      "step: 600  loss: =  1.019425\n",
      "step: 700  loss: =  1.040550\n",
      "step: 800  loss: =  1.070359\n",
      "step: 900  loss: =  1.109718\n",
      "step: 1000  loss: =  1.091100\n",
      "step: 1100  loss: =  1.021221\n",
      "completed data: 11000  kernels: = 28.000000\n"
     ]
    }
   ],
   "source": [
    "kernels = [3, 8, 16, 28]\n",
    "lambdas = [1e-1, 1e1, 1e1, 1e1] # knowledge 0.2\n",
    "\n",
    "datas = [60, 340, 700, 1400, 2800, 5500, 11000, 22000, 98000]\n",
    "training_epochs = [6000, 4000, 4000, 3000, 3000, 2000, 2000, 1000, 500]\n",
    "test_sizes = [2080, 2080, 2080, 2080, 2080, 2080, 2080, 2080, 2080, 2000]\n",
    "\n",
    "learn_rate = 1e-3\n",
    "learn_rate_late = 1e-4\n",
    "\n",
    "\n",
    "for i_data in range(7):\n",
    "    \n",
    "    for i_kernel in range(4): \n",
    "        \n",
    "        if i_kernel > 0: \n",
    "            del stimulus_\n",
    "            del bipolar_cell_layer\n",
    "            del gc_activation\n",
    "            del gc_output\n",
    "            del bipolar_bias\n",
    "            del bipkernels1\n",
    "        \n",
    "        no_train=datas[i_data]\n",
    "        epochs = training_epochs[i_data]\n",
    "        no_kernels = kernels[i_kernel]\n",
    "        lambda1 = lambdas[i_kernel]\n",
    "        bipkernels1 = bipkernels[:, :, :, 0:no_kernels]\n",
    "        bip_gc_syn_init = bip_gc_syn_init1[:, :, :, 0:no_kernels]\n",
    "        bip_am_syn_mask1 = bip_am_syn_mask[ :, :, 0:no_kernels, :, :]\n",
    "        \n",
    "        \n",
    "        no_test=test_sizes[i_data] \n",
    "        no_bipolars = 10\n",
    "        no_amacrines = 5\n",
    "\n",
    "        wheretosave = '/home/ubuntu/Notebooks/Circuit3A_Trained_Network_data' + str(no_train) + '_kernel' + str(no_kernels) + '_sd' + str(sd) + '.mat'\n",
    "#         wheretosave = '/home/ubuntu/Notebooks/Circuit3B_Trained_Network_data' + str(no_train) + '_kernel' + str(no_kernels) + '_sd' + str(sd) + '.mat'\n",
    "\n",
    "        ## initialize all variables\n",
    "        bip_bias_init_all = -1.0*np.ones([28])\n",
    "        bip_bias_init_all[0]=-2.0\n",
    "        bip_bias_init_all[1]=-3.0\n",
    "        bip_bias_init_all[3]=-15.0\n",
    "        bip_bias_init_all[8]=-25.0\n",
    "        bip_bias_init_all[9]=-10.0\n",
    "        \n",
    "        bip_bias_init_all[4]=-2.0\n",
    "        bip_bias_init_all[5]=-3.0\n",
    "        bip_bias_init_all[7]=-15.0\n",
    "        bip_bias_init_all[12]=-25.0\n",
    "        bip_bias_init_all[13]=-10.0\n",
    "\n",
    "        \n",
    "        bip_bias_init = bip_bias_init_all[0:no_kernels]\n",
    "        bip_bias_init = bip_bias_init.astype(float32)\n",
    "        bipolar_bias = bias_var([no_kernels], bip_bias_init)\n",
    "        \n",
    "        am_bias_init = -5.0 \n",
    "        am_bias = bias_var([1], am_bias_init)\n",
    "        \n",
    "        gc_bias = bias_var([1], gc_bias_init)\n",
    "        \n",
    "        bip_gc_syn_init=tf.random.normal([1, no_bipolars, no_bipolars, no_kernels], mean = 0.0, stddev = sqrt(2.0/(no_kernels*100)), dtype=tf.dtypes.float32, seed=sd)\n",
    "        bip_gc_syn = synapse_var([1, no_bipolars, no_bipolars, no_kernels], bip_gc_syn_init)\n",
    "        \n",
    "        bip_am_syn_inds = np.zeros([no_kernels*100, 6])\n",
    "        for i in range(10):\n",
    "            for j in range(10):\n",
    "                for k in range(no_kernels):\n",
    "                    bip_am_syn_inds[no_kernels*10*(i)+no_kernels*(j)+k]=[0, i, j, k, floor(i/2), floor(j/2)]\n",
    "        bip_am_syn_inds = bip_am_syn_inds.astype(int64)\n",
    "        bip_am_syn_init11 = abs(np.random.normal(0.0, (sqrt(2.0/no_kernels)), size=[no_kernels*100]))\n",
    "        bip_am_syn_init111=0.1*bip_am_syn_init11.astype(float32)  \n",
    "        bip_am_syn_val = synapse_var([no_kernels*no_bipolars*no_bipolars], bip_am_syn_init111)\n",
    "        bip_am_syn1 = tf.sparse.SparseTensor(indices=bip_am_syn_inds, values=bip_am_syn_val, dense_shape=[1, no_bipolars, no_bipolars, no_kernels, no_amacrines, no_amacrines])\n",
    "        bip_am_syn = tf.sparse.to_dense(tf.sparse.reorder(bip_am_syn1))        \n",
    "\n",
    "        am_gc_syn = synapse_var([1, no_amacrines, no_amacrines], am_gc_syn_init1)\n",
    "\n",
    "        stimulus_ = tf.placeholder(\"float32\", name=\"stim_placeholder\")\n",
    "\n",
    "        bipolar_cell_layer = tf.nn.relu(tf.nn.bias_add(bip_conv2d(stimulus_, bipkernels1), bipolar_bias))\n",
    "\n",
    "        biplyr = tf.reshape(bipolar_cell_layer, [-1, no_bipolars*no_bipolars*no_kernels, 1])\n",
    "\n",
    "        tilebip_am_syn=tf.tile(tf.transpose(tf.reshape(tf.abs(bip_am_syn), [1, no_bipolars*no_bipolars*no_kernels, no_amacrines*no_amacrines]), [0, 2, 1]), [1, 1, 1])\n",
    "\n",
    "        amacrine_activation = 3.0*tf.reshape(tf.linalg.matmul(tilebip_am_syn, biplyr), [-1,no_amacrines, no_amacrines])\n",
    "     \n",
    "        amacrine_cell_layer = tf.nn.relu(tf.add(amacrine_activation, am_bias))\n",
    "\n",
    "        gc_activation = tf.multiply(tf.abs(bip_gc_syn), bipolar_cell_layer)\n",
    "\n",
    "        gc_activation_inhib = tf.multiply(tf.abs(am_gc_syn), amacrine_cell_layer)\n",
    "\n",
    "        gc_output = tf.nn.relu(tf.add_n([tf.reduce_sum(gc_activation, [1, 2, 3]), -1.0*tf.reduce_sum(gc_activation_inhib, [1, 2])])+gc_bias)\n",
    "\n",
    "        ## training procedure\n",
    "        y_ = tf.placeholder(\"float32\", name=\"output_spikes\")\n",
    "        \n",
    "        batchsize=20\n",
    "\n",
    "        loss = (tf.nn.l2_loss((tf.squeeze(gc_output) - tf.squeeze(y_)), name='loss'))\n",
    "\n",
    "        regularizer=tf.add_n([tf.reduce_sum(tf.abs(bip_am_syn)), tf.reduce_sum(tf.abs(bip_gc_syn)), \\\n",
    "                              0.0*tf.reduce_sum(tf.abs(am_gc_syn))])\n",
    "\n",
    "        objective=tf.add(loss, lambda1*regularizer)\n",
    "        \n",
    "        bip_am_ygrad = tf.gradients(loss, [bip_am_syn])\n",
    "        bip_am_reggrad = tf.gradients(regularizer, [bip_am_syn])\n",
    "        \n",
    "        am_gc_ygrad = tf.gradients(loss, [am_gc_syn])\n",
    "        am_gc_reggrad = tf.gradients(regularizer, [am_gc_syn])\n",
    "        \n",
    "        bip_gc_ygrad = tf.gradients(loss, [bip_gc_syn])\n",
    "        bip_gc_reggrad = tf.gradients(regularizer, [bip_gc_syn])\n",
    "        \n",
    "\n",
    "\n",
    "        algorithm_choice=2\n",
    "        lr_min = 1e-4\n",
    "        lr_max = 1e-5\n",
    "        max_step =500\n",
    "        lr_ = tf.placeholder(\"float32\", name=\"learn_rate\")\n",
    "        \n",
    "        if algorithm_choice==1:\n",
    "            train_step = tf.train.GradientDescentOptimizer(lr_).minimize(objective)\n",
    "        elif algorithm_choice==2:\n",
    "            my_epsilon=1e-8\n",
    "            train_step = tf.train.AdamOptimizer(learning_rate=lr_, epsilon=my_epsilon).minimize(objective)\n",
    "        elif algorithm_choice==3:\n",
    "            momentum_par=0.9\n",
    "            train_step = tf.train.MomentumOptimizer(lr_, momentum_par).minimize(objective)\n",
    "        elif algorithm_choice==4:\n",
    "            train_step = tf.train.AdagradOptimizer(lr_).minimize(objective)\n",
    "        elif algorithm_choice==5:\n",
    "            train_step = tf.train.RMSPropOptimizer(lr_).minimize(objective)\n",
    "            \n",
    "\n",
    "        sess.run(tf.global_variables_initializer())    \n",
    "\n",
    "        bip_gc_syn_hist=tf.reshape(bip_gc_syn.eval(session=sess), [1, no_bipolars, no_bipolars, no_kernels]) \n",
    "        bip_am_syn_hist=tf.reshape(bip_am_syn.eval(session=sess), [1, no_bipolars, no_bipolars, no_kernels, no_amacrines, no_amacrines]) \n",
    "        am_gc_syn_hist=tf.reshape(am_gc_syn.eval(session=sess), [1, no_amacrines, no_amacrines]) \n",
    "        train_loss_hist = ones([1])\n",
    "        test_loss_hist = ones([1])\n",
    "        \n",
    "        bip_am_ygrad_hist=np.reshape(sess.run(bip_am_ygrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_bipolars, no_bipolars, no_kernels, no_amacrines, no_amacrines]) \n",
    "        bip_am_reggrad_hist=np.reshape(sess.run(bip_am_reggrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_bipolars, no_bipolars, no_kernels, no_amacrines, no_amacrines]) \n",
    "        am_gc_ygrad_hist=np.reshape(sess.run(am_gc_ygrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_amacrines, no_amacrines])\n",
    "        am_gc_reggrad_hist=np.reshape(sess.run(am_gc_reggrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_amacrines, no_amacrines])\n",
    "        bip_gc_ygrad_hist=np.reshape(sess.run(bip_gc_ygrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_bipolars, no_bipolars, no_kernels]) \n",
    "        bip_gc_reggrad_hist=np.reshape(sess.run(bip_gc_reggrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_bipolars, no_bipolars, no_kernels]) \n",
    "\n",
    "        train_fd={stimulus_: x_train[0:50, :, :, :]}\n",
    "        test_fd={stimulus_: x_test[0:50, :, :, :]}\n",
    "        train_output_hist=reshape(gc_output.eval(session=sess, feed_dict=train_fd), [1, 50])\n",
    "        test_output_hist=reshape(gc_output.eval(session=sess, feed_dict=test_fd), [1, 50])\n",
    "        \n",
    "        \n",
    "        check=1.0\n",
    "        step=0\n",
    "        end_flag=0\n",
    "\n",
    "        fd = {stimulus_:x_train[0:100, :, :, :], y_:y_train[0, 0:100]}\n",
    "        train_loss_val = sess.run(loss, feed_dict = fd)\n",
    "        print(train_loss_val)\n",
    "\n",
    "        fd = {stimulus_:x_test[0:100, :, :, :], y_:y_test[0, 0:100]}\n",
    "        test_loss_val = sess.run(loss, feed_dict = fd)\n",
    "        print(test_loss_val)\n",
    "\n",
    "\n",
    "\n",
    "        train_loss_hist=train_loss_val*train_loss_hist\n",
    "        test_loss_hist=test_loss_val*test_loss_hist\n",
    "\n",
    "        \n",
    "        endflag=0\n",
    "        step=0\n",
    "        while endflag == 0:\n",
    "            # learning rate schedule\n",
    "            learn_rate_sch = lr_min + 0.5*(lr_max - lr_min)*(1.0+np.cos(np.pi*(step%max_step/max_step))) \n",
    "            if step>=10*max_step:\n",
    "                learn_rate_sch = lr_min\n",
    "\n",
    "            inds = np.reshape(np.random.permutation(range(no_train)), [-1, batchsize])\n",
    "            for n in range(len(inds)): \n",
    "                fdd = {stimulus_: x_train[inds[n, :], :, :, :], y_: y_train[0, inds[n, :]], lr_: learn_rate_sch} \n",
    "                                                                 \n",
    "                sess.run(train_step, feed_dict=fdd)\n",
    "\n",
    "                        \n",
    "            if (step % 100 ==0):\n",
    "\n",
    "                train_loss_val = sess.run(loss, feed_dict= {stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]})/100.0\n",
    "                test_loss_val = sess.run(loss, feed_dict= {stimulus_: x_test[0:100, :, :, :], y_: y_test[0, 0:100]})/100.0\n",
    "                print(\"step: %d  loss: = %9f\" % (step, train_loss_val))\n",
    "\n",
    "                bip_gc_syn_hist=tf.concat( [bip_gc_syn_hist, tf.reshape(bip_gc_syn.eval(session=sess), [1, no_bipolars, no_bipolars, no_kernels])], 0,  name='bip_gc_syn_concat')\n",
    "                bip_am_syn_hist=tf.concat( [bip_am_syn_hist, tf.reshape(bip_am_syn.eval(session=sess), [1, no_bipolars, no_bipolars, no_kernels, no_amacrines, no_amacrines])], 0,  name='bip_am_syn_concat')\n",
    "                am_gc_syn_hist=tf.concat( [am_gc_syn_hist, tf.reshape(am_gc_syn.eval(session=sess), [1, no_amacrines, no_amacrines])], 0,  name='am_gc_syn_concat')\n",
    "\n",
    "                bip_am_ygrad_hist=tf.concat( [bip_am_ygrad_hist, np.reshape(sess.run(bip_am_ygrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_bipolars, no_bipolars, no_kernels, no_amacrines, no_amacrines])], 0)\n",
    "                bip_am_reggrad_hist=tf.concat( [bip_am_reggrad_hist, np.reshape(sess.run(bip_am_reggrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_bipolars, no_bipolars, no_kernels, no_amacrines, no_amacrines])], 0)\n",
    "                am_gc_ygrad_hist=tf.concat( [am_gc_ygrad_hist, np.reshape(sess.run(am_gc_ygrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_amacrines, no_amacrines])], 0)\n",
    "                am_gc_reggrad_hist=tf.concat( [am_gc_reggrad_hist, np.reshape(sess.run(am_gc_reggrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_amacrines, no_amacrines])], 0)\n",
    "                bip_gc_ygrad_hist=tf.concat( [bip_gc_ygrad_hist, np.reshape(sess.run(bip_gc_ygrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_bipolars, no_bipolars, no_kernels])], 0)\n",
    "                bip_gc_reggrad_hist=tf.concat( [bip_gc_reggrad_hist, np.reshape(sess.run(bip_gc_reggrad, feed_dict={stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}), [1, no_bipolars, no_bipolars, no_kernels])], 0)\n",
    "\n",
    "                train_loss_hist=np.concatenate([train_loss_hist, np.array([train_loss_val])], axis=0)\n",
    "                test_loss_hist=np.concatenate([test_loss_hist, np.array([test_loss_val])], axis=0)\n",
    "                \n",
    "                train_fd={stimulus_: x_train[0:50, :, :, :]}\n",
    "                test_fd={stimulus_: x_test[0:50, :, :, :]}\n",
    "                train_output=reshape(gc_output.eval(session=sess, feed_dict=train_fd), [1, 50])\n",
    "                test_output=reshape(gc_output.eval(session=sess, feed_dict=test_fd), [1, 50])\n",
    "                \n",
    "                train_output_hist=np.concatenate([train_output_hist, train_output], axis=0)\n",
    "                test_output_hist=np.concatenate([test_output_hist, test_output], axis=0)\n",
    "                \n",
    "                # stopping condition\n",
    "                if (step/100)>=5:\n",
    "                    b=np.diff(train_loss_hist[int(step/100-5):int(step/100)])\n",
    "                    a=abs(b)<1.0\n",
    "                    c=b>=0.0\n",
    "                    if sum(c)>=3:\n",
    "                        endflag=1\n",
    "            step = step + 1\n",
    "\n",
    "\n",
    "        db = {}\n",
    "\n",
    "        db['bipolar_bias'] = bipolar_bias.eval(session=sess)\n",
    "        db['bip_gc_syn_hist'] = bip_gc_syn_hist.eval(session=sess)\n",
    "        db['bip_am_syn_hist'] = bip_am_syn_hist.eval(session=sess)\n",
    "        db['am_gc_syn_hist'] = am_gc_syn_hist.eval(session=sess)\n",
    "        db['gc_bias'] = gc_bias.eval(session=sess)\n",
    "        \n",
    "        db['bip_am_ygrad_hist'] = bip_am_ygrad_hist.eval(session=sess)\n",
    "        db['bip_am_reggrad_hist'] = bip_am_reggrad_hist.eval(session=sess)\n",
    "        db['am_gc_ygrad_hist'] = am_gc_ygrad_hist.eval(session=sess)\n",
    "        db['am_gc_reggrad_hist'] = am_gc_reggrad_hist.eval(session=sess)\n",
    "        db['bip_gc_ygrad_hist'] = bip_gc_ygrad_hist.eval(session=sess)\n",
    "        db['bip_gc_reggrad_hist'] = bip_gc_reggrad_hist.eval(session=sess)\n",
    "\n",
    "\n",
    "\n",
    "        db['no_train']=no_train\n",
    "        db['no_test']=no_test\n",
    "\n",
    "        db['no_kernels'] = no_kernels\n",
    "        db['no_bipolars']=no_bipolars\n",
    "\n",
    "        db['bipkernels'] = bipkernels\n",
    "        db['randomseed'] = sd\n",
    "\n",
    "        db['train_output_hist'] = train_output_hist\n",
    "        db['test_output_hist'] = test_output_hist\n",
    "\n",
    "        db['algorithm_choice'] = algorithm_choice\n",
    "        db['learn_rate'] = learn_rate\n",
    "        db['lambda'] = lambda1\n",
    "\n",
    "        db['train_loss_hist'] = train_loss_hist\n",
    "        db['test_loss_hist'] = test_loss_hist                          \n",
    "\n",
    "        struct_proj = np.zeros([len(train_loss_hist), 1])\n",
    "\n",
    "        syn_hist = bip_gc_syn_hist.eval(session=sess)\n",
    "        basyn_hist = abs(bip_am_syn_hist.eval(session=sess))\n",
    "        agsyn_hist = abs(am_gc_syn_hist.eval(session=sess))\n",
    "        \n",
    "        truesyn = np.zeros([10, 10, no_kernels])\n",
    "        \n",
    "        truebasyn = np.zeros([no_bipolars, no_bipolars, no_kernels, no_amacrines, no_amacrines])\n",
    "        truebasyn[:, :, 0:3, :, :]=bip_am_syn_init\n",
    "        \n",
    "        trueagsyn=am_gc_syn_init\n",
    "        \n",
    "        norm_factor = (np.sum(np.square(truebasyn)) + np.sum(np.square(trueagsyn)))\n",
    "        for i in range(len(train_loss_hist)):\n",
    "            norm_factor = (np.sum(np.square(basyn_hist[i, :, :, :, :, :])) + np.sum(np.square(agsyn_hist[i, :, :])))\n",
    "            struct_proj[i] = (np.sum(np.multiply((basyn_hist[i, :, :, :, :, :]), truebasyn))+np.sum(np.multiply((agsyn_hist[i, :, :]), trueagsyn)))/norm_factor\n",
    "\n",
    "        db['struct_proj'] = struct_proj\n",
    "\n",
    "        sio.savemat(wheretosave, db)\n",
    "        \n",
    "        print(\"completed data: %d  kernels: = %9f\" % (no_train, no_kernels))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.95063984 0.5828657  0.38144782 0.25167096 0.6110331  0.63634413\n",
      "  1.1375167  0.37116188 0.3738968  0.36353296 0.16722773 0.4230566\n",
      "  0.72445667 0.7080679  0.36235395 0.47952414 0.78879434 0.5060988\n",
      "  0.94206756 0.5335892  1.7905157  0.25151402 0.9466383  0.7494325\n",
      "  0.69904053 0.5597612  0.5946239  0.6806819  0.4319149  0.5497766\n",
      "  0.7678618  0.1727006  0.7676616  0.43221897 1.5884638  0.5670322\n",
      "  1.3228142  0.24523371 0.3425265  0.49594355 0.9181344  1.053214\n",
      "  0.5079092  0.78113127 1.214992   0.7045511  0.54614013 0.792282\n",
      "  1.8342131  0.59105015 0.6204924  0.7848783  0.2890526  0.27828673\n",
      "  0.43007386 0.37421447 1.1664753  0.77967423 0.2872109  0.42856836\n",
      "  0.1907293  0.7879961  0.6665944  0.5663151  0.63656473 1.2446904\n",
      "  0.79417825 1.0074445  0.40405294 0.24392527 0.4967049  0.40421417\n",
      "  0.31279206 0.28556365 0.886541   0.42507738 1.1005278  0.95588815\n",
      "  0.26616296 0.19438986 1.7633483  0.83045363 1.5218362  0.68291825\n",
      "  0.66377425 1.0193806  0.96206844 0.20753859 1.376122   0.23152187\n",
      "  0.3526694  0.8364196  0.5493345  0.5894148  1.22297    0.5147882\n",
      "  0.571411   0.50981516 1.0741011  0.14399125]]\n",
      "[[[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "fd=feed_dict= {stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}\n",
    "train_output=gc_output.eval(session=sess, feed_dict=fd)\n",
    "print(train_output)\n",
    "\n",
    "fd=feed_dict= {stimulus_: x_train[0:100, :, :, :], y_: y_train[0, 0:100]}\n",
    "train_output=amacrine_cell_layer.eval(session=sess, feed_dict=fd)\n",
    "print(train_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p27)",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
